<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="一、理论部分 有了机器学习的基础知识, 这一周咱们进入人工神经网络的学习。
(1)人工神经网络? 不是深度学习么? 二者是什么关系? 人工神经网络是一种模仿人脑神经网络的数学模型，由多个节点（人工神经元）互相连接而成，用来对数据之间的复杂关系进行建模。每个节点代表一种特定函数，来自其他节点的信息经过其相应的权重计算后被传递给下一个节点，以此模拟生物神经网络的工作机制。 深度学习是机器学习的一个分支领域，强调从连续的层（layer）中进行学习，这些层对应于越来越有意义的表示。深度学习是从数据中学习表示的一种新方法，现代深度学习通常包含数十个甚至上百个连续的表示层，这些表示层全都是从训练数据中自动学习的。与其他机器学习方法相比，深度学习的重点往往是学习多个层次的数据表示，从而学会更深层次的理解，因此有时也被称为分层表示学习或层级表示学习。 所以深度学习是通过构建多层神经网络来实现对数据的高层次抽象和复杂模式的提取，从而实现对数据的分类，识别和预测。
随着人们对自然界的认识, 仿生学也逐渐发展起来, 我们要学习的人工神经网络就是这其中一个产物。1943年, McCulloch与Pitts从对人脑神经元的认识抽象出下图所示的简单模型, 这就是著名的“M-P神经元模型”。在这个模型中, 神经元接收到来自$n$ 个其它神经元传递过来的输入信号, 这些输入信号通过带权重的连接对输入信号进行 线性变换, 神经元接收到的总输入与神经元的阈值进行比较, 然后通过激活函数处理以产生神经元的输出。"><meta property="og:title" content="机器学习基础作业班 第二周"><meta property="og:description" content="一、理论部分 有了机器学习的基础知识, 这一周咱们进入人工神经网络的学习。
(1)人工神经网络? 不是深度学习么? 二者是什么关系? 人工神经网络是一种模仿人脑神经网络的数学模型，由多个节点（人工神经元）互相连接而成，用来对数据之间的复杂关系进行建模。每个节点代表一种特定函数，来自其他节点的信息经过其相应的权重计算后被传递给下一个节点，以此模拟生物神经网络的工作机制。 深度学习是机器学习的一个分支领域，强调从连续的层（layer）中进行学习，这些层对应于越来越有意义的表示。深度学习是从数据中学习表示的一种新方法，现代深度学习通常包含数十个甚至上百个连续的表示层，这些表示层全都是从训练数据中自动学习的。与其他机器学习方法相比，深度学习的重点往往是学习多个层次的数据表示，从而学会更深层次的理解，因此有时也被称为分层表示学习或层级表示学习。 所以深度学习是通过构建多层神经网络来实现对数据的高层次抽象和复杂模式的提取，从而实现对数据的分类，识别和预测。
随着人们对自然界的认识, 仿生学也逐渐发展起来, 我们要学习的人工神经网络就是这其中一个产物。1943年, McCulloch与Pitts从对人脑神经元的认识抽象出下图所示的简单模型, 这就是著名的“M-P神经元模型”。在这个模型中, 神经元接收到来自$n$ 个其它神经元传递过来的输入信号, 这些输入信号通过带权重的连接对输入信号进行 线性变换, 神经元接收到的总输入与神经元的阈值进行比较, 然后通过激活函数处理以产生神经元的输出。"><meta property="og:type" content="website"><meta property="og:image" content="https://www.knowlnk.tech/icon.png"><meta property="og:url" content="https://www.knowlnk.tech/notes/machine_leaning/begin_home_work/%E4%BD%9C%E4%B8%9A%E8%AE%AD%E7%BB%83%E8%90%A5-%E7%AC%AC%E4%BA%8C%E5%91%A8/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content="机器学习基础作业班 第二周"><meta name=twitter:description content="一、理论部分 有了机器学习的基础知识, 这一周咱们进入人工神经网络的学习。
(1)人工神经网络? 不是深度学习么? 二者是什么关系? 人工神经网络是一种模仿人脑神经网络的数学模型，由多个节点（人工神经元）互相连接而成，用来对数据之间的复杂关系进行建模。每个节点代表一种特定函数，来自其他节点的信息经过其相应的权重计算后被传递给下一个节点，以此模拟生物神经网络的工作机制。 深度学习是机器学习的一个分支领域，强调从连续的层（layer）中进行学习，这些层对应于越来越有意义的表示。深度学习是从数据中学习表示的一种新方法，现代深度学习通常包含数十个甚至上百个连续的表示层，这些表示层全都是从训练数据中自动学习的。与其他机器学习方法相比，深度学习的重点往往是学习多个层次的数据表示，从而学会更深层次的理解，因此有时也被称为分层表示学习或层级表示学习。 所以深度学习是通过构建多层神经网络来实现对数据的高层次抽象和复杂模式的提取，从而实现对数据的分类，识别和预测。
随着人们对自然界的认识, 仿生学也逐渐发展起来, 我们要学习的人工神经网络就是这其中一个产物。1943年, McCulloch与Pitts从对人脑神经元的认识抽象出下图所示的简单模型, 这就是著名的“M-P神经元模型”。在这个模型中, 神经元接收到来自$n$ 个其它神经元传递过来的输入信号, 这些输入信号通过带权重的连接对输入信号进行 线性变换, 神经元接收到的总输入与神经元的阈值进行比较, 然后通过激活函数处理以产生神经元的输出。"><meta name=twitter:image content="https://www.knowlnk.tech/icon.png"><title>机器学习基础作业班 第二周</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://www.knowlnk.tech//icon.png><link href=https://www.knowlnk.tech/styles.80333fa2099c0bee674efa435fde378c.min.css rel=stylesheet><link href=https://www.knowlnk.tech/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://www.knowlnk.tech/js/darkmode.fb33e48379e1030738da955472e32fad.min.js></script>
<script src=https://www.knowlnk.tech/js/util.00639692264b21bc3ee219733d38a8be.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script defer src=https://www.knowlnk.tech/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://www.knowlnk.tech/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://www.knowlnk.tech/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://www.knowlnk.tech/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://www.knowlnk.tech/",fetchData=Promise.all([fetch("https://www.knowlnk.tech/indices/linkIndex.d6c8376188db408bf8fa89b451143502.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://www.knowlnk.tech/indices/contentIndex.b44a9b467822601084beb78a10df7f75.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://www.knowlnk.tech",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://www.knowlnk.tech",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'’':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/www.knowlnk.tech\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=www.knowlnk.tech src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://www.knowlnk.tech/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://www.knowlnk.tech/>寻剑之路</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>机器学习基础作业班 第二周</h1><p class=meta>Last updated
Unknown
<a href=https://github.com/rootyue/myblog/tree/hugo/content/notes/machine_leaning/begin_home_work/%e4%bd%9c%e4%b8%9a%e8%ae%ad%e7%bb%83%e8%90%a5%20%e7%ac%ac%e4%ba%8c%e5%91%a8.md rel=noopener>Edit Source</a></p><ul class=tags><li><a href=https://www.knowlnk.tech/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></li></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#一理论部分>一、理论部分</a><ol><li><a href=#1人工神经网络-不是深度学习么-二者是什么关系>(1)人工神经网络? 不是深度学习么? 二者是什么关系?</a></li><li><a href=#2为什么说m-p神经元将所有信号的输入看做是同时发生的>(2)为什么说M-P神经元将所有信号的输入看做是同时发生的。</a></li><li><a href=#3输入的信号在传导的过程中分别做了怎样的计算>(3)输入的信号在传导的过程中分别做了怎样的计算。</a></li><li><a href=#4常见到的激活函数还有relu与leakyrelutanh等请写出各自函数定义>(4)常见到的激活函数还有Relu与leakyRelu，tanh等，请写出各自函数定义。</a></li><li><a href=#5在python环境下使用numpy编写sgnlogistic激活函数>(5)在Python环境下使用Numpy编写Sgn、Logistic激活函数。</a></li><li><a href=#算法的收敛性证明被称为感知器收敛定理>算法的收敛性证明被称为感知器收敛定理</a></li><li><a href=#6感知器的收敛算法是怎样的-请用自己语言描述选做>(6)感知器的收敛算法是怎样的? 请用自己语言描述。(选做)</a></li><li><a href=#7代码实现rosenblatt感知器与收敛算法-并使用均方误差-mse-衡量模型性能使用线性可分的双月数据集-进行训练与测试-选做-参考效果如图>(7)代码实现Rosenblatt感知器与收敛算法, 并使用均方误差 (MSE) 衡量模型性能。使用线性可分的双月数据集 进行训练与测试。 (选做) 参考效果如图:</a></li><li><a href=#8那么什么是输入层隐藏层和输出层>(8)那么，什么是输入层、隐藏层和输出层?</a></li><li><a href=#9请将上面右图b分类区域中4个点分别按左图a网络参数设置进行计算>(9)请将上面右图(b)分类区域中4个点分别按左图(a)网络参数设置进行计算。</a></li><li><a href=#10现在你应当清楚的体会到-模型的节点神经元数量越多-参数越大-模型的拟合能力也就越强了那么请你-思考激活函数的作用到底是什么-如果没有激活函数网络会怎样>(10)现在你应当清楚的体会到, 模型的节点（神经元）数量越多, 参数越大, 模型的拟合能力也就越强了。那么请你 思考，激活函数的作用到底是什么? 如果没有激活函数网络会怎样?</a></li><li><a href=#有监督情况下11对于回归问题二分类问题多分类问题通常使用的损失函数有哪些>(有监督情况下）。11)对于回归问题、二分类问题、多分类问题通常使用的损失函数有哪些?</a></li><li><a href=#12-将梯度下降过程表述清楚>(12) 将梯度下降过程表述清楚</a></li><li><a href=#13看我的视频手撕神经网络前向反向求导更参将例子中的-9-个参数分别手撕求导并更新参数>(13)看我的视频”手撕神经网络前向、反向、求导、更参“、将例子中的 9 个参数分别手撕求导，并更新参数。</a></li><li><a href=#14梯度下降随机梯度下降与小批量梯度下降都分别是怎样设计的>(14)梯度下降、随机梯度下降与小批量梯度下降都分别是怎样设计的?</a></li></ol></li><li><a href=#二编程部分>二、编程部分</a><ol><li><a href=#15请你构建一个三行四列由随机数填充的tensor类型变量并通过属性获得它的形状>(15)请你构建一个三行四列由随机数填充的tensor类型变量，并通过属性获得它的形状。</a></li><li><a href=#17这四行代码都用了哪些函数或属性具有什么作用-请进行详细解释>(17)这四行代码都用了哪些函数或属性，具有什么作用? 请进行详细解释。</a></li><li><a href=#18如果将-requires_grad--true-设置为-requires_grad--false-会怎样-这个属性的作用是什么>18如果将 requires_grad = True 设置为 requires_grad = False 会怎样? 这个属性的作用是什么?</a></li><li><a href=#19-zbackward-执行后-都发生了什么>(19) z.backward(). 执行后, 都发生了什么?</a></li><li><a href=#20将我的视频手撕神经网络前向反向求导更参例子对应代码实现>(20)将我的视频”手撕神经网络前向、反向、求导、更参“例子对应代码实现。</a></li></ol></li></ol></nav></details></aside><a href=#一理论部分><h2 id=一理论部分><span class=hanchor arialabel=Anchor># </span>一、理论部分</h2></a><p>有了机器学习的基础知识, 这一周咱们进入人工神经网络的学习。</p><a href=#1人工神经网络-不是深度学习么-二者是什么关系><h3 id=1人工神经网络-不是深度学习么-二者是什么关系><span class=hanchor arialabel=Anchor># </span>(1)人工神经网络? 不是深度学习么? 二者是什么关系?</h3></a><p>人工神经网络是一种模仿人脑神经网络的数学模型，由多个节点（人工神经元）互相连接而成，用来对数据之间的复杂关系进行建模。每个节点代表一种特定函数，来自其他节点的信息经过其相应的权重计算后被传递给下一个节点，以此模拟生物神经网络的工作机制。
深度学习是机器学习的一个分支领域，强调从连续的层（layer）中进行学习，这些层对应于越来越有意义的表示。深度学习是从数据中学习表示的一种新方法，现代深度学习通常包含数十个甚至上百个连续的表示层，这些表示层全都是从训练数据中自动学习的。与其他机器学习方法相比，深度学习的重点往往是学习多个层次的数据表示，从而学会更深层次的理解，因此有时也被称为分层表示学习或层级表示学习。
所以深度学习是通过构建多层神经网络来实现对数据的高层次抽象和复杂模式的提取，从而实现对数据的分类，识别和预测。</p><p>随着人们对自然界的认识, 仿生学也逐渐发展起来, 我们要学习的人工神经网络就是这其中一个产物。1943年, McCulloch与Pitts从对人脑神经元的认识抽象出下图所示的简单模型, 这就是著名的“M-P神经元模型”。在这个模型中, 神经元接收到来自$n$ 个其它神经元传递过来的输入信号, 这些输入信号通过带权重的连接对输入信号进行 线性变换, 神经元接收到的总输入与神经元的阈值进行比较, 然后通过激活函数处理以产生神经元的输出。</p><p><img src=https://www.knowlnk.tech//res/media/2023_05_15_3adf0d30128fda106703g-1.jpg width=auto alt></p><p>注意: 上述模型中$x_{1}, x_{2}, \ldots, x_{n}$ 表示外部刺激, 也可记为$\mathbf{x}$, 也就是模型输入,$w_{1}, w_{2}, \ldots, w_{n}$ 为突触权值, 也就是模型自由参数, 也可记为$\mathbf{w}, b$ 表示阈值。$y$ 代表计算后的输出。函数$f$ 我们下面细说。</p><p>根据上图请尝试回答:</p><a href=#2为什么说m-p神经元将所有信号的输入看做是同时发生的><h3 id=2为什么说m-p神经元将所有信号的输入看做是同时发生的><span class=hanchor arialabel=Anchor># </span>(2)为什么说M-P神经元将所有信号的输入看做是同时发生的。</h3></a><p>M-P神经元是一种最早的神经元模型，它是由 Marvin Minsky 和 Seymour Papert 在 1969 年提出的。M-P神经元（也称为阶跃函数神经元）的输入和输出可以用以下公式表示：</p><p>$$
y=\begin{cases}
1, &\text{if } \sum_{i=1}^n w_i x_i + b \geq 0\\ -1, &\text{otherwise}
\end{cases}
$$</p><p>其中，$x_i$ 是输入向量 $\mathbf{x}$ 的第 $i$ 个分量，$w_i$ 是权重向量 $\mathbf{w}$ 的第 $i$ 个分量，$b$ 是偏置项。</p><p>在 M-P 神经元模型中，所有输入信号被同时处理，因此被视为是同时发生的。这意味着，在一个时间点上，M-P 神经元模型可以接收多个输入信号，并且将它们组合成一个输出信号。如果 $\sum_{i=1}^n w_i x_i + b \geq 0$，则神经元会输出1，否则输出-1。这个判断过程类似于人类大脑中的神经元，当足够的刺激通过突触传递到一个神经元时，该神经元就会被激活，并生成一个输出信号。</p><p>需要注意的是，M-P神经元模型虽然非常简单，但它只能处理线性可分问题，且无法学习非线性决策边界。由于其局限性，M-P神经元很快被更加强大的模型所取代，如感知机、多层感知机等。</p><a href=#3输入的信号在传导的过程中分别做了怎样的计算><h3 id=3输入的信号在传导的过程中分别做了怎样的计算><span class=hanchor arialabel=Anchor># </span>(3)输入的信号在传导的过程中分别做了怎样的计算。</h3></a><p>在M-P神经元模型中，每个神经元接收来自 $n$ 个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接对输入信号进行线性变换，然后将所有的输入信号加权求和，并与神经元的阈值进行比较。</p><p>具体而言，假设第 $i$ 个神经元接收到来自 $n$ 个其他神经元传递过来的输入信号，分别为 $x_1, x_2, &mldr;, x_n$。这些输入信号通过带权重的连接进行线性变换，得到：</p><p>$$z = \sum_{j=1}^{n} w_j x_j $$</p><p>其中，$w_j$ 表示第 $j$ 个神经元到第 $i$ 个神经元的连接权重。</p><p>然后，我们将所有的输入信号加权求和，得到该神经元的总输入：</p><p>$$y = z + b $$</p><p>其中，$b$ 是神经元的阈值（或者说是偏置），用于控制神经元的激活程度。</p><p>最后，我们将总输入 $y$ 传递给激活函数 $f(y)$ 进行处理，得到神经元的输出：</p><p>$$o = f(y) $$</p><p>综上所述，M-P神经元输入的信号在传导的过程中，分别经历了线性变换、加权求和、阈值比较和激活函数处理等计算步骤。这些计算步骤产生了神经元的总输入和输出，用于描述神经元对输入信号的响应和处理方式。</p><p>假设人脑神经元有兴奋和抑制两种状态, 为了模拟类似效果, 现在希望输入信息在经过神经元之后会得到两种输出, “1”和“0”, 分别代表神经元兴奋和神经元抑制。而这里的函数$f$ 的作用就是来完成这件事, 这就是我们常说的 激活函数, 如下图左图所示。但现在这个激活函数具有不连续、不光滑等不太好的性质, 因此实际中常常使用 Sigmoid函数作为激活函数, 其中常用的一种Sigmoid激活为Logistic激活函数如下图右图所示。</p><p><img src=https://www.knowlnk.tech//res/begin_home_work/image2.jpeg width=auto alt></p><a href=#4常见到的激活函数还有relu与leakyrelutanh等请写出各自函数定义><h3 id=4常见到的激活函数还有relu与leakyrelutanh等请写出各自函数定义><span class=hanchor arialabel=Anchor># </span>(4)常见到的激活函数还有Relu与leakyRelu，tanh等，请写出各自函数定义。</h3></a><ol><li>ReLU函数（Rectified Linear Unit）</li></ol><ul><li>函数定义：$f(x) = max(0, x)$</li><li>图像：
<img src=https://www.knowlnk.tech//res/ReLU%20%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.png width=auto alt></li></ul><ol start=2><li>Leaky ReLU函数</li></ol><ul><li>函数定义： $f(x) = \begin{cases} \alpha x & x&lt;0 \\ x & x\geq 0 \end{cases}, \alpha \in [0, 1]$</li><li>图像：</li></ul><p><img src=https://www.knowlnk.tech//res/Leaky%20ReLU.png width=auto alt></p><ol start=3><li>Tanh函数</li></ol><ul><li>函数定义：$f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$</li><li>图像：</li></ul><p><img src=https://www.knowlnk.tech//res/%E5%8F%8C%E6%9B%B2%E6%AD%A3%E5%88%87%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.png width=auto alt></p><p><img src=https://www.knowlnk.tech//res/tanh%20%E5%87%BD%E6%95%B0%E5%92%8C%20sigmoid%20%E5%87%BD%E6%95%B0%E7%9A%84%E6%9B%B2%E7%BA%BF.png width=auto alt></p><a href=#5在python环境下使用numpy编写sgnlogistic激活函数><h3 id=5在python环境下使用numpy编写sgnlogistic激活函数><span class=hanchor arialabel=Anchor># </span>(5)在Python环境下使用Numpy编写Sgn、Logistic激活函数。</h3></a><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Sgn激活函数</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>sgn</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>sign</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Logistic激活函数</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>logistic</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mi>1</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 测试Sgn激活函数</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=o>-</span><span class=mi>5</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>5</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>sgn</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>  <span class=c1># 输出：[-1. -1.  0.  1.  1.]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 测试Logistic激活函数</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=o>-</span><span class=mi>5</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>5</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>logistic</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>  <span class=c1># 输出：[0.00669285 0.26894142 0.5        0.73105858 0.99330715]</span>
</span></span></code></pre></td></tr></table></div></div><p>说到这里, 不得不提一下大名鼎鼎的Rosenblatt感知器。在McCulloch and Pitts (1943) 引入神经网络的概念, 并设计了M-P模型后, Rosenblatt (1958) 提出的感知器则作为神经网络中监督学习的第一个模型。它在M-P模 型的基础上添加了训练方法, 并且Rosenblatt证明了当用来训练感知器的模式 (向量) 取自两个线性可分的类 时, 感知器算法是收敛的。</p><a href=#算法的收敛性证明被称为感知器收敛定理><h3 id=算法的收敛性证明被称为感知器收敛定理><span class=hanchor arialabel=Anchor># </span>算法的收敛性证明被称为感知器收敛定理</h3></a><p>感知器算法的收敛定理是感知器算法的重要理论基础。它表明，当训练集是线性可分的时候，感知器算法可以通过有限次迭代得到一个分类器，使得该分类器对训练集中的所有样本都进行正确分类。</p><p>具体来说，感知器算法的收敛定理可以描述如下：</p><p>假设训练数据集 $T={(\mathbf{x}_1,y_1),(\mathbf{x}_2,y_2),\cdots,(\mathbf{x}_N,y_N)}$ 是线性可分的，即存在某个超平面 $w \cdot x+b=0$ 能够将正负样本完全分开，其中 $\mathbf{x}_i$ 表示第 $i$ 个样本的特征向量，$y_i\in{-1,+1}$ 表示第 $i$ 个样本的真实标签。</p><p>定义误分类点集合 $M$ 表示当前分类器对于训练集中的错误样本的集合，即：</p><p>$$
M={\mathbf{x}_i | y_i(w\cdot\mathbf{x}_i+b) \leq 0}
$$</p><p>则感知器算法在经过有限次迭代后，能够找到一组分类器参数 $w,b$，使得 $M$ 为空集，即对于任意一个样本 $(\mathbf{x}_i,y_i)$ 都有 $y_i(w\cdot\mathbf{x}_i+b)>0$。此时，我们就得到了一个对训练数据集 $T$ 进行完美分类的超平面 $w\cdot x+b=0$。</p><p>感知器收敛定理的证明可以分为以下几个步骤：</p><ol><li><p>假设训练数据集 $T={(\mathbf{x}_1,y_1),(\mathbf{x}_2,y_2),\cdots,(\mathbf{x}_N,y_N)}$ 是线性可分的，即存在某个超平面 $w \cdot x+b=0$ 能够将正负样本完全分开。</p></li><li><p>定义误分类点集合 $M$ 表示当前分类器对于训练集中的错误样本的集合，即：</p></li></ol><p>$$M={\mathbf{x}_i | y_i(w\cdot\mathbf{x}_i+b) \leq 0}$$</p><ol start=3><li><p>假设该超平面的法向量 $\mathbf{w}$ 的范数为 $|\mathbf{w}|_2=1$，并且假设所有误分类样本 $\mathbf{x}_i$ 到超平面的距离为 $\gamma$。由于数据集是线性可分的，因此 $\gamma>0$。</p></li><li><p>在进行第 $t$ 次迭代时，假设我们选取了一个误分类样本 $(\mathbf{x}<em>i,y_i)$ 进行更新，得到新的权重向量 $\mathbf{w}</em>{t+1}$ 和偏置项 $b_{t+1}$。那么，根据感知器算法的更新规则，我们有：</p></li></ol><p>$$\begin{cases}\mathbf{w}_{t+1}=\mathbf{w}_t + \eta y_i \mathbf{x}<em>i \\ b</em>{t+1}=b_t + \eta y_i\end{cases}$$</p><ol start=5><li>我们现在来计算分类器在第 $t+1$ 轮迭代后与真实决策边界之间的距离。假设超平面的参数为 $(\mathbf{w}<em>t,b_t)$，误分类样本 $(\mathbf{x}<em>i,y_i)$ 到超平面的距离为 $\gamma$，则新的超平面的参数为 $(\mathbf{w}</em>{t+1},b</em>{t+1})$。根据向量投影的定义，我们可以将误分类样本的特征向量 $\mathbf{x}_i$ 分解成两个部分：一个垂直于超平面的向量 $\mathbf{v}$，和一个在超平面上的向量 $\mathbf{u}$：</li></ol><p>$$\mathbf{x}_i=\mathbf{v}+\mathbf{u}$$</p><p>其中，$\mathbf{v}$ 的长度等于 $\gamma$，$\mathbf{u}$ 的长度等于 $\sqrt{|\mathbf{x}_i|^2-\gamma^2}$。显然，向量 $\mathbf{v}$ 垂直于超平面，而向量 $\mathbf{u}$ 在超平面上。我们可以将 $\mathbf{w}<em>t$ 和 $\mathbf{w}</em>{t+1}$ 投影到向量 $\mathbf{v}$ 上，得到它们在 $\mathbf{v}$ 方向上的变化量：</p><p>$$
\begin{aligned}
\Delta \mathbf{w} &= \mathbf{w}_{t+1}-\mathbf{w}_t \\ &= \eta y_i \mathbf{x}_i \\ &= \eta y_i (\mathbf{v}+\mathbf{u}) \\ &= \eta y_i \mathbf{v}
\end{aligned}
$$</p><ol start=6><li>我们现在来计算新的分类器与真实决策边界之间的距离。由于超平面的法向量 $\mathbf{w}$ 的长度等于1，因此分类器与真实决策边界之间的距离可以表示为：</li></ol><p>$$
\begin{aligned}
d &= \frac{\mathbf{w}_{t+1}\cdot\mathbf{w}}{|\mathbf{w}|} \\ &= \frac{(\mathbf{w}_t+\Delta \mathbf{w})\cdot\mathbf{w}}{|\mathbf{w}|} \\ &= \frac{\mathbf{w}_t\cdot\mathbf{w}+\Delta \mathbf{w}\cdot\mathbf{w}}{|\mathbf{w}|} \\ &= d_t + \eta y_i |\mathbf{v}|
\end{aligned}
$$</p><p>其中，$d_t$ 表示第 $t$ 轮迭代后分类器与真实决策边界之间的距离。</p><ol start=7><li>我们需要证明的是，在假设 $\mathbf{w}$ 的范数为 $1$ 的情况下，感知器算法能够收敛。因此，我们需要证明分类器与真实决策边界之间的距离随着迭代次数的增加而不断减小，即：</li></ol><p>$$d_{t+1} \leq d_t - \gamma^2 \eta$$</p><p>其中，$\gamma$ 表示所有误分类样本到真实决策边界的距离的最小值。</p><ol start=8><li>因为 $(\mathbf{x}_i,y_i)$ 是一个误分类样本，所以有：</li></ol><p>$$y_i(\mathbf{w}_t\cdot\mathbf{x}_i+b_t) \leq 0$$</p><p>移项并乘以 $\gamma$，得到：</p><p>$$y_i(\mathbf{w}_t\cdot\mathbf{v}+\gamma b_t) \leq 0$$</p><p>因为 $\mathbf{v}$ 垂直于超平面，所以 $\mathbf{w}_t\cdot\mathbf{v}=-b_t$。因此，上式可以进一步化简为：</p><p>$$y_i\gamma(b_t-y_i\mathbf{w}_t\cdot\mathbf{v}) \geq \gamma^2$$</p><p>由于我们选择了一个误分类样本进行更新，因此有 $y_i\mathbf{w}_t\cdot\mathbf{x}_i+y_i b_t \leq 0$。将其代入上式中，得到：</p><p>$$y_i\gamma(b_t-y_i\mathbf{w}_t\cdot\mathbf{v}) \geq y_i\gamma(-\mathbf{w}_t\cdot\mathbf{x}_i) \geq \gamma^2$$</p><p>因此，</p><p>$$b_t-y_i\mathbf{w}_t\cdot\mathbf{v} \geq -\eta\gamma$$</p><p>将上式代入第七步中的公式，得到：</p><p>$$
\begin{aligned}
d_{t+1} &= d_t + \eta y_i |\mathbf{v}| \\ &= d_t - y_i(b_t-y_i\mathbf{w}_t\cdot\mathbf{v}) \\ &\leq d_t + \eta\gamma^2 \\ &= d_t - \gamma^2\eta
\end{aligned}
$$</p><p>这证明了感知器算法的收敛性。由此可知，当训练数据集线性可分时，感知器算法将会在有限次迭代内找到一个能够正确分类所有样本的超平面。</p><a href=#6感知器的收敛算法是怎样的-请用自己语言描述选做><h3 id=6感知器的收敛算法是怎样的-请用自己语言描述选做><span class=hanchor arialabel=Anchor># </span>(6)感知器的收敛算法是怎样的? 请用自己语言描述。(选做)</h3></a><p>在感知机算法中，我们需要将每个样本投影到一个超平面上，判断它们是否被正确分类。为了实现这一目标，我们需要学习一个权重向量 $\mathbf{w}=(w_1,w_2,\cdots,w_n)$，使得对于所有的样本 $\mathbf{x}<em>i=(x</em>{i1},x_{i2},\cdots,x_{in})$，其预测值 $\hat{y}<em>i=\text{sign}(\sum</em>{j=1}^n w_j x_{ij}+b)$ 能够尽可能地接近真实标签 $y_i$。</p><p>在每次迭代中，我们会选取一个误分类点进行更新。具体来说，当一个样本被错误地分类时，即 $y_i\hat{y}_i\leq 0$，我们需要将模型参数进行调整，使得该样本能够被正确分类。其中，$y_i$ 是真实标签，$\hat{y}_i$ 是当前模型对于该样本的预测值。为了实现这一目标，我们需要按照如下公式来更新权重和偏置：</p><p>$$
\begin{aligned}
w_j &\leftarrow w_j + \eta y_i x_{ij}\\ b &\leftarrow b + \eta y_i
\end{aligned}
$$</p><p>其中，$w_j$ 表示第 $j$ 个权重的值，$x_{ij}$ 表示第 $i$ 个样本的第 $j$ 个特征值，$\eta$ 是学习率，表示每次更新的步长大小。$y_i x_{ij}$ 则表示要更新的权重和样本特征值之积，可以看作是对于这个特定特征的梯度。通过反复更新权重和偏置，我们不断逼近最优解，最终得到一个能够正确分类所有样本的超平面。</p><p>具体的过程如下：</p><ol><li><p>选取初值 $w_0, b_0$：这是感知机算法的初始状态。我们需要确定权重和偏置的初始值。一般情况下，可以随机初始化权重和偏置，或者将它们设置为0。</p></li><li><p>在训练集中选取数据 $(x_i, y_i)$：从训练集中随机选择一个样本，并将它作为当前迭代的输入。其中，$x_i=(x_{i1},x_{i2},\cdots,x_{in})$ 是输入特征向量，$y_i\in{-1,+1}$ 是真实标签。</p></li><li><p>如果 $y_i(w \cdot x_i+b) \leq 0$，则将 $w,b$ 更新为：</p><ul><li>更新权重：$w_j \leftarrow w_j + \eta y_i x_{ij}$ 对于所有特征。这个公式表示更新第 $j$ 个权重 $w_j$ 的值。$x_{ij}$ 表示第 $i$ 个样本的第 $j$ 个特征值。</li><li>更新偏置：$b \leftarrow b + \eta y_i$。这个公式表示更新偏置 $b$ 的值。$\eta$ 是学习率，表示每次更新的步长大小。</li></ul></li><li><p>转至步骤 2，直至训练集中没有误分类点：不断重复步骤 2 和 3，直到训练集中所有样本都被正确分类为止。如果训练集线性可分，这个算法会收敛；否则，它会在有限次迭代后停止。</p></li><li><p>输出感知机模型 $f(x)=\operatorname{sign}(w \cdot x+b)$：当训练完成后，我们就可以使用学习到的权重和偏置来进行新样本的预测。具体来说，对于一个输入样本 $\mathbf{x}=(x_1,x_2,\cdots,x_n)$，我们可以计算出其预测结果 $f(\mathbf{x})=\text{sign}(\mathbf{w}\cdot\mathbf{x}+b)$，其中 $\text{sign}$ 是符号函数，表示当输入大于0时输出1，否则输出-1。</p></li></ol><a href=#7代码实现rosenblatt感知器与收敛算法-并使用均方误差-mse-衡量模型性能使用线性可分的双月数据集-进行训练与测试-选做-参考效果如图><h3 id=7代码实现rosenblatt感知器与收敛算法-并使用均方误差-mse-衡量模型性能使用线性可分的双月数据集-进行训练与测试-选做-参考效果如图><span class=hanchor arialabel=Anchor># </span>(7)代码实现Rosenblatt感知器与收敛算法, 并使用均方误差 (MSE) 衡量模型性能。使用线性可分的双月数据集 进行训练与测试。 (选做) 参考效果如图:</h3></a><p><img src=https://www.knowlnk.tech//res/media/2023_05_15_3adf0d30128fda106703g-2.jpg width=auto alt></p><p>提示：scikit-learn 中有直接生成双月数据集的函数方法。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.datasets</span> <span class=kn>import</span> <span class=n>make_moons</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 生成包含1000个样本的双月数据集</span>
</span></span><span class=line><span class=cl><span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>make_moons</span><span class=p>(</span><span class=n>n_samples</span><span class=o>=</span><span class=mi>1000</span><span class=p>,</span> <span class=n>noise</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>123</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>X</span><span class=p>[</span><span class=n>y</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>]</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 构建 Rosenblatt感知器类</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Rosenblatt</span><span class=p>():</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>max_iter</span><span class=o>=</span><span class=mi>10000</span><span class=p>,</span> <span class=n>eta</span><span class=o>=</span><span class=mf>0.1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>max_iter</span> <span class=o>=</span> <span class=n>max_iter</span>  <span class=c1># 最大迭代次数</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>eta</span> <span class=o>=</span> <span class=n>eta</span>  <span class=c1># 学习率</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>fit</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>n_samples</span><span class=p>,</span> <span class=n>n_features</span> <span class=o>=</span> <span class=n>X</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>w</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>n_features</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span>  <span class=c1># 初始化权重向量</span>
</span></span><span class=line><span class=cl>        <span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>insert</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># 将偏置项 b 合并到权重向量 w 中</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>max_iter</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>error_count</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>xi</span><span class=p>,</span> <span class=n>yi</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                <span class=n>update</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>eta</span> <span class=o>*</span> <span class=p>(</span><span class=n>yi</span> <span class=o>-</span> <span class=n>np</span><span class=o>.</span><span class=n>sign</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>xi</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>w</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>w</span> <span class=o>+=</span> <span class=n>update</span> <span class=o>*</span> <span class=n>xi</span>
</span></span><span class=line><span class=cl>                <span class=k>if</span> <span class=n>update</span> <span class=o>!=</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=n>error_count</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>error_count</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=k>break</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>predict</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>insert</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># 添加偏置项</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>sign</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>w</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 训练 Rosenblatt感知器</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>Rosenblatt</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 绘制分类平面（一条线）</span>
</span></span><span class=line><span class=cl><span class=n>plot_x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>1000</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plot_y</span> <span class=o>=</span> <span class=o>-</span> <span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>w</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>+</span> <span class=n>model</span><span class=o>.</span><span class=n>w</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>*</span><span class=n>plot_x</span><span class=p>)</span> <span class=o>/</span> <span class=n>model</span><span class=o>.</span><span class=n>w</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>plot_x</span><span class=p>,</span> <span class=n>plot_y</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;red&#39;</span><span class=p>,</span> <span class=n>linestyle</span><span class=o>=</span><span class=s1>&#39;-&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>c</span><span class=o>=</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p>以上都只是一个神经元, 还不能称之为网络。（但是复杂的神经网络也就是由这种结构的众多神经元相互连接构成。）更重要的是, 一个神经元无法完成像抑或问题这样线性不可分的复杂问题。如下图：</p><p><img src=https://www.knowlnk.tech//res/begin_home_work/image4.jpeg width=auto alt></p><p>在实际中遇到的大多数模式分类问题都是线性不可分问题, 这就使得感知器的用途变得极为狭窄。要解决线性不 可分问题，需要使用多层功能神经元（多于0个隐藏层）。简单的来看就是把许多个神经元排成一排，可以组成一 个单层前馈神经网络, 这个网络的每一个神经元接收相同的数据作为输入, 但每个神经元具有不同的连接权重、 阈值, 这样的一层网络可以输出与神经元一样多数量的值。单层前馈神经网络的输出也可以作为另一个单层神经 网络的输入, 以此相连在一起就组成了规模庞大的神经网络。</p><p><img src=https://www.knowlnk.tech//res/media/2023_05_15_3adf0d30128fda106703g-3.jpg width=auto alt></p><a href=#8那么什么是输入层隐藏层和输出层><h3 id=8那么什么是输入层隐藏层和输出层><span class=hanchor arialabel=Anchor># </span>(8)那么，什么是输入层、隐藏层和输出层?</h3></a><blockquote><p>输入层是数据输入的那一层，而输出层是数据输出的那一层，除此之外的都是隐藏层</p></blockquote><p>注意: 每个层的人工神经元与前一层神经元全连接, 即为全连接神经网络（也存在其它连接形式, 如卷积神经网络的局部连接等）。</p><p>上述异或问题只需要三层全连接的神经网络（包含一个隐藏层的神经网络）即可解决。其中隐藏层与输出层均包 含有非线性的激活函数。如下图所示, 左图为3层全连接神经网络的结构图, 图中标明了连接权重与阈值; 右图表 示对应的决策边界与决策域。</p><p><img src=https://www.knowlnk.tech//res/begin_home_work/image6.jpeg width=auto alt></p><p>这里每一个神经元模型为M-P模型，所以其激活函数为：</p><p>$\operatorname{sgn}(x)= \begin{cases}1, & x \geq 0 \\ 0, & x&lt;0\end{cases}$</p><a href=#9请将上面右图b分类区域中4个点分别按左图a网络参数设置进行计算><h3 id=9请将上面右图b分类区域中4个点分别按左图a网络参数设置进行计算><span class=hanchor arialabel=Anchor># </span>(9)请将上面右图(b)分类区域中4个点分别按左图(a)网络参数设置进行计算。</h3></a><ol><li><p>(0, 1)
第二层 左节点：f(0 x 1 + 1 x (-1)) = f(0 - 1) = f(-1) = 0
第二层 右节点：f(0 x (-1) + 1 x 1) = f(0 + 1) = f(1) = 1
第三层 输出：f(0 x 1 + 1 x 1) = f(1) = 1</p></li><li><p>(1, 1)
第二层 左节点：f(1 x 1 + 1 x (-1)) = f(1 - 1) = f(0) = 0
第二层 右节点：f(1 x (-1) + 1 x 1) = f(0) = 0
第三层 输出：f(1 x 1 + 1 x 1) = f(2) = 0</p></li><li><p>(0, 0)
第二层 左节点：f(0 x 1 + 0 x (-1)) = f(0) = 0
第二层 右节点：f(0 x (-1) + 0 x 1) = f(0) = 0
第三层 输出：f(0 x 1 + 0 x 1) = f(0) = 0</p></li><li><p>(1, 0)
第二层 左节点：f(1 x 1 + 0 x (-1)) = f(1) = 1
第二层 右节点：f(1 x (-1) + 0 x (1)) = f(-1) = 0
第三层 输出：f(1 x 1 + 0 x 1) = f(1) = 1</p></li></ol><a href=#10现在你应当清楚的体会到-模型的节点神经元数量越多-参数越大-模型的拟合能力也就越强了那么请你-思考激活函数的作用到底是什么-如果没有激活函数网络会怎样><h3 id=10现在你应当清楚的体会到-模型的节点神经元数量越多-参数越大-模型的拟合能力也就越强了那么请你-思考激活函数的作用到底是什么-如果没有激活函数网络会怎样><span class=hanchor arialabel=Anchor># </span>(10)现在你应当清楚的体会到, 模型的节点（神经元）数量越多, 参数越大, 模型的拟合能力也就越强了。那么请你 思考，激活函数的作用到底是什么? 如果没有激活函数网络会怎样?</h3></a><blockquote><p>激活函数可以实现非线性变换，这样子能让网络表达的能力更强。如果没有激活函数，那么再多的节点拼接而成的网络最终也只是进行了一次线性变换，也就是矩阵乘法而已。</p></blockquote><p>在你完成第(9)问的过程中, 就像是拿一个已经训练好的模型去进行预测。这里的参数$(\mathbf{w}, \mathbf{b})$ 是已知的, 你只是 将数据输入在这个网络结构里从前往后的进行了计算, 最终获得预测结果。当然你现在计算的结果和右图情况是 相符的, 因为现在你有一个能够很好拟合当前问题的模型。得到这个能够拟合你要解决的问题的模型, 就是我们 进行模型训练的目的。当模型结构已定时, 能不能拟合要解决的问题, 就取决于模型里面参数的取值。所以说人 话就是，训练模型的目的，就是去寻找当前结构下的最优参数。 上次作业讲到训练模型前首先确定三件事：1. 选择一个具有末知参数的模型；2. 定义损失函数；3. 确定优化方 法。</p><p>在这基础上，就可以通过“前向传播(求得损失)&ndash;>反向传播(更新参数)&ldquo;的循环进行来训练模型了。</p><p>求得损失需要通过调用损失函数来进行。损失函数的目的就是看看现在模型的预测情况和真实值的差距有多大</p><a href=#有监督情况下11对于回归问题二分类问题多分类问题通常使用的损失函数有哪些><h3 id=有监督情况下11对于回归问题二分类问题多分类问题通常使用的损失函数有哪些><span class=hanchor arialabel=Anchor># </span>(有监督情况下）。11)对于回归问题、二分类问题、多分类问题通常使用的损失函数有哪些?</h3></a><p>对于回归问题、二分类问题、多分类问题，通常使用的损失函数有：</p><a href=#回归问题><h4 id=回归问题><span class=hanchor arialabel=Anchor># </span>回归问题</h4></a><ol><li><p>均方误差（Mean Squared Error, MSE）：是最常用的回归问题损失函数，计算预测值与真实值之间的平方差，并求其平均值。</p></li><li><p>平均绝对误差（Mean Absolute Error, MAE）：计算预测值与真实值之间的绝对差，并求其平均值。</p></li><li><p>Huber Loss：是一种可以平衡 MSE 和 MAE 的损失函数。它在误差较小的时候采用 MSE，误差较大的时候采用 MAE。</p></li></ol><a href=#二分类问题><h4 id=二分类问题><span class=hanchor arialabel=Anchor># </span>二分类问题</h4></a><ol><li><p>交叉熵损失（Cross Entropy Loss）：这是最常用的损失函数，计算模型输出的概率与真实标签的对数概率的负值。</p></li><li><p>Hinge Loss：适用于支持向量机（Support Vector Machine, SVM）和感知机（Perceptron）等算法，通过最小化错误分类的样本到分类超平面的距离来学习分类器。</p></li><li><p>Focal Loss：解决类别不平衡问题的一个损失函数，弱化易分类的样本的权重，增强难分类的样本的权重，减少易分类的样本对损失函数的贡献。</p></li></ol><a href=#多分类问题><h4 id=多分类问题><span class=hanchor arialabel=Anchor># </span>多分类问题</h4></a><ol><li><p>交叉熵损失（Cross Entropy Loss）：同样也是最常用的损失函数，但针对多分类问题，计算模型输出的概率与真实标签的对数概率的负值。</p></li><li><p>多项式分布损失（Multinomial Distribution Loss）：同样针对多分类问题，基于多项式分布建立损失函数，并通过最大化似然函数来最小化损失。</p></li><li><p>Kullback-Leibler 散度损失（Kullback-Leibler Divergence Loss）：也称为相对熵损失，可以度量两个概率分布之间的距离，用于多分类问题中的类别分布预测和真实类别分布之间的匹配程度。</p></li></ol><p>下面就该重头戏”反向传播算法“出场了。</p><p>反向传播 (Backpropagation, BP) 是“误差反向传播”的简称, 是一种与最优化方法（如梯度下降法）结合使用 的, 用来训练人工神经网络的常见方法。该方法对网络中所有权重计算损失函数的梯度。这个梯度会反馈给最优 化方法, 用来更新权值以最小化损失函数。</p><p>人话翻译: 在前向传播获得预测之后, 就可以通过损失函数求得损失。求得损失之后, 我们希望能够对参数进行 调整，使得损失能够在当前情况下变小一点, 也就是往让损失减小的方向调整一次参数。这个让损失减小的方 向, 就可以通过求得梯度来解决。更新的基本方法为：对于一个参数$\mathrm{w}$, 求得它的梯度$\mathrm{d}$ 后, 让它往梯度的反方向 以指定步长a (学习率) 走一步。也就是$\mathrm{w}(\mathrm{new})=\mathrm{w}-\mathrm{a} * \mathrm{~d}$ ，这里的a是超参数。</p><p>注意！根本之处在于：损失本来就是一个关于参数$\mathbf{w}$ 和$\mathbf{b}$ 的函数！</p><a href=#12-将梯度下降过程表述清楚><h3 id=12-将梯度下降过程表述清楚><span class=hanchor arialabel=Anchor># </span>(12) 将梯度下降过程表述清楚</h3></a><p>好的，我会详细地说明每个步骤的含义和具体操作，帮助初学者更好地理解梯度下降算法。</p><ol><li>选择一个初始点作为起始点</li></ol><p>在梯度下降算法中，我们需要选择一个初始点作为起始点来开始求解函数的最小值。这个初始点可以是随机选择的，也可以是根据经验预设的。它的选取可能会影响到最终结果的收敛速度和质量，因此需要进行一定的实验和调试来确定最佳的初始点。</p><p>例如，在训练神经网络时，我们通常会使用一些常见的初始化方法来设置模型的初始参数。例如，对于一个拥有 $n$ 个输入和 $m$ 个输出的全连接层，我们可以按照高斯分布或均匀分布初始化权重 $W$ 和偏置 $b$：</p><ul><li>高斯分布：$W_{i,j}\sim N(0, \sigma^2)$，其中 $\sigma=\sqrt{\frac{2}{n+m}}$</li><li>均匀分布：$W_{i,j}\sim U(-\frac{\sqrt{6}}{\sqrt{n+m}}, \frac{\sqrt{6}}{\sqrt{n+m}})$</li></ul><p>这样的初始化方法可以帮助我们避免陷入局部极小值或者梯度消失/爆炸等问题，并提高模型的泛化能力和性能。</p><ol start=2><li>计算当前点对应的代价函数关于各个参数的导数（即梯度）</li></ol><p>在梯度下降算法中，我们需要计算当前点对应的代价函数关于各个参数的导数（即梯度），以确定函数在该点的变化方向。这个计算过程需要使用微积分知识来求解，对初学者来说可能有一定难度。</p><p>例如，在训练神经网络时，我们需要使用反向传播算法来计算代价函数关于模型参数的梯度，从而利用梯度下降算法来更新参数。反向传播算法的主要思想是利用链式法则，将代价函数的梯度逐层传递给前面的层，从而计算出每个参数对代价函数的贡献。</p><p>具体来说，我们可以按照以下步骤来计算代价函数关于某个参数的梯度：</p><ul><li>正向传播：将输入数据送入神经网络，得到输出结果。</li><li>计算代价函数：根据输出结果和真实标签计算代价函数。</li><li>反向传播：计算代价函数关于输出结果的梯度，然后逐层计算出每个参数对代价函数的梯度。</li><li>更新参数：沿着梯度的反方向更新模型的参数，使得代价函数的值逐渐降低。</li></ul><p>通过不断地迭代，我们可以逐步优化模型的参数，从而提高模型的性能和准确率。</p><ol start=3><li>按照负梯度方向更新当前点的位置，即让当前点沿着梯度的反方向移动一定的距离</li></ol><p>在梯度下降算法中，我们需要根据当前点对应的梯度方向，按照一定的步长（即学习率）来更新当前点的位置，使得函数值逐渐减小。这个更新过程可以理解为“爬山”或“下山”的过程，即沿着梯度的反方向不断前进，直到达到最小值或者停止条件。</p><p>具体来说，我们可以按照以下公式来更新当前点的位置：</p><p>$$
x_{i+1}=x_i - \eta \nabla f(x_i)
$$</p><p>其中，$x_i$ 表示第 $i$ 次迭代的当前点，$\eta$ 表示学习率（即步长），$\nabla f(x_i)$ 表示在当前点 $x_i$ 处的梯度。这个公式表示了当前点沿着梯度的反方向移动一定的距离，从而让函数值逐渐减小。</p><p>例如，在训练神经网络时，我们可以使用如下公式来更新模型的参数：</p><p>$$
w_{i+1}=w_i - \eta \frac{\partial \mathcal{L}}{\partial w_i}
$$</p><p>其中，$w_i$ 表示第 $i$ 次迭代的模型参数，$\eta$ 表示学习率，$\mathcal{L}$ 表示损失函数。根据梯度下降算法的定义，我们需要让损失函数逐渐降低，因此需要按照负梯度方向来更新模型的参数。</p><ol start=4><li>重复执行步骤2和步骤3，直到满足停止条件</li></ol><p>在梯度下降算法中，我们需要重复执行步骤2和步骤3，直到满足停止条件为止。停止条件可以是达到一定的迭代次数、达到一定的精度要求、出现震荡或者发散等情况等等。</p><p>例如，在训练神经网络时，我们通常会将数据集分成若干个batch，然后对每个batch进行多次迭代，直到达到一定的迭代次数或者达到一定的精度要求。此外，我们还可以使用一些实用技巧来提高梯度下降算法的效果，例如加入动量、自适应学习率等。</p><p>总之，梯度下降算法是深度学习中最基础、最为重要的优化算法之一，初学者需要理解其基本原理和流程，并结合具体实例进行掌握和应用。</p><p>学到这里, 你应该已经清楚了神经网络的训练过程。下一步的问题就是搞定梯度值。</p><p>高数那里讲的东西咱们这里就不再细说, 我只想说, 为了求导方便, 为了计算机计算方便, 这里使用链式法则来 进行求导, 也就是会从损失反向往前求得参数节点的导数。</p><a href=#13看我的视频手撕神经网络前向反向求导更参将例子中的-9-个参数分别手撕求导并更新参数><h3 id=13看我的视频手撕神经网络前向反向求导更参将例子中的-9-个参数分别手撕求导并更新参数><span class=hanchor arialabel=Anchor># </span>(13)看我的视频”手撕神经网络前向、反向、求导、更参“、将例子中的 9 个参数分别手撕求导，并更新参数。</h3></a><p>当输入为x1 = 1，x2 = 2时预测值y = 5但是正确的y为2：
<img src=https://www.knowlnk.tech//res/2_%E7%AE%80%E5%8D%95%E7%9A%84%E7%BD%91%E7%BB%9C%201.jpg width=auto alt></p><p>我们考虑一个仅包含一个隐藏层的神经网络，其中输入层有2个节点（即$x_1$和$x_2$），隐藏层有2个节点，输出层只有1个节点。偏置 $b$ 在隐藏层和输出层中都为1。</p><p>我们定义以下变量：</p><ul><li>隐藏层的权重：$w_{11}, w_{12}, w_{21}, w_{22}$</li><li>隐藏层的偏置：$b_{11},b_{12}$</li><li>输出层的权重：$w_{1}, w_{2}$</li><li>输出层的偏置：$b_{21}$</li></ul><p>我们可以将前向传播计算过程分成两步:</p><a href=#步骤1计算隐藏层><h4 id=步骤1计算隐藏层><span class=hanchor arialabel=Anchor># </span>步骤1：计算隐藏层</h4></a><p>$$z_1 = w_{11}x_1 + w_{12}x_2 + b_{11}$$
$$a_1 = \sigma(z_1)$$</p><p>$$z_2 = w_{21}x_1 + w_{22}x_2 + b_{12}$$
$$a_2 = \sigma(z_2)$$</p><p>其中 $\sigma$ 是激活函数，这里使用 ReLU 函数。</p><a href=#步骤2计算输出层><h4 id=步骤2计算输出层><span class=hanchor arialabel=Anchor># </span>步骤2：计算输出层</h4></a><p>$$y = w_{1}a_1 + w_{2}a_2 + b_{21}$$</p><p>此处没有使用激活函数。</p><p>现在我们需要根据给定的损失函数$L(y,\hat{y})=\frac{1}{2}(y-\hat{y})^2$，对模型参数进行反向传播以进行训练。</p><a href=#反向传播><h4 id=反向传播><span class=hanchor arialabel=Anchor># </span>反向传播</h4></a><p>反向传播的目标是计算代价函数相对于各个变量的导数，以便我们可以使用这些导数来更新每个变量的值。我们从代价函数开始，通过链式法则，逐层计算导数。</p><p>首先计算 $L$ 相对于 $y$ 的导数。</p><p>$$\frac{\partial L}{\partial y} = (y-\hat{y}) = 3$$</p><p>接下来计算 $a_1$ 的导数。</p><p>$$\frac{\partial L}{\partial a_1} = w_1 \frac{\partial L}{\partial y} = 1 \times 3 = 3$$</p><p>然后，计算 $z_1$ 和 $w_{11}$ 的导数。</p><p>$$\frac{\partial L}{\partial z_1} = \frac{\partial L}{\partial a_1}\frac{\partial a_1}{\partial z_1} = 3 \times [z_1 > 0] = 3$$</p><p>其中 $[z_1 > 0]$ 表示当 $z_1 > 0$ 时取值为1，否则为0。</p><p>$$\frac{\partial L}{\partial w_{11}} = x_1 \frac{\partial L}{\partial z_1} = 1 \times 3 = 3$$</p><p>现在我们可以使用这些导数来更新模型参数 $w_{11}$</p><a href=#参数更新><h4 id=参数更新><span class=hanchor arialabel=Anchor># </span>参数更新</h4></a><p>假设我们选择学习率为 $\eta = 0.01$，则可以按照以下方式更新参数：</p><p>$$w_{11} = w_{11} - \eta \frac{\partial L}{\partial w_{11}} = 1 - 0.01 \times 3 = 0.97$$</p><a href=#14梯度下降随机梯度下降与小批量梯度下降都分别是怎样设计的><h3 id=14梯度下降随机梯度下降与小批量梯度下降都分别是怎样设计的><span class=hanchor arialabel=Anchor># </span>(14)梯度下降、随机梯度下降与小批量梯度下降都分别是怎样设计的?</h3></a><ul><li><p>梯度下降（Gradient Descent，GD）：在每次迭代中，计算所有训练样本的损失函数对于模型参数的梯度，然后更新模型参数。具体来说，将当前模型参数 $w$ 沿着负梯度方向移动一定步长 $\eta$，即 $w \leftarrow w - \eta \nabla_w L(w)$，其中 $\nabla_w L(w)$ 表示损失函数 $L(w)$ 对于参数 $w$ 的梯度。梯度下降方法可以保证在所有训练样本上最小化损失函数，但每次迭代需要处理大量数据，所以计算效率低下。</p></li><li><p>随机梯度下降（Stochastic Gradient Descent，SGD）：在每次迭代中，随机选择一个训练样本计算梯度，并更新模型参数。与梯度下降不同，SGD 每次只用一个样本计算梯度，所以速度更快。但由于随机性，SGD 更新方向不一定很好地表示了整个训练集的整体趋势，因此可能会导致损失函数收敛到局部最小值而非全局最小值。</p></li><li><p>小批量梯度下降（Mini-batch Gradient Descent，MBGD）：在每次迭代中，随机选择一个大小为 $b$ 的小批量（mini-batch）样本计算梯度，并更新模型参数。与 SGD 不同，MBGD 每次使用多个样本计算梯度，得到的更新方向更有代表性，同时运算速度也比 GD 更快。小批量梯度下降是目前深度学习中最常用的优化算法之一，通常 $b$ 的取值范围在几十到几百之间。</p></li></ul><a href=#二编程部分><h2 id=二编程部分><span class=hanchor arialabel=Anchor># </span>二、编程部分</h2></a><p>这次我们开始说PyTorch。</p><p>通常对于PyTorch的导入写成这样:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span></code></pre></td></tr></table></div></div><p>PyTorch中的数据类型是tensor。类似NumPy中ndarray, 唯一区别是tensor可以在GPU上运算。</p><a href=#15请你构建一个三行四列由随机数填充的tensor类型变量并通过属性获得它的形状><h3 id=15请你构建一个三行四列由随机数填充的tensor类型变量并通过属性获得它的形状><span class=hanchor arialabel=Anchor># </span>(15)请你构建一个三行四列由随机数填充的tensor类型变量，并通过属性获得它的形状。</h3></a><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 构建一个形状为 (3, 4) 的张量变量并随机初始化</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用 shape 属性获取张量的形状</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[</span><span class=mf>1.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>],[</span><span class=o>-</span><span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>]],</span><span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>z</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span> 
</span></span><span class=line><span class=cl><span class=n>z</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>x</span><span class=o>.</span><span class=n>grad</span>
</span></span></code></pre></td></tr></table></div></div><a href=#17这四行代码都用了哪些函数或属性具有什么作用-请进行详细解释><h3 id=17这四行代码都用了哪些函数或属性具有什么作用-请进行详细解释><span class=hanchor arialabel=Anchor># </span>(17)这四行代码都用了哪些函数或属性，具有什么作用? 请进行详细解释。</h3></a><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[</span><span class=mf>1.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>],[</span><span class=o>-</span><span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>]],</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>z</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>z</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>x</span><span class=o>.</span><span class=n>grad</span>
</span></span></code></pre></td></tr></table></div></div><p>这四行代码中使用了以下函数和属性：</p><ol><li><p><code>torch.tensor()</code>：用于创建一个张量对象，其可接受各种输入类型（如Python列表、NumPy数组等），并将其转换为PyTorch张量。</p></li><li><p><code>.pow(2)</code>：计算张量x的平方，返回一个新的张量。在本例中，它计算了x的每个元素的平方。</p></li><li><p><code>.sum()</code>：计算张量所有元素的总和，并返回一个标量值。</p></li><li><p><code>.backward()</code>：计算与给定张量相关的梯度。在此示例中，我们调用backward()方法，计算z相对于x的梯度。</p></li><li><p><code>.grad</code>：返回一个保存了计算出的梯度的张量。在这个例子中，我们访问了张量x的grad属性，它包含相对于x的梯度值。</p></li></ol><a href=#18如果将-requires_grad--true-设置为-requires_grad--false-会怎样-这个属性的作用是什么><h3 id=18如果将-requires_grad--true-设置为-requires_grad--false-会怎样-这个属性的作用是什么><span class=hanchor arialabel=Anchor># </span>18如果将 requires_grad = True 设置为 requires_grad = False 会怎样? 这个属性的作用是什么?</h3></a><blockquote><p><code>requires_grad</code>在PyTorch中是一个张量（Tensor）的属性，用于指定是否对该张量进行自动求梯度。如果将requires_grad属性设置为True，则表示希望跟踪所有依赖于该变量的操作，并在计算时自动计算梯度。如果requires_grad属性设置为False，则表示不需要计算相对于该张量的梯度。</p></blockquote><blockquote><p>在给定代码中，我们创建了一个2x2的张量x，并将其requires_grad属性设置为True。然后对x的元素平方并求和得到标量z。接下来，我们调用z.backward()方法计算z相对于x的梯度。最后，我们可以通过访问x.grad属性来获得计算出的梯度。</p></blockquote><blockquote><p>总之，requires_grad属性允许我们控制哪些张量需要计算梯度以及哪些张量不需要计算梯度，这对于训练深度学习模型非常有用。</p></blockquote><a href=#19-zbackward-执行后-都发生了什么><h3 id=19-zbackward-执行后-都发生了什么><span class=hanchor arialabel=Anchor># </span>(19) z.backward(). 执行后, 都发生了什么?</h3></a><blockquote><p>执行之后会计算得到对应的梯度，但是此时并没有进行相关的参数更新，需要手动更新</p></blockquote><a href=#20将我的视频手撕神经网络前向反向求导更参例子对应代码实现><h3 id=20将我的视频手撕神经网络前向反向求导更参例子对应代码实现><span class=hanchor arialabel=Anchor># </span>(20)将我的视频”手撕神经网络前向、反向、求导、更参“例子对应代码实现。</h3></a><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>x1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=c1># 当前输入的特征 x1，因为只是输入，所以不需要计算梯度</span>
</span></span><span class=line><span class=cl><span class=n>x2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span> <span class=c1># 当前输入的特征 x2，因为只是输入，所以不需要计算梯度</span>
</span></span><span class=line><span class=cl><span class=n>Y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span> <span class=c1># 当前输入对应的真实值 Y，因为只是输出，所以不需要计算梯度</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>w11</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=mf>1.</span><span class=p>,</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> <span class=c1># 参数 w11，需要计算梯度</span>
</span></span><span class=line><span class=cl><span class=n>w12</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=mf>1.</span><span class=p>,</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> <span class=c1># 参数 w12，需要计算梯度</span>
</span></span><span class=line><span class=cl><span class=n>w21</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=mf>1.</span><span class=p>,</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> <span class=c1># 参数 w21，需要计算梯度</span>
</span></span><span class=line><span class=cl><span class=n>w22</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=o>-</span><span class=mf>1.</span><span class=p>,</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> <span class=c1># 参数 w22，需要计算梯度</span>
</span></span><span class=line><span class=cl><span class=n>w1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=mf>1.</span><span class=p>,</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> <span class=c1># 参数 w1，需要计算梯度</span>
</span></span><span class=line><span class=cl><span class=n>w2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=o>-</span><span class=mf>1.</span><span class=p>,</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> <span class=c1># 参数 w2，需要计算梯度</span>
</span></span><span class=line><span class=cl><span class=n>b11</span><span class=p>,</span> <span class=n>b12</span><span class=p>,</span> <span class=n>b21</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>],</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> <span class=c1># 给定偏置 b11、b12 和 b21，需要计算梯度</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>lr</span> <span class=o>=</span> <span class=mf>0.01</span> <span class=c1># 定义学习率</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 前向传播</span>
</span></span><span class=line><span class=cl><span class=n>z1</span> <span class=o>=</span> <span class=n>w11</span> <span class=o>*</span> <span class=n>x1</span> <span class=o>+</span> <span class=n>w12</span> <span class=o>*</span> <span class=n>x2</span> <span class=o>+</span> <span class=n>b11</span> <span class=c1># 第一层神经元输入加权和 z1</span>
</span></span><span class=line><span class=cl><span class=n>a1</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>z1</span><span class=p>)</span> <span class=c1># 第一层神经元输出 a1（经过 ReLU 函数激活）</span>
</span></span><span class=line><span class=cl><span class=n>z2</span> <span class=o>=</span> <span class=n>w21</span> <span class=o>*</span> <span class=n>x1</span> <span class=o>+</span> <span class=n>w22</span> <span class=o>*</span> <span class=n>x2</span> <span class=o>+</span> <span class=n>b12</span> <span class=c1># 第二层神经元输入加权和 z2</span>
</span></span><span class=line><span class=cl><span class=n>a2</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>z2</span><span class=p>)</span> <span class=c1># 第二层神经元输出 a2（经过 ReLU 函数激活）</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>w1</span> <span class=o>*</span> <span class=n>a1</span> <span class=o>+</span> <span class=n>w2</span> <span class=o>*</span> <span class=n>a2</span> <span class=o>+</span> <span class=n>b21</span> <span class=c1># 输出层输入加权和 y</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 求损失</span>
</span></span><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=p>(</span><span class=n>y</span> <span class=o>-</span> <span class=n>Y</span><span class=p>)</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span> <span class=o>/</span> <span class=mi>2</span> <span class=c1># 计算当前模型预测值 y 与真实值 Y 的平方差</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 求参数梯度</span>
</span></span><span class=line><span class=cl><span class=n>w11_grad</span> <span class=o>=</span> <span class=p>(</span><span class=n>y</span> <span class=o>-</span> <span class=n>Y</span><span class=p>)</span> <span class=o>*</span> <span class=n>w1</span> <span class=o>*</span> <span class=mi>1</span> <span class=o>*</span> <span class=n>x1</span> <span class=c1># 手动计算参数 w11 的梯度</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span> <span class=c1># 自动计算所有参数的梯度</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 打印输出手动计算的梯度、自动计算的梯度以及它们是否相等</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>w11_grad</span><span class=p>,</span> <span class=n>w11</span><span class=o>.</span><span class=n>grad</span><span class=p>,</span> <span class=n>w11_grad</span> <span class=o>==</span> <span class=n>w11</span><span class=o>.</span><span class=n>grad</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>w11</span> <span class=o>=</span> <span class=n>w11</span> <span class=o>-</span> <span class=n>lr</span> <span class=o>*</span> <span class=n>w11_grad</span> <span class=c1># 更新参数 w11</span>
</span></span></code></pre></td></tr></table></div></div><p>最后, 有余力的同学去看看PyTorch中的计算图是怎么回事, 这会让你对自动微分这件事儿更通透, 也会让你在网络的搭建过程中更加明朗。</p></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://www.knowlnk.tech/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Wu Chenyue using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2023</p><ul><li><a href=https://www.knowlnk.tech/>Home</a></li><li><a href=https://github.com/rootyue>GitHub</a></li></ul></footer></div></div></body></html>