---
title: "机器学习基础作业班 第四周"
tags:
- 机器学习
---

如果在上次的作业中, 你使用PyTorch实现了LeNet网络的定义的话, 你应当已经知道一个网络的定义过程了。把 对应的功能封装成函数后根据你对网络的设计将这些零件拼装在一起, 现在我们会用到全连接层、卷积层、池化 层和激活函数, 之后随着网络设计的复杂, 你会见到更多的零件, 比如dropout、Batch normalization等等。所以 先学理论, 搞清楚这些零件的作用, 以及网络的结构, 用代码实现只是分分钟的事儿。

以下是一个可以用来实现LeNet网络的class, 目的是之后实例出这个网络去跑MNIST数据集。上次有同学说定义 网络不可思议, 你看看是不是我说的零件拼装, 搞定这个以后理论上你可以尝试自己去构建你的网络了。
```python
import torch
import torch.nn as nn 
class LeNet(nn.Module):
    def __init__(self):
        super(LeNet, self).__init__()
        self.conv1 = nn.Conv2d(1, 6, 5)
        self.pool1 = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.pool2 = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(16 * 4 * 4, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)
    def forward(self, x):
        x = self.pool1(torch.sigmoid(self.conv1(x)))
        x = self.pool2(torch.sigmoid(self.conv2(x)))
        x = x.view(-1, 16 * 4 * 4)
        x = torch.sigmoid(self.fc1(x))
        x = torch.sigmoid(self.fc2(x))
        x = self.fc3(x)
    return x
```

参考上例回答


### (1) init (self). 部分定义了什么?

  

### (2) forward (self, x ) 部分定义了什么?

  

### (3) nn. Linear 的作用是什么?

  

### (4) x.view \((-1,16 * 4 * 4)\) 的作用是什么? 为什么需要这么做?

  

### (5)怎么是" 16 x 4 x4"?论文的结构图中不是" 16 x 5 x5"? 为什么会换?

  

这段代码里面涉及到了Python类的定义和初始化方法等基础知识, 不了解的话去看Python的函数

这段代码⾥⾯涉及到了Python类的定义和初始化⽅法等基础知识，不了解的话去看Python的函数 https://docs.python.org/zh-cn/3/tutorial/controlﬂow.html#deﬁning-functions 与类 https://docs.python.org/zh-cn/3/tutorial/classes.html 这⼀部分。  模型被定义好后，使⽤之前需要先实例出来，⽐如


```python
net = LeNet()
```


下面来看看一个网络的训练是怎么进行的。
  

先来准备好数据, 现在以MNIST数据集为例。


```python
import numpy as np
train_X, test_X, train_y, test_y = np.load( './mnist.npy', allow_pickle=True)
x_train = train_X.reshape(60000, 28*28).astype(np.float32) 
x_test = test_X.reshape(10000, 28*28).astype(np.float32)
```  

现在我们要把数据集处理一下再用作训练, 对于当前数据集, 需要有以下几步：
  
1. 转换成 tensor 类型
2. 再转换成 Dataset 类型
3. 再生成 DataLoader

这几步是先后依赖的。我们想用小批量随机下降的方法来进行模型的训练, 所以需要借助 DataLoader 来进行 数据的批量加载-> DataLoader 在创建的时候需要数据是 Dataset 类型-> Dataset 类型的创建需要数据是 tensor 类型。


下面具体来说, 首先将 NumPy 加载进来的数据类型 ndarray 转换成 tensor 类型:

```python
x_train, y_train, x_test, y_test = map(torch.tensor, (x_train, train_y, x_test, test_y))
```
  
### (6) map 函数Python中很常使用到，在这里的作用是什么?


下面是生成 Dataset 类。

```python
from torch.utils.data import TensorDataset
```
  
x_train 和     y_train 都可以合并为⼀个  TensorDataset ，这将更易于迭代和切⽚。

```python
train_ds = TensorDataset(x_train, y_train)
```
  

在我们自己定义数据集的时候, 可以使用 PyTorch 的 TensorDataset 来实现这一步, 可是(7)我们为什么需 要这么做?


除此之外, Pytorch 的 DataLoader 负责批次管理。需要从 Dataset 创建一个 DataLoader。 DataLoader 使迭代变得更加容易, 可以自动为我们提供随机打乱的每个小批量。

```python
from torch.utils.data import DataLoader 
bs = 64
train_ds = TensorDataset(x_train, y_train)
train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)
```
  

现在, 训练时使用的数据加载器已经处理好了, 来写一个训练过程。


补充:

  

1. 通常管所有数据扫完一遍叫做一个epoch。

```python
epochs = 32
```

2. Pytorch 还提供了一个包含各种优化算法的软件包 torch.optim。我们可以使用优化器中的step方法来进 行参数的更新 (注意：loss.backward() 只是求得了每一个参数的梯度并且记录在参数对应的节点上, 但 是参数并不更新)。

```python
from torch import optim
opt = optim.SGD(net.parameters(), lr=0.01) # 定义⼀个优化器
```


3. 学习率记作 Ir

  
```python
lr = 0.01
```
  

4. 损失函数也提前准备好

```python
import torch.nn.functional as F loss_func = F.cross_entropy
```
  

现在开始训练吧。

  
```python
for epoch in range(epochs):
    for xb, yb in train_dl:
        xb = torch.reshape(xb, [-1, 1, 28, 28])
        pred = net(xb)
        loss = loss_func(pred, yb)
        loss.backward()
        opt.step()
        opt.zero_grad()
        print(epoch, loss)
```
  

### 上例中, (8) torch.reshape (xb，\[-1，1，28，28\]_). 的作用是什么?

  

### (9) opt. zero_grad() 的作用是什么? 为什么需要有这一步?

  

### (10)尝试对Irlbslepochs进行调整再观察训练过程。

  

LeNet 就说到这里, 下面来看另一个经典网络 "AlexNet"。相对于LeNet, AlexNet做的改进有:

  

1. 更深的网络结构。AlexNet使用了比LeNet更深的网络结构, 包含5层卷积层和3层全连接层, 这样可以提取更 高级别的特征, 进而提高模型的准确率。

2. ReLU激活函数的使用。AlexNet在每个卷积层之后都使用了ReLU激活函数, 这个激活函数相比于sigmoid激 活函数可以大大减少梯度消失的问题, 进而训练更深的神经网络。

3. Dropout。AlexNet使用Dropout技术在全连接层中加入随机丟弃部分神经元的操作, 来防止过拟合, 提高模 型的泛化能力。

![](res/AlexNet模型结构图.png)

  

### (11)用到的新零件有哪些? 如何定义?

  

### (12)请尝试定义AlexNet网络, 然后将上述过程对应实现



### (13)在同样的数据集, bs和Ir等超参数设置下, 两个网络的训练效果差别是怎样的? 下面进行下一个网络, 何恺明大神的"ResNet"。

![](res/ResNet中的残差(residual)结构.png)

这绝对称得上是"中国之光", 这个网络的出现, 让深度学习网络模型真正可以变深了。请参考我的视频"ResNet 残差神经网络"回答下列问题:

  

### (14)请画出通道数有变化和无变化两种情况下的残差块结构。

  

### (15)在无通道数变化情况下，进入残差块的数据是x，那么输出可以表示为什么? 中间的计算过程是怎样的?

  

### (16)引残差块解决的是什么问题? 请用公式推倒证明。

  

### (17)对于ResNet你还有哪些认识? (背景、作者、影响、延伸等方向不限)

  

如果你有信心, 也可以自己去定义这个网络, 这里就不作要求了。但如果你想使用这个网络, 可以在PyTorch中直 接下载调用预训练好的模型。

  

### (18)如何下载调用? 请下载后重新对MNIST数据集进行训练。

  

上面的模型大多用来对图片进行分类, 下面来说图像处理中的另一个主要任务：目标检测。

  

### (19)相对于图片分类来说, 目标检测是怎样的任务?

  

目标检测中有两种常见解决思路, two stages 和 one stage。

  

其中R-CNN网络可以看做是 two stages 的代表, 后续还有Fast-RCNN和Faster-RCNN。

![](res/R_CNN_regions_with_cnn_features.png)

看我的视频"R-CNN的设计思想"后, 20简述"R-CNN的设计思想"



one stage 的代表当然就是你一定听过的YOLO模型了, 我们来以YOLOv1为例进行学习。

![](res/yolov1_模型结构.png)
  

### (21)测试过程中, 网络的输入图像是多大?

  

### (22)输出的是 \(7 x 7 x 30\) 的张量, 每一个位置的数值分别代表什么意义?

  

### (23)参考下图解释, 模型预测出来的框是98个, 之后经过哪些处理才变为下图中的3个框?

![](res/yolov1模型预测结果.png)
  

(24)训练过程中, 选用的损失函数是怎样的? 请详细解释。


造不造轮子这个看个人, 但是使用轮子的话, 把具体内容了解清楚了也肯定会收益。强烈建议有余力的同学, 将 本次内容中涉及到的所有论文进行精读, 他们都非常经典。