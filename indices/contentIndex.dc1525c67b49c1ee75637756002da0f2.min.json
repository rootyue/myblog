{"/":{"title":"寻剑之路","content":"\n测试一下\n\n[作业训练营 第一周](notes/machine_leaning/begin_home_work/作业训练营%20第一周.md)\n\n[[notes/games101/Lecture 02 Review of Linear Algebra]]\n\n[[notes/hosting]]\n\nHost your second brain and [digital garden](https://jzhao.xyz/posts/networked-thought) for free. Quartz features\n\n1. Extremely fast natural-language [[notes/search]]\n2. Customizable and hackable design based on [Hugo](https://gohugo.io/)\n3. Automatically generated backlinks, link previews, and local graph\n4. Built-in [[notes/CJK + Latex Support (测试) | CJK + Latex Support]] and [[notes/callouts | Admonition-style callouts]]\n5. Support for both Markdown Links and Wikilinks\n\nCheck out some of the [amazing gardens that community members](notes/showcase.md) have published with Quartz or read about [why I made Quartz](notes/philosophy.md) to begin with.\n\n## Get Started\n\u003e 📚 Step 1: [Setup your own digital garden using Quartz](notes/setup.md)\n\nReturning user? Figure out how to [[notes/updating|update]] your existing Quartz garden.\n\nIf you prefer browsing the contents of this site through a list instead of a graph, you see a list of all [setup-related notes](/tags/setup).\n\n### Troubleshooting\n- 🚧 [Troubleshooting and FAQ](notes/troubleshooting.md)\n- 🐛 [Submit an Issue](https://github.com/jackyzha0/quartz/issues)\n- 👀 [Discord Community](https://discord.gg/cRFFHYye7t)\n\n","lastmodified":"2023-06-07T15:55:55.946774843Z","tags":[]},"/notes/CJK-+-Latex-Support-%E6%B5%8B%E8%AF%95":{"title":"CJK + Latex Support (测试)","content":"\n## Chinese, Japanese, Korean Support\n几乎在我们意识到之前，我们已经离开了地面。\n\n우리가 그것을 알기도 전에 우리는 땅을 떠났습니다.\n\n私たちがそれを知るほぼ前に、私たちは地面を離れていました。\n\n## Latex\n\nBlock math works with two dollar signs `$$...$$`\n\n$$f(x) = \\int_{-\\infty}^\\infty\n    f\\hat(\\xi),e^{2 \\pi i \\xi x}\n    \\,d\\xi$$\n\t\nInline math also works with single dollar signs `$...$`. For example, Euler's identity but inline: $e^{i\\pi} = -1$\n\nAligned equations work quite well:\n\n$$\n\\begin{aligned}\na \u0026= b + c \\\\ \u0026= e + f \\\\\n\\end{aligned}\n$$\n\nAnd matrices\n\n$$\n\\begin{bmatrix}\n1 \u0026 2 \u0026 3 \\\\\na \u0026 b \u0026 c\n\\end{bmatrix}\n$$\n\n## RTL\nMore information on configuring RTL languages like Arabic in the [config](notes/config.md) page.\n","lastmodified":"2023-06-07T15:55:55.950775144Z","tags":[]},"/notes/callouts":{"title":"Callouts","content":"\n## Callout support\n\nQuartz supports the same Admonition-callout syntax as Obsidian.\n\nThis includes\n- 12 Distinct callout types (each with several aliases)\n- Collapsable callouts\n\nSee [documentation on supported types and syntax here](https://help.obsidian.md/Editing+and+formatting/Callouts).\n\n## Showcase\n\n\u003e [!EXAMPLE] Examples\n\u003e\n\u003e Aliases: example\n\n\u003e [!note] Notes\n\u003e\n\u003e Aliases: note\n\n\u003e [!abstract] Summaries \n\u003e\n\u003e Aliases: abstract, summary, tldr\n\n\u003e [!info] Info \n\u003e\n\u003e Aliases: info, todo\n\n\u003e [!tip] Hint \n\u003e\n\u003e Aliases: tip, hint, important\n\n\u003e [!success] Success \n\u003e\n\u003e Aliases: success, check, done\n\n\u003e [!question] Question \n\u003e\n\u003e Aliases: question, help, faq\n\n\u003e [!warning] Warning \n\u003e\n\u003e Aliases: warning, caution, attention\n\n\u003e [!failure] Failure \n\u003e\n\u003e Aliases: failure, fail, missing\n\n\u003e [!danger] Error\n\u003e\n\u003e Aliases: danger, error\n\n\u003e [!bug] Bug\n\u003e\n\u003e Aliases: bug\n\n\u003e [!quote] Quote\n\u003e\n\u003e Aliases: quote, cite\n","lastmodified":"2023-06-07T15:55:55.950775144Z","tags":[]},"/notes/config":{"title":"Configuration","content":"\n## Configuration\nQuartz is designed to be extremely configurable. You can find the bulk of the configuration scattered throughout the repository depending on how in-depth you'd like to get.\n\nThe majority of configuration can be found under `data/config.yaml`. An annotated example configuration is shown below.\n\n```yaml {title=\"data/config.yaml\"}\n# The name to display in the footer\nname: Jacky Zhao\n\n# whether to globally show the table of contents on each page\n# this can be turned off on a per-page basis by adding this to the\n# front-matter of that note\nenableToc: true\n\n# whether to by-default open or close the table of contents on each page\nopenToc: false\n\n# whether to display on-hover link preview cards\nenableLinkPreview: true\n\n# whether to render titles for code blocks\nenableCodeBlockTitle: true \n\n# whether to render copy buttons for code blocks\nenableCodeBlockCopy: true \n\n# whether to render callouts\nenableCallouts: true\n\n# whether to try to process Latex\nenableLatex: true\n\n# whether to enable single-page-app style rendering\n# this prevents flashes of unstyled content and improves\n# smoothness of Quartz. More info in issue #109 on GitHub\nenableSPA: true\n\n# whether to render a footer\nenableFooter: true\n\n# whether backlinks of pages should show the context in which\n# they were mentioned\nenableContextualBacklinks: true\n\n# whether to show a section of recent notes on the home page\nenableRecentNotes: false\n\n# whether to display an 'edit' button next to the last edited field\n# that links to github\nenableGitHubEdit: true\nGitHubLink: https://github.com/jackyzha0/quartz/tree/hugo/content\n\n# whether to render mermaid diagrams\nenableMermaid: true\n\n# whether to use Operand to power semantic search\n# IMPORTANT: replace this API key with your own if you plan on using\n# Operand search!\nsearch:\n  enableSemanticSearch: false\n  operandApiKey: \"REPLACE-WITH-YOUR-OPERAND-API-KEY\"\n  operandIndexId: \"REPLACE-WITH-YOUR-OPERAND-INDEX-ID\"\n\n# page description used for SEO\ndescription:\n  Host your second brain and digital garden for free. Quartz features extremely fast full-text search,\n  Wikilink support, backlinks, local graph, tags, and link previews.\n\n# title of the home page (also for SEO)\npage_title:\n  \"🪴 Quartz 3.3\"\n\n# links to show in the footer\nlinks:\n  - link_name: Twitter\n    link: https://twitter.com/_jzhao\n  - link_name: Github\n    link: https://github.com/jackyzha0\n```\n\n### Code Block Titles\nTo add code block titles with Quartz:\n\n1. Ensure that code block titles are enabled in Quartz's configuration:\n\n    ```yaml {title=\"data/config.yaml\", linenos=false}\n    enableCodeBlockTitle: true\n    ```\n\n2. Add the `title` attribute to the desired [code block\n   fence](https://gohugo.io/content-management/syntax-highlighting/#highlighting-in-code-fences):\n\n      ```markdown {linenos=false}\n       ```yaml {title=\"data/config.yaml\"}\n       enableCodeBlockTitle: true  # example from step 1\n       ```\n      ```\n\n**Note** that if `{title=\u003cmy-title\u003e}` is included, and code block titles are not\nenabled, no errors will occur, and the title attribute will be ignored.\n\n### HTML Favicons\nIf you would like to customize the favicons of your Quartz-based website, you \ncan add them to the `data/config.yaml` file. The **default** without any set \n`favicon` key is:\n\n```html {title=\"layouts/partials/head.html\", linenostart=15}\n\u003clink rel=\"shortcut icon\" href=\"icon.png\" type=\"image/png\"\u003e\n```\n\nThe default can be overridden by defining a value to the `favicon` key in your \n`data/config.yaml` file. For example, here is a `List[Dictionary]` example format, which is\nequivalent to the default:\n\n```yaml {title=\"data/config.yaml\", linenos=false}\nfavicon:\n  - { rel: \"shortcut icon\", href: \"icon.png\", type: \"image/png\" }\n#  - { ... } # Repeat for each additional favicon you want to add\n```\n\nIn this format, the keys are identical to their HTML representations.\n\nIf you plan to add multiple favicons generated by a website (see list below), it\nmay be easier to define it as HTML. Here is an example which appends the \n**Apple touch icon** to Quartz's default favicon:\n\n```yaml {title=\"data/config.yaml\", linenos=false}\nfavicon: |\n  \u003clink rel=\"shortcut icon\" href=\"icon.png\" type=\"image/png\"\u003e\n  \u003clink rel=\"apple-touch-icon\" sizes=\"180x180\" href=\"/apple-touch-icon.png\"\u003e\n```\n\nThis second favicon will now be used as a web page icon when someone adds your \nwebpage to the home screen of their Apple device. If you are interested in more \ninformation about the current and past standards of favicons, you can read \n[this article](https://www.emergeinteractive.com/insights/detail/the-essentials-of-favicons/).\n\n**Note** that all generated favicon paths, defined by the `href` \nattribute, are relative to the `static/` directory.\n\n### Graph View\nTo customize the Interactive Graph view, you can poke around `data/graphConfig.yaml`.\n\n```yaml {title=\"data/graphConfig.yaml\"}\n# if true, a Global Graph will be shown on home page with full width, no backlink.\n# A different set of Local Graphs will be shown on sub pages.\n# if false, Local Graph will be default on every page as usual\nenableGlobalGraph: false\n\n### Local Graph ###\nlocalGraph:\n    # whether automatically generate a legend\n    enableLegend: false\n    \n    # whether to allow dragging nodes in the graph\n    enableDrag: true\n    \n    # whether to allow zooming and panning the graph\n    enableZoom: true\n    \n    # how many neighbours of the current node to show (-1 is all nodes)\n    depth: 1\n    \n    # initial zoom factor of the graph\n    scale: 1.2\n    \n    # how strongly nodes should repel each other\n    repelForce: 2\n\n    # how strongly should nodes be attracted to the center of gravity\n    centerForce: 1\n\n    # what the default link length should be\n    linkDistance: 1\n    \n    # how big the node labels should be\n    fontSize: 0.6\n    \n    # scale at which to start fading the labes on nodes\n    opacityScale: 3\n\n### Global Graph ###\nglobalGraph:\n\t# same settings as above\n\n### For all graphs ###\n# colour specific nodes path off of their path\npaths:\n  - /moc: \"#4388cc\"\n```\n\n\n## Styling\nWant to go even more in-depth? You can add custom CSS styling and change existing colours through editing `assets/styles/custom.scss`. If you'd like to target specific parts of the site, you can add ids and classes to the HTML partials in `/layouts/partials`. \n\n### Partials\nPartials are what dictate what gets rendered to the page. Want to change how pages are styled and structured? You can edit the appropriate layout in `/layouts`.\n\nFor example, the structure of the home page can be edited through `/layouts/index.html`. To customize the footer, you can edit `/layouts/partials/footer.html`\n\nMore info about partials on [Hugo's website.](https://gohugo.io/templates/partials/)\n\nStill having problems? Checkout our [FAQ and Troubleshooting guide](notes/troubleshooting.md).\n\n## Language Support\n[CJK + Latex Support (测试)](notes/CJK%20+%20Latex%20Support%20(测试).md) comes out of the box with Quartz.\n\nWant to support languages that read from right-to-left (like Arabic)? Hugo (and by proxy, Quartz) supports this natively.\n\nFollow the steps [Hugo provides here](https://gohugo.io/content-management/multilingual/#configure-languages) and modify your `config.toml`\n\nFor example:\n\n```toml\ndefaultContentLanguage = 'ar'\n[languages]\n  [languages.ar]\n    languagedirection = 'rtl'\n    title = 'مدونتي'\n    weight = 1\n```\n","lastmodified":"2023-06-07T15:55:55.950775144Z","tags":["setup"]},"/notes/custom-Domain":{"title":"Custom Domain","content":"\n### Registrar\nThis step is only applicable if you are using a **custom domain**! If you are using a `\u003cYOUR-USERNAME\u003e.github.io` domain, you can skip this step.\n\nFor this last bit to take effect, you also need to create a CNAME record with the DNS provider you register your domain with (i.e. NameCheap, Google Domains).\n\nGitHub has some [documentation on this](https://docs.github.com/en/pages/configuring-a-custom-domain-for-your-github-pages-site/managing-a-custom-domain-for-your-github-pages-site), but the tldr; is to\n\n1. Go to your forked repository (`github.com/\u003cYOUR-GITHUB-USERNAME\u003e/quartz`) settings page and go to the Pages tab. Under \"Custom domain\", type your custom domain, then click **Save**.\n2. Go to your DNS Provider and create a CNAME record that points from your domain to `\u003cYOUR-GITHUB-USERNAME.github.io.` (yes, with the trailing period).\n\n\t![Example Configuration for Quartz](/notes/images/google-domains.png)*Example Configuration for Quartz*\n3. Wait 30 minutes to an hour for the network changes to kick in.\n4. Done!","lastmodified":"2023-06-07T15:55:55.950775144Z","tags":[]},"/notes/docker":{"title":"Hosting with Docker","content":"\nIf you want to host Quartz on a machine without using a webpage hosting service, it may be easier to [install Docker Compose](https://docs.docker.com/compose/install/) and follow the instructions below than to [install Quartz's dependencies manually](notes/preview%20changes.md).\n## Hosting Quartz Locally\nYou can serve Quartz locally at `http://localhost:1313` with the following script, replacing `/path/to/quartz` with the \nactual path to your Quartz folder.\n\ndocker-compose.yml\n```\nservices:\n  quartz-hugo:\n    image: ghcr.io/jackyzha0/quartz:hugo\n    container_name: quartz-hugo\n    volumes:\n      - /path/to/quartz:/quartz\n    ports:\n      - 1313:1313\n\n    # optional\n    environment:\n      - HUGO_BIND=0.0.0.0\n      - HUGO_BASEURL=http://localhost\n      - HUGO_PORT=1313\n      - HUGO_APPENDPORT=true\n      - HUGO_LIVERELOADPORT=-1\n```\n\nThen run with: `docker-compose up -d` in the same directory as your `docker-compose.yml` file.\n\nWhile the container is running, you can update the `quartz` fork with: `docker exec -it quartz-hugo make update`.\n\n## Exposing Your Container to the Internet\n\n### To Your Public IP Address with Port Forwarding (insecure)\n\nAssuming you are already familiar with [port forwarding](https://en.wikipedia.org/wiki/Port_forwarding) and [setting it up with your router model](https://portforward.com):\n\n1. You should set the environment variable `HUGO_BASEURL=http://your-public-ip` and then start your container.\n2. Set up port forwarding on your router from port `p` to `your-local-ip:1313`.\n3. You should now be able to access Quartz from outside your local network at `http://your-public-ip:p`.\n\nHowever, your HTTP connection will be unencrypted and **this method is not secure**.\n\n### To a Domain using Cloudflare Proxy\n\n1. Port forward 443 (HTTPS) from your machine.\n2. Buy a custom domain (say, `your-domain.com`) from [Cloudflare](https://www.cloudflare.com/products/registrar/). Point a DNS A record from `your-domain.com` to your public IP address and enable the proxy.\n3. Set the environment variables `HUGO_BASEURL=https://your-domain.com`, `HUGO_PORT=443`, and `HUGO_APPENDPORT=false`. Change `1313:1313` to `443:443` for the `ports` in `docker-compose.yml`.\n4. Spin up your Quartz container and enjoy it at `https://your-domain.com`!\n\n### To a Domain using a Reverse Proxy\n\nIf you want to serve more than just Quartz to the internet on this machine (or don't want to use the Cloudflare registrar and proxy), you should follow the steps in the section above (as appropriate) and also set up a reverse proxy, like [Traefik](https://doc.traefik.io/traefik). Be sure to configure your TLS certificates too!\n","lastmodified":"2023-06-07T15:55:55.950775144Z","tags":["setup"]},"/notes/editing":{"title":"Editing Content in Quartz","content":"\n## Editing \nQuartz runs on top of [Hugo](https://gohugo.io/) so all notes are written in [Markdown](https://www.markdownguide.org/getting-started/).\n\n### Folder Structure\nHere's a rough overview of what's what.\n\n**All content in your garden can found in the `/content` folder.** To make edits, you can open any of the files and make changes directly and save it. You can organize content into any folder you'd like.\n\n**To edit the main home page, open `/content/_index.md`.**\n\n### Front Matter\nHugo is picky when it comes to metadata for files. Make sure that your title is double-quoted and that you have a title defined at the top of your file like so, otherwise the generated page will not have a title!\n\nYou can also add tags here as well.\n\n```yaml\n---\ntitle: \"Example Title\"\ntags:\n- example-tag\n---\n\nRest of your content here...\n```\n\n### Obsidian\nI recommend using [Obsidian](http://obsidian.md/) as a way to edit and grow your digital garden. It comes with a really nice editor and graphical interface to preview all of your local files.\n\nThis step is **highly recommended**.\n\n\u003e 🔗 Step 3: [How to setup your Obsidian Vault to work with Quartz](notes/obsidian.md)\n\n## Previewing Changes\nThis step is purely optional and mostly for those who want to see the published version of their digital garden locally before opening it up to the internet. This is *highly recommended* but not required.\n\n\u003e 👀 Step 4: [Preview Quartz Changes](notes/preview%20changes.md)\n\nFor those who like to live life more on the edge, viewing the garden through Obsidian gets you pretty close to the real thing.\n\n## Publishing Changes\nNow that you know the basics of managing your digital garden using Quartz, you can publish it to the internet!\n\n\u003e 🌍 Step 5: [Hosting Quartz online!](notes/hosting.md)\n\nHaving problems? Checkout our [FAQ and Troubleshooting guide](notes/troubleshooting.md).\n","lastmodified":"2023-06-07T15:55:55.950775144Z","tags":["setup"]},"/notes/games101/Lecture-02-Review-of-Linear-Algebra":{"title":"Lecture 02 Review of Linear Algebra","content":"\n## 课前说明\n\n### 关键词\n\n向量、矩阵、点乘、叉乘、图形学、长度、三角形、投影、坐标系、夹角、乘法、代数、线性代数、直角坐标、光线追踪、平行四边形、数学定义、定义自然\n\n### 宣布一些事情\n\n[00:06](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=6)\n咱们这就开始在这个课之前，我们先说一说有几个事情宣布一下。首先一个是每一节的课后，然后我们的这个这个课件会以这个 PDF 的形式放出来，然后我们这个录播也会放到这个 games 的网站上，然后这两者都可以在我们课程主页上找到，也就是通过我的这个主页，然后有一个课程主页，然后找到了之后这个课件和这个录音都可以看到录像。\n\n然后这就是大家应该能够看到的这么这么一个表格，就是每一节结束之后都会有这么两个链接。然后有同学反映到说这个课件PDF，这因为访问涉及到大家从国内访问国外网站，然后会比较慢，然后从 games 的网站上大家也可以看得到这个在百度网盘上面上传的这个课件，所以说应该都没有问题。然后嗯，这里大概就是这么一个方式。\n\n![](res/Lecture_02_Review_of_Linear_AlgebraPT1M6.556S.webp)\n\n[01:05](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=65)\n然后另外大家可以看到，比如说这个今天 14 号，然后这个说向量有线性代数，那么在这个课之前，我们就会把这个阅读材料给这个放出来，大家就可以先看就是阅读材料，指的就是那本书虎书的 Tiger book，然后他的第几章。然后这个阅读材料不做强制要求，就是说这些阅读材料只是说明这个是书里面和咱们这个课上要讲的这个相似的内容，大家如果愿意的话可以提前去学习学习，然后课程之后也欢迎再去阅读。好，就是说这本书本身就不做要求，那么大概就是这么回事。\n\n然后另外一个呢？这个今天是国内时间，已经是2月 14 号了，情人节，然后祝大家情人节快乐，然后如果大家现在还单身，祝大家早日脱单，早日找到另自己的另一半。然后如果大家这个已经有了另一半的话，这个祝大家有情人终成眷属，然后长长久久。好，那这个就说到这里。另外对，还有一点涉及到这个打赏的这么一个事情，这个毫无必要，这个就是说首先这是一个本来就是这个公众平台公益的性质，然后这个要打赏，就这个显得多少有一点这个嗯，不合适，这个反正感谢大家，大家要是愿意支持我就多过来上这个课就可以了，完全没有必要来做这个打赏。\n\n### 上节课的内容\n\n[02:25](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=145)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT2M56.788S.webp)\n\n好，那就说到这里，那么这个我们上节课说到什么了？这个我们提到什么是计算机图形学？我们定义了一下，然后我们为什么要学习计算机图形学？然后我们提到了说我们这个课分为四部分，然后如果大家还记得的话，光栅化几何，然后这个光线追踪和这个模拟或动画这一块总共 4 块。然后课程涉及到这些网站，别的这些各种各样的这个这各方面的这些事情应该都已经交代到了。\n\n## 这节课的内容\n\n### 图形学依赖的基础科学\n#### 线性代数\n![](res/Lecture_02_Review_of_Linear_AlgebraPT3M38.622S.webp)\n[02:56](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=176)\n那么上一节课大概就说了这个事情，那么这节课咱们要就要从这个最基础的内容开始，一点一点把这个图形学的基础这个开始构建起来。那么图形学呢？其实依赖于很多东西，那咱们这节课主要是来说这个线性代数，大家可以看到这个标题非常恐怖，对不对？\n\n这个 Swift and Brutal introduction，然后这个其实是参考之前清华有门课的这个这个标题，那个我记得当时那门课叫计算机入门，然后这个课叫做 Swift and Brutal introduction to computer science，然后大家国内翻译成操快猛这个计算机入门，然后我们借用一下概念，然后其实怎么说大家线性代数这个概念其实本身并不难。然后大家看到待会咱们复习的时候，大家就会知道其实相对简单，没什么问题，咱们这节课只是说的快一些，然后这就是主要的内容。\n\n#### 基础数学和物理学\n[03:49](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=229)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT4M24.625S.webp)\n\n那么咱们为什么要说线性代数？是因为图形学依赖于线性代数，其实图形学依赖于很多不同的东西，比如说这个基础的数学，像这个线性代数、微积分，然后统计，像这些都是这个非常用得到的一些知识。然后也涉及到一些基础的物理，主要就是涉及到光学和力学这些方面的知识。\n\n然后以及说随着现在图形学的发展，大家越来越看重说更高深一点物理学的知识。比如说其实我自己这边也在做类似的所谓波动光学的研究，就是说那当我们不能再假设这个光是直线传播这个时候，然后这种情况下，这个光作为一种光波如何与这个物体的表面材质进行作用，然后得到不同各种各样不同的这个外观，这都是这个图形学涉及到的东西。\n\n#### 信号处理和数值分析\n[04:38](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=278)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT4M44.231S.webp)\n\n光学和力学这里，然后这个还涉及到一些杂七杂八各种各样方面的知识都会涉及到。比如说这里我特别提到信号处理，因为有很多情况下我们要分析说这个出现了一些，比如说我们平常经常听到的一些走样这种现象，然后有一些叫做反走样技术，有很多这些技术在背后都是要解决一些信号处理的问题。\n\n然后另外数值分析这是一个非常重要的事情。有很多情况下图形学其实就是在解一些复杂的一些这个数学计算，比如说积分，像这个渲染整个过程其实就在解一个这个递归定义的积分，然后像这个模拟或者仿真，这里其实有很多在解一些比如说有限元问题，或者说各方面的这些扩散方程之类的事情，就是说嗯，这块依赖的很多。然后图形学还要依赖于另外一点点，这一点给大家提一下，还需要一点点美学做出来一点东西之后，其实这个大家都希望这个图形，这个大家做出来的图形能够挺好看的，所以说有一点这个美感是很有必要的一件事情。\n\n### 本节课的线性代数内容\n[05:37](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=337)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT6M5.795S.webp)\n\n当然了上这门课也能够培养那么一点点，大概就是说我们会提到这么一些这个这些内容，这些渐渐的都会穿插在这门课里面，然后这个之后再给大家慢慢介绍。那么咱们这节课就会把这个线性代数说一下。嗯，包括这门课本身是一个入门的这个课，然后它更多的依赖一些基础的这些课程，特别是线性代数，然后线性代数我们主要说一些这个向量矩阵这些操作，对吧？\n\n向量涉及到什么点？乘，叉乘，向量这个矩阵涉及到矩阵和矩阵的乘，矩阵和向量的乘，然后比如说在图形学里面咱们要表示一个嗯，什么呢？一个点，对吧？x、y、 z 三个这个坐标，然后这个空间中的一个点，那我们用三个数来表示，这个实际上就是一种向量表示，然后这个涉及到各种各样的，比如平移、旋转、缩放，各种各样的这个操作，我们都可以把它给表示成这个矩阵和向量的乘法，所以这些是非常有必要的。\n\n### 一个旋转的例子\n[06:37](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=397)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT6M51.102S.webp)\n\n然而这些东西并不难，那咱们这个课，这今天这个课咱们就给大家这个复习一下，这里先给大家看一个这个例子，这个动画是我自己做的，然后这个，嗯，它为了说明的就是其实这个物体在旋转下它会看着是长什么样？这个蜗牛的这个壳，然后大家可以看到很多不同的这些发光点，然后但是这里大家要关注的点在哪？\n\n在这个蜗牛它本身是在这个不断旋转的，然后如果大家这个仔细看的话，其实这个蜗牛的旋转速度它一直在变化，也就是说它这个是从一边旋转到另一边，它会先加速到了中间旋转的最快，然后到了两边突然就停下，然后再回到这边也是一样，并不是以一个固定的速度来进行旋转。所以说包括这个旋转，这是一个非常简单的例子。\n\n## 向量基本概念\n### 向量的定义\n[07:27](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=447)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT7M50.402S.webp)\n\n这然后这里就已经包括了不少这个背后的这些数学知识，所以说这个是很有意思的一个事情，然后这当然了，这就是一个旋转的例子，很简单，就是用一个旋转矩阵，然后咱们很快就会说，那么咱们今天的课最基础的内容咱们自然要从向量说起。好，然后这个向量这个概念叫做vector，然后涉及到数学和物理上面不同的定义，数学上更愿意管它叫向量，物理上更愿意管它叫矢量，就是那个箭矢的矢，就是射箭的箭，就是那个矢量。然后这个我们就管它叫向量好吧。\n\n这个没有什么关系，那么这个向量这从名字就可以体现得出来，它表示的是一个方向，那么嗯，在这里大家可以看到，比如说我有一个箭头，这个从 a 指向b，那么这个向量其实表示的就是从 a 指向 b 这么一个方向，那这个方向经常是怎么得来的？就是从这个 b 的坐标减去 a 的坐标，然后就可以得到这么一个向量，从 a 指向b，然后向量。如果你去平移这个向量，它表示的都是同一个向量，因为它只要指向同一个方向，那它就是同一个向量。\n\n[08:38](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=518)\n然后像这里这个 a b 向量， b 减a 得到这么一个 a b 这个向量，然后我们也可以把它写作另外一个，用某一个字母来表示，比如说写作a，然后上面打一个箭头，然后或者这个印刷上面经常会涉及到这个向量，把它写作一个粗体的一个字母，然后写作 a 没有问题。然后这个向量最重要的有两个属性，一个是方向，就是这个 a 和b，它当然了向量表示的就是不同的方向，然后另外一个向量本身还能表示它的长度。\n\n所以从 a 到b，如果这个 b 离得远一点， a 和 b 之间这个距离长，那就是这个向量的长度要长。如果要短的话，向量短就相对较短。所以向量表示两个不同的内容，一个是这个方向，一个是长度。然后我们刚才提到这个向量，如果你平移它移动各个这个到不同的位置，那它表示的仍然是同一个向量，因为 a 和 b 之间的相对位置没有改变，所以说这个向量我们并不关心它的这个绝对的开始的位置，然后就是这么一个意思，然后向量我们刚才既然提到它的方向和长度，那我们就把它的长度可以直接写出来。\n\n### 向量的长度 和 归一化\n[09:49](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=589)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT10M23.595S.webp)\n\n是什么呢？就是说比如说一个 a 向量，就是 a 上面的一个箭头，然后这个左边右边各加两个斜杠，然后这个表示的就是这个向量的长度，然后向量的长度这个有什么用？就是这个向量的长度可以给我们提供一个这个什么呢？给我们提供一种向量，叫做单位向量。什么叫单位向量？就是如果这些向量它表示的长度是1，那么这个就是单位向量。那么给你任何一个向量，你怎么把这个向量给变成单位向量呢？很简单，你去把这个向量本身去除，以它的长度，那么你得到的就是一个和你原始的向量同方向，并且长度为 1 的向量，也就变成了一个单位向量。\n\n[10:29](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=629)\n这也就是大家看到这里这个公式，大家可以看到这个公式，它这个是这个原始的这个向量 a 除以它的长度，然后我们把它定义成a，然后上面写一个这个东西，这个我们正常英文会管它读作 a hat， hat 就是那个帽子（$\\hat{a}$  读作 a hat）。然后正常情况下我们用这个形式来表示单位向量，然后在图形学里面可能更多的是大家一提到方向，就认为我们用一个单位向量来表示方向，然后我们不关心它的长度，好吧？然后那就是说我们用单位向量是可以直表示一个方向，那这个就非常好用。\n\n### 向量求和\n[11:08](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=668)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT12M55.794S.webp)\n\n然后在图形学里有涉及到各种各样不同的方向，然后咱们再待会再给大家这个一一解释。然后向量有很多基本操作，然后基本操作挺简单的，比如说最简单的操作就叫做这个向量求和，那么向量求和很简单，你要给两个向量，比如说给你 a 和b，然后给你这么两个向量，然后你如何去算出 a 加 b 是什么？\n\n那么这里有两个不同的解释，一个叫做平行四边形法则，一个叫做三角形法则，那么平行四边形法则是什么意思？大家看左边这幅图，那咱们可以把这个 a 和b，既然我们刚才说这 a 和 b 都可以任意的这个移动不影响它们的值，然后 a 和 b 我们可以把它放在同一个起点上，然后放在同一个起点上，那就是大家看到的左边这张图的左下角，然后然后我们把这个 b 和 a 都这个平行的搬移一段距离，使得这个 a 和 b 和另外一个 a 和 b 围成一个平行四边形。\n\n[12:05](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=725)\n那么这个平行四边形的对角线，这条红线就是 a 加b，那这是一个平行四边形法则，然后这样可以告诉我们两个向量加起来是什么，然后另外向量还有一个计算方法叫做三角形法则，三角形法则在这个图形学里面用的也挺多，就是说这个很简单，我们要相加若干向量，我们就把这些向量首尾相接的拼起来，那么最后形成的这个最开始和最结束我们把它连起来。那就是这个相加的和，那这个不只适用于一两个向量，就很多向量也可以这么做。\n\n#### 向量求和不同角度的理解\n[12:37](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=757)\n比如咱们看右边这幅图，这个这要算 a 加 b 怎么办？把这个 a 和 b 首尾相加，也就是说把这个 b 的开始放在 a 的结束上，然后 a 加b，也就是从 a 的起点到 b 的终点，那如果 a 加 b 加c，大家可以这个一直这么操作下去，那这个就很方便。然后这两种不管是平行四边形法则还是这个三角形法则，最简单的做法就是，嗯，通过这么一种几何的这么一种理解，然后来看这个在几何上两个向量加起来是什么。\n\n那么还有一个理解，就是在这个数学上或者代数上，这个向量相加是什么呢？那就是直接把他们的这个坐标加起来，这个非常简单，这个很快就要跟大家说，大家如果看这个例子，就是说我们现在在描述同样是一个向量a，但是这里大家会看到这个有一个这个坐标系在这里面，然后有一个x，有个y， x 向右， y 向上。然后这个这里就其实是在用这个直角坐标系，或者叫笛卡尔坐标系来描述这个向量。\n\n#### 笛卡尔坐标系 Cartesian Coordinates\n\n[13:43](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=823)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT13M43.672S.webp)\n\n那么这个向量什么意思？我就认为这个向量永远都是从这个圆点开始，零零，也就是左下角，然后这个沿着 x 有一些这个单位向量，然后向右走这个大家可以看到这个 x 被分成了好多段，这就认为是这有很多不同的职业单位。然后 y 方向也是一样，这个分为好多段，那么一个向量到底是多少，我们就可以用几个 x 加几个 y 来表示，然后通常这个 x y 是这个定义成这个互相垂直的，并且都是单位向量。\n\n然后这个像这里的情况，这个 a 向量黄颜色表示，我们就可以把它表示成 4 个 x 加 3 个y，大家可以看到这个比较清楚，对吧？然后这样的话有什么好处？就是说我们可以直接用这个坐标 4 和 3 这两个数来表示这么一个向量，那在这里这个 a 向量，大家可以看到左下方这个 a 向量写作 x y，这个 x y 就是表示它这个前面的数就是多少个 x 和多少个 y 的意思，那么这就是它的这个坐标表示。\n\n#### 向量书写默认是列向量\n[14:42](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=882)\n然后这里有一点在图形学上，我不知道在其他的领域怎么样定义。在图形学上大家默认给一个向量，如果不说它是一个什么样的形式，我们就认为这个向量是一个这个往下写，也就是说任何一个向量，这个缺省的设置是这个向量是个列向量，就是一列数。好，然后这个向量我们也可以把它给变成一个这个横的，也就是变成一个行向量。\n\n#### 代数形式的含义\n[15:09](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=909)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT13M43.672S.webp)\n\n像这个中间大家看到的一个 a t 等于 x y，这个 t 是什么意思？就表示这个转置就是把这个任何一个向量的行和列互换，那自然是这个原本是默认是列向量，那么这个转置之后就变成了这个横向的 x 和y，横着写这个在矩阵上面大家也会这么用。然后我们为什么要这么写这个定义这么一个i，这个 x 和 y 这个直角坐标，是因为如果我们这么定义的话，对于两个轴如果垂直，而且又是这个以单位向量这么一节一节给加起来，就是说那我们算这个向量长度就非常简单，那怎么算呢？\n\n比如说像这里这个二维的向量，对吧？然后我们就用它的x，嗯，平方加上 y 平方，然后开个根号就可以得到这个向量的长度。然后比如说像这个情况，大家看到水平方向是四个格子，竖直方向是三个格子，那么这个大家知道这个勾股定理，就告诉大家这个向量的长度是5，那也就是说我们把向量表示成直角坐标系的这样一种代数形式，是非常有助于计算它的长度的。所以这就是为什么大家平常会用一套坐标系，那大概就是这么一个道理，那向量呢？加法以及他们这个之前的这个表示都是非常简单的。\n\n### 向量点乘\n[16:25](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=985)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT16M54.485S.webp)\n\n那么向量更广泛的用法，也就是这节课主要跟大家说的两个这个计算，就是向量的这个点乘和差乘这么两个计算，那么都是什么意思？这个就是说这个，嗯，向量的乘法，它和这个数的乘法不太一样，像我们提到数的乘法，那就是乘法，那向量的乘法其实两个不同类型的乘法，那也就是说两种不同的运算。\n\n#### 点乘的定义\n[16:51](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=1011)\n那咱们把这两个分别看明白好吗？就是说，比如说啊，大家现在看到这个屏幕中间这有两个向量， $\\vec{a}$ 和 $\\vec{b}$，然后这个向量的点乘是什么意思？就是这么定义的，大家可以看到这个 $\\vec{a}$ 向量点乘 $\\vec{b}$ 向量，然后等于这个等于什么呢？等于 $\\|\\vec{a}\\|$ 乘以 $\\|\\vec{b}\\|$，再乘以它们两个之间夹角的余弦，然后就是这么这么一个定义，然后这个定义自然是最标准的定义了。\n\n\n#### 点乘的意义\n[17:22](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=1042)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT16M54.485S.webp)\n\n比如说给大家给定了两个不同的向量，那么它长度我们自然知道，然后这个两个向量之间的夹角，我们假设能够算出来，然后这个用一个余弦 cosine 就可以算出来这个数是多少。那么有一点这个是需要这跟大家说明的，就是说这个大家看到从这个 a 向量点乘，这个 b 向量中间打个点，这个最后的结果你看 a 向量的长度是一个数， b 向量的长度是一个数，这个余弦仍然是一个数，所以说这三个数乘起来它其实还是一个数。\n\n所以左边是两个向量，然后它的点乘是一个数字，然后或者叫做一个数量，然后就是说这里就是点乘能够告诉大家的结果就是说向量的点乘给大家，最后得到的是一个数。那么下面要给大家说的就是说，嗯，这个定义我们要怎么用它？特别是在图形学里怎么用它？这个很简单，比如说我们可以做一个简单的变换，把这个 a 和 b 的长度拿和那个 a 和 b 的点乘拿到一边去，把这个夹角的余弦放在另一边，那也就是发现大家可以看到这个左下角的这么一个画面。\n\n[18:29](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=1109)\n一个定义这个给定两个向量，两个向量之间夹角的余弦是他们的点乘，然后这个除以他们这个各自的长度的积，那么也就是说给你两个向量，大家立刻就可以算出这个，嗯，这两个向量之间的夹角余弦也进而可以算出两个向量之间的夹角。\n\n也就是说点乘可以帮助大家快速的得到这个两个向量之间的夹角，特别是在一个什么样的情况下最方便？特别是在两个向量都只是方向，也就是都是单位向量的时候，大家可以看右边这个公式，当两个向量都是单位向量的时候，它们的长度自然都是1，所以它们的点乘自然直接就是它们夹角的余弦，所以这个时候只需要我们做一个这个反余弦，我们就可以得到这个这个两个向量之间的夹角，所以向量的点点乘是很有用的。\n\n#### 点乘运算的性质（满足的定律）\n[19:24](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=1164)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT19M47.599S.webp)\n\n那么说到现在其实还没有说明这个向量点乘要怎么算，对不对？然后这个我们之后在代数的这一方面，然后给大家解释。那么这个点乘，然后它本身既然是一种运算法则，都会满足各种各样不同的这个性质，比如说点乘满足这么几个性质，这个和数字的乘法非常相似，什么呢？交换率、结合率和分配率，对吧？\n\n这个大家可以看到，交换律，也就是说 $\\vec{a}$ 和 $\\vec{b}$ 点乘，等于 $\\vec{b}$ 和 $\\vec{a}$ 点乘，这个挺好的。然后如果 $\\vec{a}$ 和 $\\vec{b}$ 加 $\\vec{c}$ 这个做点乘，那么就等于是 $\\vec{a}$ 和 $\\vec{b}$ 做点乘，然后再加上 $\\vec{a}$ 和 $\\vec{c}$ 做点乘，然后如果有一个数字 $k$ 去乘以某个向量，再和另外一个向量做点乘，那就好像是说这个某个向量和这个 $k$ 应用在另外一个向量上，他们两个做点乘，或者说先做点乘，在前面再乘以一个 $k$，这个问题都不大。\n\n#### 点乘在笛卡尔坐标系上的运算\n[20:20](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=1220)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT21M29.015S.webp)\n\n这些是这个向量的一些基本属性，这个向量点乘的一些基本属性，然后我们刚才就提到这个事情，就是说这个点乘自然有它的定义，对吗？就是 a 的长度 $\\left \\| a \\right \\|$， b 的长度 $\\left \\| b \\right \\|$，然后夹角的余弦乘起来，那么在这个直角坐标系或者笛卡尔坐标系下，他们其实这个点乘的运算会更加简单，为什么呢？因为大家可以看到，比如说在二维的情况下，我们用两个数 $x$ 和 $y$ 来表示一个向量，那 $a$ 就是 $x_a y_a$，然后这个 $b$ 就是 $x_b y_b$，然后他们两个的点乘就是 $x_a x_b + y_a y_b$，大家发现这个就是其实就是对应的元素相乘并且加起来。\n\n[21:04](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=1264)\n那么这个道理可以同样扩展到高维，在三维上也是一样的，就是说这个两个向量在三维上，那自然怎么表示呢？那就是用$x_a$，$y_a$，$z_a$ 这三个数，然后表示a，然后 $x_b$，$y_b$，$z_b$ 这三个数表示b。那么这个两个向量怎么做？点乘很简单，也是对应元素点乘，$x_a x_b + y_a y_b + z_a z_b$。就是说对应的元素相乘，再把它们加起来。那么从这个过程大家可以清晰的看到这个给定的两个数，两个这个输入都是向量，都是这个 3 个一组或者两个一组，对吧？然后最后的结果就是一个数，这也就是点乘的性质，那么就是通过这种方法来计算的。\n\n#### 点乘的应用\n[21:53](\u003cfile:///Z:/阿里云盘/技术美术/GAMES课程/GAMES101-现代计算机图形学入门-闫令琪/Lecture_02_Review_of_Linear_Algebra.mp4#t=1312.755021\u003e)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT21M52.755S.webp)\n\n##### 计算得到两个方向之间的夹角\n[21:50](file:///Z:/阿里云盘/技术美术/GAMES课程/GAMES101-现代计算机图形学入门-闫令琪/Lecture_02_Review_of_Linear_Algebra.mp4#t=1310.183806)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT22M24.261S.webp)\n\n然后这个点乘，我们刚才已经提到它最大的作用，特别是在图形学里面，就是用来找到这个两个向量或者两个方向之间的夹角，然后或者说是余弦夹角。\n然后呢？嗯，特别是比如说我们之后在做光照模型的时候，我们知道光从哪一个方向进来，然后这个物体表面的法线是什么样的，然后这个我们从哪个方向去看，那这些方向之间互相的夹角，这个计算都是通过点乘来这个运算的。\n\n##### 计算一个向量到另外一个向量的投影\n[22:28](file:///Z:/阿里云盘/技术美术/GAMES课程/GAMES101-现代计算机图形学入门-闫令琪/Lecture_02_Review_of_Linear_Algebra.mp4#t=1348.369125)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT22M34.603S.webp)\n\n然后这个点乘还有另外一个这个很有用的事情，就是说我们要找到这个一个向量投影到另一个向量上是长什么样，这个是什么意思？大家可以看这么一个例子，就是说我们现在看 a 和b，它们两个方向是不一样的，然后但是我现在希望把这个 b 向量所谓投影到这个 a 向量上去，那是什么意思？就是说假设有一束这个光线平行着的，然后他们垂直着 a 方向照过来，那么这个 b 自然会投出一个阴影在 a 上，那投出这个阴影这一段，我们就管它叫做这个 b 在 a 上的投影，然后这个大家就会看到这里是应该怎么计算它，这个这里就需要用到刚才我们提到的这个向量的这个基本属性的知识。\n\n[23:09](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=1389)\n那向量有哪两个属性？有方向和大小和长度，对吧？那么既然是 b 向量投影在 a 市 a 向量上，那么这个投影一定是沿着 a 方向，对吧？所以说首先这个投影，这个所谓 $\\vec{b}_{\\perp}$，这里这个东西这个符号叫做perp，也就是 perpendicular 的缩写。\n\n[23:31](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=1411)\n这个 $\\vec{b}_{\\perp}$ 的这个方向就已经定下来了，它一定是沿着a，对吧？因为我们要算它沿着 a 的投影，那么一定我可以把这个 $\\vec{b}_{\\perp}$ 表示成这个 a 向量对应的方向，也就是这个 a 的单位向量 a hat 乘以一个长度这个k，那么现在只要我们能够把它的长度算出来就可以了，对不对？\n\n![](res/Lecture_02_Review_of_Linear_AlgebraPT22M34.603S.webp)\n\n那么怎么算它的长度呢？那大家看到这里这个有一个直角三角形关系，因为我们要把 b 向量投影成 $\\vec{b}_{\\perp}$，那么这里自然有一个这个直角三角形在这里存在，也就是从这个嗯， $\\vec{b}_{\\perp}$ 的终点到这个 b 的终点，然后这么一条边，然后还有 $\\vec{b}_{\\perp}$  本身加上这个 b 向量自己，那这个形成了一个直角三角形，那么 $\\vec{b}_{\\perp}$ 的长度是多少呢？那也就是 b 的长度乘以这个 a 和 b 的夹角余弦，对吧？\n\n然后也就是这里为什么是 b 的长度乘以 cosine c 的，然后这个夹角鱼弦怎么做大家已经知道了，直接拿这个点成这个 a 和 b 的点击，然后它会告诉我们夹角的鱼线，所以这里就等于是能够把一个这个，能够把这么一个向量在另外一个向量的投影算出来，那么投影算出来有什么好处，对吧？然后这个，嗯，投影算出来有一个好处，就是说我们可以把一个向量分解成两个向量，其中让一个方向这个平行于某方向，然后另外一个方向垂直于某方向。\n\n[25:01](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=1501)\n比如说像这里我们既然知道 $\\vec{b}_{\\perp}$ 怎么算，那么另外一个方向这个 b 减去 $\\vec{b}_{\\perp}$，我们就可以知道是什么。然后这里减法不给大家多做介绍，因为减法跟加法是互逆的，就是说这个，嗯， $\\vec{b}_{\\perp}$ 这一段加上这个 b 减去 $\\vec{b}_{\\perp}$ 这一段加起来之后，大家用平行四边形法则会发现，唉，这个就等于这个 b 向量没有问题。\n\n##### 计算两个向量的相近程度\n[25:25](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=1525)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT22M34.603S.webp)\n\n这就是点乘能够给大家带来的一个好处，就是说大家可以把一个向量任意的进行这个垂直与平行的分解，然后这是一个很重要的事情。那么在图形学里面这个点乘还会给大家这个另外的一些好处，比如说我们可以计算这个两个向量或者两个方向有多么接近，那什么叫接近呢？就是说我们可以算这个两个向量的这个点乘的结果，然后根据他们点乘的结果，我就知道它是否是接近还是远离这两个方向，那待会给大家说。\n\n##### 计算两个向量的方向是相同还是相反\n[25:60](file:///Z:/阿里云盘/技术美术/GAMES课程/GAMES101-现代计算机图形学入门-闫令琪/Lecture_02_Review_of_Linear_Algebra.mp4#t=1559.631683)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT26M8.868S.webp)\n\n然后这个向量的点乘还可以告诉大家关于一个前与后的信息，这个事情非常有用。\n那什么意思？咱们可以看一下这个例子，就是说大家可以看到如果这个有一个向量 a 给定了某一个方向，那么我可以这个考虑到这个，嗯，比如说从 a 的起点向这个整个上面方向看过去，以及向下面方向看过去，然后各会形成一个半圆。\n\n然后在这里整个大家可以看到一个整个圆被分成了两部分，一部分在这个上半部分，我们认为如果有一个向量是处于这一部分的，就是说它的终点是落在整个这个虚线以上的，那我们就认为这个 a 和 b 是属于基本上算是相同方向，或者说都是向前的。\n\n然后如果有一个向量，比如像这里向量c，它的终点落在了这个曲线的下部分，然后这个时候我们就认为说这个，噢， a 和 c 他们两个方向基本是相反的，那么这里怎么样判定这个事情？就是说这就是点成的一个好处，就是说我们如果用这个 a 和 b 向量求点乘，我们会发现这个点乘会给我们一个这个大于 0 的值，也就是正数。然后如果我们去点乘 a 和c，然后这里的点乘的结果会告诉我们这是负数。\n\n##### 计算两个向量的方向接近程度\n[27:19](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=1639)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT26M8.868S.webp)\n\n然后如果说有一个向量正好它的这个终点在这个虚线上，那么这个 a 和这个向量求了点乘之后，会给我们一个 0 这么一个结果，那就是说，嗯，这个向量的点乘可以告诉我们方向性，就是方向基本一致或者方向基本相反，或者方向垂直。那么同样道理，点乘也可以告诉我们这个这两个向量有多接近，比如说这个 b 向量和 a 向量它就比较接近，然后我们求点乘点击的时候，这个得到的结果就会比较接近1，然后如果说这个 b 向量渐渐这个方向和 a 远离了，比如说远离到一个什么程度到和 a 垂直，它就会渐渐变成0，然后这个再远就会渐渐变成-1，直到他们俩完全相反这个方向。\n\n[28:06](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=1686)\n然后他们两个如果都是方向量的话，那么点乘的结果会这个从这个 1 就是完全相同的方向变成完全相反的方向，也就是-1，就是说它这个值可以告诉大家这两个向量在方向上有多么接近。然后这个很有用的，就是说这个之后，这个大家会看到这个之后这个在图形学中的应用，然后这里就是点乘，那么嗯，这样哈，我们很快要给大家介绍这个叉乘。然后在这个之前我先看一下这几个大家之前有没有什么讨论的一些问题。\n\n##### 两个向量方向接近程度计算反射高光\n[28:55](\u003cfile:///Z:/阿里云盘/技术美术/GAMES课程/GAMES101-现代计算机图形学入门-闫令琪/Lecture_02_Review_of_Linear_Algebra.mp4#t=1734.942232\u003e)\n\n我明白了，大家有同学问这个说为什么要用这个？为什么默认是个列向量，这个是约定俗成的一件事情，当然可以用这个行向量，有一些书是用航向量没有问题，但是就是说更多的应用，比如说特别是在这个图形学的这些硬件上面，这个或者说 API 上面这个 OpenGL 之类的东西，大家默认都是在用这个这个默认的列向量。然后这样的话所有的矩阵可以佐乘这个之后我们再给大家说，然后嗯，这个有人问有什么应用点成的这个说为什么两个向量是否接近？这个有什么应用？这里这个很简单，比如说这个大家看一个镜子，然后有一束光打在这个镜子上，然后大家知道有这个镜面反射这么一个反射的定律。\n\n[29:30](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=1770)\n那如果说你的眼睛这个是，就是这个啊，嗯，就是从它的这个镜面反射出射光的这个方向上看过去，那你就会看到一个反射的点非常亮，如果你这个眼睛稍微这个和这个方向错开一点点，那你就看不到这个反射。那如果对于金属来说，金属的这个高光是怎么来的？同样的道理，就是说入射的入射光打到这个金属表面，它会反射，然后在这个它的镜面反射的这个方向周围，然后如果你从那这些方向去观测，那这个得到的结果都是这个基本上属于在这个高光里面，那如果说离得远了你就看不到。所以我们需要有一个办法来这个提供这个两个向量是否接近这么一个标准。\n\n### 向量叉乘\n[30:13](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=1813)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT30M42.148S.webp)\n\n好吧？那大概就说到这，然后这里这个我们继续往后说这个向量的这个叉积。那么向量的差级是什么意思？就是这是另外一种计算，它虽然也是乘法，但是它是和刚才的点击是完全不一样的计算。那么这个大家可以看到这是一个示意图，所谓这个叉机它会输入两个不同的向量 a 和b，然后它会给这个给出，也就是计算出另外一个向量，这个计算出的这个向量，也就是说这两个向量的原本这个输入向量的这个差积的结果和原本的两个向量都要垂直，那么大家想一想，那既然这么定义了的话，嗯，这个叉乘之后的向量假如说咱们把它记作c，那么 c 要垂直于 a C 又要垂直于b。\n\n[31:02](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=1862)\n那么 c 必然垂直于 a 和 b 所在的平面，那也就说明了这个向量 c 一定和 a 和 b 不在一个平面内，对吧？然后这个大家可以从这个公式的定义上面来看，就是说我们会写 a叉乘b，然后这个它的定义是这个 a 的长度乘以 b 的长度，再乘以 sin c 态其实还需要一个方向，这里这个这里写的并不并不正确，就是说不是说不正确，而是不完善，就是说这里只是给大家定义了 a 这个叉乘 b 的这么一个大小应该是多大？但是它的方向并没有写出来，然后它的方向应该如何决定？我们刚才说这个叉乘会给大家一个新的向量，垂直于 a 又垂直于b，这里就需要大家应用一个叫做右手定则的一个这个规律。\n\n#### 右手定则\n[31:51](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=1911)\n什么叫右手定则？很简单，伸出右手来，说起来是这样，有两种不同的右手定责，一个是伸出你的这个右手的三个指，这个拇指、食指和中指，这样的话，然后你会把它给摆成一个这个互相垂直的一个方向，这是一种右手定则。但是在这里我们给大家介绍的是另外一种叫做右手螺旋定则，大概是这么一个意思，就是好像大家点赞这么一个样子，什么意思？就是说大家是这个 4 指的方向，代表了一种旋转的方向，就是说我们要算这个 a 和 b 的这个叉乘，或者说叉积，那我们就从 a 旋转到 b 方向，那么这个拇指对应的方向就是它这个向量的这个这个他们两个向量叉乘出来得到的这个结果，它的方向。\n\n[32:37](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=1957)\n那么这里大家可以对照这个屏幕，这个 a 和 b 大家看一下，对吧？从 a 绕到b，那自然就是向上的，那那么这个给大家这个既然说了这个右手定责哈，那咱们大家可以试一下，那如果 b 插成 a 是什么呢？那大家可以想象一下，从 b 叉乘 a，抱歉，对大家做这个动作并不意味着什么事情。\n\n[32:57](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=1977)\n这个从 b 到 a 这个转过来，大家会发现这个 B叉乘 a 其实得到的一个向量正好和 A叉乘b 相反，他们的这个上下这个方向是相反的。然后这也就告诉我们向量的差积并不满换率，如果我们要交换两个向量的差乘的顺序，那么必须得加上一个负号，就是这么个意思了。\n\n#### 向量叉乘的作用\n[33:23](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=2003)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT33M39.441S.webp)\n\n好，然后这个，嗯，向量的差积有什么作用？我们可以用它来建立一个三维空间中的直角坐标系，然后这里大家就可以看到这是什么意思。比如说这个在三维空间中大家给定了一个 x 轴和一个 y 轴，然后我们可以通过 x 轴叉乘 y 轴的方式算出 z 轴是什么，这个就是这个向量叉乘的这么一个其中一个明显的作用。\n\n那同样道理，大家可以这个试试看这个比如说用 y 轴叉乘 z 轴会得到x，对吧？然后 z 轴叉乘 x 轴可以得到y，这个大家只要这个对右手这个螺旋定则把握对了，就应用对的话，就是这些正负。这个左边列出来这 6 个是一定不会有问题的，咱们就只用记住头一个就 x 叉乘 y 得到z。\n\n[34:11](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=2051)\n另外这里多说一句，就是说如果说在一个三维的坐标系里面， x 和 y 的差乘得到的是z，那么我们就说这个坐标系是一个右手坐标系，因为大家是通过右手这个螺旋定则算出来的这个 z 方向。然后我们在这门课里面考虑的全部都是右手坐标系。然后大家如果说会接触到这个 OpenGL 或者是一些别的 API 里面他们会假设说我们用的是左手坐标系，也就是说 XX 乘 y 得到的结果是负z，这个就不是很方便，特别是作为教学用途，我们觉得我们在这里就这个做一个这么一个假设，我们认为这个叉乘如果 x 叉乘 y 得到z，那这就是右手坐标系。\n\n[35:00](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=2100)\n然后咱们这个就一直用这个右手坐标系，好吧？然后我们看右侧这里面有几个不同的这个性质，我们刚才提到这个叉乘没有交换律，如果要交换顺序一定要加一个负号。然后我们现在想另外一个事情，就是说这个一个向量叉乘，它自己得到的是什么呢？得到的是0，为什么呢？咱们刚才回到刚才这个叉乘的这长度，叉乘的长度是两个这个输入向量的长度，再乘以它的这个夹角的这个正弦值。我们知道这个两个向量如果它是重合的，那它们两个夹角是0，正弦就是0，所以这个得出来的结果是一个长度为 0 的向量。\n\n[35:40](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=2140)\n这里是需要跟大家指出的，就是说这个不管怎么样向量叉乘得到的都是另外一个向量，所以它是一个向量，但长度是0，所以它是个 0 向量，所以它并不是一个 0 这么一个数字，这个大家知道就好。然后后面两个都相对简单，这个分配率和结合率仍然存在。\n\n\n[36:02](\u003cfile:///Z:/阿里云盘/技术美术/GAMES课程/GAMES101-现代计算机图形学入门-闫令琪/Lecture_02_Review_of_Linear_Algebra.mp4#t=2162.275838\u003e)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT36M3.391S.webp)\n\n好，这里就是说这个向量的这个叉乘，然后向量的叉乘在这个代数上也是可以直接写出来的。然后大家可以看到这里叉乘这个相对较为复杂一些，但是没有关系，大家看这个上半部分就是 a 向量叉乘 b 向量，假设都是 x a y a z a 这种表示方法，那么得到的是一个向量，大家可以看到又是一个列向量，是这个三个元素，三个元素分别是这么算出来的，知道就行。\n\n[36:25](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=2185)\n也就是说通过这个在这个笛卡尔坐标系下向量的叉乘，也可以非常明确的写出来，然后这里给大家这个多说一句，这个之后我们会在这个，我们会在这个后续的讲座里面再继续说，就是说这个向量的叉乘可以表示成矩阵形式，就是说我们可以把这个向量 a 写成一个对应的矩阵，然后我们再用这个矩阵去乘以这个向量b，然后一样可以得到这个上面写出来的这么一个结果。\n\n#### 向量叉乘的作用\n[36:56](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=2216)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT37M14.502S.webp)\n\n那么嗯，这里之后很快就会给大家讲，好吗？然后那咱们刚才提到这个向量的这个叉积或者叉乘这个怎么算，对吧？那么它到底有什么样的作用？特别是在图形学里面有什么样的作用？这个作用当然特别重要，那这里我列出两点，第一判定左和右，第二判定内与外，当然这两个是同一个概念，然后是什么意思？就比如说给你两个向量，然后你可以算出来它的这个差积是多少，那为什么就能告诉大家左和右的信息？\n\n[37:32](\u003cfile:///Z:/阿里云盘/技术美术/GAMES课程/GAMES101-现代计算机图形学入门-闫令琪/Lecture_02_Review_of_Linear_Algebra.mp4#t=2251.968471\u003e)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT37M33.187S.webp)\n\n那咱们看下面这么两个例子，那大家看左边这个这幅图，左边这幅图，假设大家现在在看一个平面，一个平面，然后这个平面是 x y 平面，也就是所有这个操作都是 x 跟y，这个 z 是大家可以通过这个右手螺旋定则，可以这么算出来， XX 乘 y 就从 x 绕到 y 那么大拇指对应的方向 z 算出来右手细没问题。\n\n##### 判断向量在左还是右\n[37:54](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=2274)\n那么这里如果大家有两个向量， a 和b，大家想判定这个 b 在 a 的左侧还是右侧，那什么叫左侧？右侧就是从 a 我这个向左旋转，或者说逆时针旋转，还是说向右旋转？顺时针旋转，就这么个意思，那现在这里大家可以看到，嗯，这个 b 向量应该是在 a 向量的左侧，没问题，我直接可以看得出来。\n\n[38:17](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=2297)\n那么在数字上如何表示？很简单，那么用 AX 乘 b 得到的结果，大家会发现这个 z 是正的，那就说明 b 在 a 的左侧，那如果说反过来，那 a 是不是在 b 的右侧？咱们可以试一试。比如说 b 插成a，那应该是这么一种方式啊， b 插成 a 这样一种右手螺旋的方式，然后大家会发现，噢，大家得到一个向量，这个向量的 z 必然是负的，那就说明这个 a 是在 b 的右侧，然后这个是没有问题的。所以说叉机可以告诉大家这个向量的左和右这么一个关系好。\n\n##### 判断向量在内还是在外\n[38:50](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=2330)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT37M33.187S.webp)\n\n然后，嗯，那么我刚才还提到判断左和右是一方面，那么另外一方面就是判断内和外这一点在这个图形学里就得到相当广泛的应用了。比如说一个非常这个直接的例子，现在大家看到右侧有一个三角形a、b、c，然后有一个点 p 点，这个 p 点我想问他是不是在三角形内部？嗯，就是这么一个事情，那这个问题大家要如何回答？那这里就需要用到咱们的这个这个这个叉机的这么一个应用了。\n\n那么我们刚刚提到左和右这么一个信息，假设说咱们有这个a、b、 c 这么三个点，这三个点是逆时针排列的，那么咱们可以做这么一个，这是差级，谁和谁呢？先看 a b 和 a p，就是说这个点p，现在是大家看到在三角形里，然后这个 a b 叉乘 a p 得到的结果是向外的，对吧？然后也就是说这个 p 点是在 a b 的左侧，没问题，那咱们对于 b c 来说可以做相同的操作，用 b c 去叉乘 b p，我们就可以得到o，OK，没问题， p 点仍然在 b c 的左侧。\n\n[40:03](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=2403)\n然后呢，大家再看 c a，大家注意这个绕的顺序， a b c。所以应该是 a b b c c a。然后大家如果看这个 c a，那么这个 p 点也在 c a 的左边，诶，那，那这就说明什么呢？这就说明这个 p 点在三角形的内部，否则的话一定至少得有一边，使得这个 p 点它是在这个某一个这个边的右侧。我不信的话，大家可以把这个 p 点大家脑补一下。把这个 p 点放在这个 a C 这个右边，放在 a C 右边，大家会发现，OK，判断 a b 的时候，好，它仍然在 a b 的左边。没问题。判断 b c 的时候， b c 叉乘 b p 仍然可以得到这个结果，它在 b c 的左边，那么在判断 c a 和 c p 的时候就不是了，因为那个时候如果 p 点在外面的话，那就变成了这个 p 点在 CA 的右边。\n\n[40:52](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=2452)\n对，所以说通过这个叉乘，我们可以立刻的判断这个三角点是不是在三角形内。那么这个大家可能会问一个问题，这里你先假设了三角形这个a、b、c，它的这个绕向它本身这个规定，这三个点就是逆时针排布。那假如说我有个三角形，我比较倒霉，我写成了 a C、 b 这么一种形式，就是a、b、 c 的排布变成了顺时针，那这怎么办？这个没有问题，为什么呢？因为这个时候你会发现在这个时候这个 p 点都在三条边的右边，也就是说如果大家想判断的一个就是说最准确，最这个合理。\n\n[41:32](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=2492)\n对于任何的这个三角形，a、b、c，不管你的输入顺序是逆时针还是顺时针的，那我们都可以说这个都可以算对，那怎么判断呢？就判断这个 p 点一定一直在三条边的左边，或者是都在三条边的右边就对了，就是这样的话，就直接就可以这个忽略这个三角形给定的这个三角形三个点的顺序，那么这点非常之重要，这就是之后大家做这个三角形的这个光栅化的一个基础，我们要判断三角形覆盖了哪些像素，那我们自然要知道这个像素这个是不是在三角形内部，然后我们好给这个像素进行着色。\n\n[42:11](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=2531)\n对，那么有同学会问了，那如果得到结果正好是0，那怎么办？那算是在内部，算是在外部，就是说这个正常情况下，在图形学里面，咱们这个正常的考虑就是像这种，我们管它叫 corner case， corner case 意思就是说呢这种情况你自己决定，你说它在里面行没问题，说它在外面也可以。咱们之后做光线追踪也会涉及到这个问题，假如说有一个光线正好擦至三角形一个边过去了，你说这算相交不算这个这自己说了算，就是这么个意思。\n\n## 正交系 和 坐标系\n[42:42](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=2562)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT43M8.996S.webp)\n\n好，然后这个这就是我们这个关于这个向量叉乘的一个应用，然后向量叉乘之后我们还会再继续提到。好，那么这里既然定义了这个向量的这个基本操作，然后我们就可以说用这些向量来定义一些这个不同的这个坐标系，其实刚才咱们已经这个跟大家说了，对吧？然后这个就是说我们用这个向量的叉乘，我们可以定义一些互相垂直的轴，让这些轴就会形成一个坐标系，我们刚才提到右手系，对吧？\n\n然后我们如果定义这样一个这个坐标系，向这里定义u、v、w，然后u、v、 w 认为这 3 个向量都是单位向量，然后并且它们之间的这个都互相垂直，也就是点乘的结果都是0，然后给你 u 和v，你是叉乘得到w，对应就是 x y 叉乘得到z，这样的话得到的结果自然就是这个一个右手的这个直角坐标系，然后三维的那么这样定义一个坐标系有什么好处？我们就可以把任意一个向量都给分解到这三个轴上去，就是这个向量的分解。\n\n### 定义\n[43:49](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=2629)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT43M55.007S.webp)\n\n咱们刚才已经说了，这个就要利用他们的这个投影，他们的投影怎么算？就是用点击，而且这里有一个好处是u、v、 w 都是单位向量，所以他们这个一个向量投影到这个对应的这个方向上，他们的长度是多少？这个直接用点乘的结果就立刻可以得到，因为什么呢？点成，比如说 p 点成u，它是什么呢？\n\np 的长度点成 u 的长度再乘以 Cos Theta，但是 u 的长度偏偏就是1，所以它这个结果就是 p 的长度乘以 Cos Theta，所以正好就是它投影的长度，然后投影的长度乘上它的这个 u 方向的单位向量，就把它变成了一个在 u 方向上的投影的向量，然后 v 方向、 w 方向三个方向都是这么投影得了，三个向量加起来就是原来的向量。\n\n## 矩阵\n[44:30](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=2670)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT45M6.814S.webp)\n\n这就是为什么我们喜欢用这个直角的这个坐标系，因为它非常方便，对吧？那这里就是当然了另外一个点乘的一个向量，一个应用，那么到现在我看一下这个大家有没有什么这个什么问题，应该差不多，我觉得那咱们就继续，那咱们这里要再给大家提到的就是矩阵，对，那么这个矩阵这个大家可以看到几乎所有的这个计算机的课都会涉及到一些这个矩阵的一些操作，然后这个当然在大家这个可能会很头疼。\n\n但是矩阵其实并不难，咱们这个给大家说一说，特别是应用在这个这个变换上，矩阵其实挺简单的。然后这个我们这个之后会给大家特别提到说，我们如何用矩阵表示一些基础的一些变化，比如说这个移动、旋转、缩放、错切之类的这种操作。然后嗯，在图形学里，当然了这个变换就是矩阵的一个最大的应用。\n\n### 矩阵是什么？\n[45:30](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=2730)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT45M36.653S.webp)\n\n那么矩阵是什么呢？很简单，矩阵就是一堆数，然后这堆数把它给安排在一个这个平面上，然后这个变成一个几行几列的这么一个结构，对吧？像这里大家看到一个矩阵，这个矩阵是 3 行两列，所以是这个 3* 2 这样的一个矩阵，然后这个矩阵的一些基本操作，比如说矩阵可以乘以一个数，然后成立一个数，很简单就把矩阵里面每一个数都乘这个数就可以了。\n\n![](res/Lecture_02_Review_of_Linear_AlgebraPT46M3.388S.webp)\n\n然后这个非常简单，那么咱们这个就直接忽略它，那么矩阵最困难的操作也其实最有用的操作是什么呢？是矩阵的乘积。那么矩阵的成绩，这个首先前提要求你要是给定两个矩阵，它两个必须得能乘才行。那什么是这个可以成的矩阵？那就是说如果大家看这两个矩阵分别是 3 行 2 列和 2 行 4 列，如果写成这个形式，大家可以看到上面 m 乘n，然后点成，这不就是这个 m 乘 n 的一个矩阵去乘以一个 n 乘 p 的矩阵，这样乘才有意义，也就是说第一个这个矩阵它的列数必须得等于第二个矩阵的行数。\n\n### 矩阵乘法\n[46:42](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=2802)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT46M57.483S.webp)\n\n然后这个我觉得这个就直接用这个几乘几这样看着最明白，就是说这个中间这个 n 必须相同，也就是这里 3* 2，那必须乘以一个 2 乘什么什么什么，在这里是 2* 4，那乘的结果其实也很明白，就是说就好像是中间这个 n 抵消了一样，就相当于变成了原来 m 乘n，然后这个这样一个矩阵乘以 n 乘 p 这么一个矩阵，那么我们就把它得到的结果就是一个 m 乘 p 的一个矩阵。\n\n然后就是有这么一个性质，那么关于这个矩阵，这个新的这个矩阵，它这个每一个元素都是什么？这个中间有各种各样不同的数学定义，也是这个地方比较容不容易记住。但是这里我给大家提供一个简单一点的记法，然后这个怎么做？那大家首先看这个例子，是 3 行 2 列 3* 2 的矩阵乘以一个 2* 4 的矩阵，那么大家知道最后得到结果肯定是 3* 4 的矩阵不会错，咱们先把这个大小定下来。\n\n[47:37](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=2857)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT46M57.483S.webp)\n\n那么现在我就问这么一个事情，就是说每一个元素，比如说这里大家看到这个 26 这么一个数字，就是 2 行 4 列，在这个最后结果上这是怎么算出来的？那就是这么算的，因为我要想算这个 26 是多少，他我知道我想算的是 2 行 4 列的结果，那么我们就去找第2行和第4列，分别是在两个这个输入矩阵里面找，很简单，比如说这个 26 是二行四列，那么我们找原来的第二行 5 和2，然后四列是 4 和3，那么这两个向量找到了求一个点击就可以了。比如 4 乘 5 20 2* 3 得6，加起来26，就是这么回事。\n\n然后比如说这个，我们看这个左下角这个 8 就是新的这个矩阵左下角这个8，然后它应该是 3 行一列，那 3 行一列很简单，我们就找第三行，找第一列，那第三行是在第一个矩阵里面 0 和4，然后第一列是指这个第二个矩阵里面 3 和2，那么他们两个乘积，他们两个点击就是 3* 0 得 04* 2 得8，嗯加起来就是8。\n\n### 矩阵乘法的性质\n[48:40](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=2920)\n所以这样做就比较简单，就是说省得去记一系列的这个各种各样的这个复杂的公式， i j k 的什么东西不需要？咱们就利用了一下点乘的这个概念。好吧？就是记得需要算第几行、第几列，那就去找第几行和第几列就可以了。好吧，那这就是这个矩阵的成绩。那么矩阵的成绩这个有一个非常重要的性质，这个性质就是没有任何交换率。\n\nOK，所以说 a 乘以b，正常情况下都不等于 b 乘以a，有一些情况下是这个等于的，但是更通用的情况下是这两者基本不等于。所以说 a 和 b 乘a，这个两个是不能交换顺序的。但是矩阵的结合率和分配率都有，特别是什么比较有用？结合律很有用。\n\n#### 矩阵乘法的结合律\n[49:29](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=2969)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT49M31.862S.webp)\n\n什么叫结合率？就是a、b、 c 这个放在一块乘，那我可以先乘a、 b 再乘c，或者先乘b、 c 再乘a，只要我不涉及到把它顺序对掉这种就可以，就是说结合率是有的，就是不能交换，交换率是不行的，对吧？就这个意思，那当然这个分配率很简单，就是这个意思了，然后呢？这就是这么一个性质，然后就记得它没有这个交换率就好了。\n\n#### 矩阵和向量相乘\n[50:02](\u003cfile:///Z:/阿里云盘/技术美术/GAMES课程/GAMES101-现代计算机图形学入门-闫令琪/Lecture_02_Review_of_Linear_Algebra.mp4#t=3002.099688\u003e)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT50M29.088S.webp)\n\n然后那么一个矩阵如何在一个特殊的情况如何和一个向量成，那就是说我认为这个向量是一个列向量，这里就有意义了，那么这里我就始终认为这个矩阵在左边，然后向量在右边，然后向量永远都是一个列向量，也就是永远都是 m 乘 1 的这么一个矩阵，然后左边这个矩阵只要它是什么乘 m 就可以，这样就可以在一块乘。\n\n[50:21](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=3021)\n那这就是最重要的核心，也就是说我们要算这个变换，比如这里可以给大家剧透一下这个，咱们下节课要给大家说这个变换。一个最简单的变化，就是说任何一个形状二维的，然后咱们要把它给按 y 轴做一个对称操作，那 y 轴做对称操作，也就是说给你任何一个x，你都得把它变成负x，那 y 的话不变，因为它是按 y 轴进行操作，也就是 x 轴在变好。那也就是说给你 x y，然后一定会这个把它给变化成负 x 和y。\n\n[50:56](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=3056)\n那么什么样的矩阵可以让 x y 变成负 x 和y，那就是左边的这个看大家看到这么一个矩阵，- 10 和 0 和 1 这么一个矩阵，大家可以乘 1 乘试试，大家就会发现这个确实是这样，那这就是一个左边这个矩阵，就是一个这个用来求这个镜像的这么一个矩阵，当然咱们之后会说的更多，好吧，这里这简单给大家提一下而已。那么矩阵还有什么什么别的操作呢？\n\n### 矩阵的转置\n[51:24](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=3084)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT51M26.584S.webp)\n\n很简单，矩阵还有一些这个转制操作，咱们刚才在这个这个向量上已经说了，那就是把行和列互换，那矩阵上也一样，那么原本 3 行 2 列，那这里自然变成 2 行 3 列，那这个原本的这个 i 行 j 列变成了这行 i 列，就是把它给这个调换一下顺序。\n\n然后这个矩阵的转制有一个这个性质，就是说如果你要乘两个矩阵再转制，那就好像你先对后一个矩阵做转制，再乘以前一个矩阵做转制。所以这是一个比较有意思的现象，它这个就是说乘积的转制，你要先把它这个顺序调过来，然后分别做转制再做乘积。这里就是一个这个这和直观上并不一样的事情，就是说这里是矩阵的一个性质转制。\n\n### 单位矩阵\n[52:13](\u003cfile:///Z:/阿里云盘/技术美术/GAMES课程/GAMES101-现代计算机图形学入门-闫令琪/Lecture_02_Review_of_Linear_Algebra.mp4#t=3133.240908\u003e)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT52M16.438S.webp)\n\n然后另外一个给大家提了一个特殊的矩阵，叫做这个单位矩阵，然后这个单位矩阵什么意思？它是一个对角阵，所谓什么叫对角阵？就是说这个就是说只有对角线上有这个非 0 的元素在这里，这个对角线上全都是1。\n\n[52:30](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=3150)\n什么对角线？左上到右下这条上面就是说单位矩阵也分这个它的这个维度或者大小，像这里大家看到的是一个 3* 3 的单位矩阵，然后单位矩阵是什么意思？或者说它用来干什么呢？单位矩阵基本不做任何操作，比如说这个单位矩阵，咱们管它叫i， i 如果乘以a，得到的结果一定还是a，然后 a 乘以i，得到结果也应该是a，然后就它基本没用。它没用，但是可以定义下面一个式子，叫做这个矩阵的逆。\n\n### 矩阵的逆\n[52:59](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=3179)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT52M16.438S.webp)\n\n矩阵的逆是什么意思？如果你能找到一个矩阵，和你原来的矩阵相乘，然后不管顺序，然后得到的结果就是得到的结果都是一个这个单位矩阵，那么我们就认为这两个矩阵是互逆的，那也就是说我们原本有一个这个矩阵a，然后它乘以 a 的逆就可以得到单位矩阵。\n\n那么当这个逆矩阵的计算和这个转置的计算其实很相似，就是说如果有乘积的逆，其实相当于先把这些这个对应的这些矩阵逆过来，然后在这个相乘，然后并且顺序是反的。这里可以看到大家先这个对 b 进行逆，然后再对 a 进行这个逆，然后再乘起来，这当然是它的一个性质。\n\n### 向量乘法矩阵的形式\n[53:42](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=3222)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT53M44.68S.webp)\n\n然后这个我们刚才提到说向量可以写成这个，可以有这个点乘和叉乘的操作，那么其实向量的点乘和叉乘都可以写成矩阵形式。什么意思？比如说咱们看这个点乘，就是 a 点乘， b 可以写成 a 转制和 b 的乘法，就是 a 转置。\n\n你认为 AA 如果是一个，这个原本是一个列向量， a 转制变成了一个行向量，那么这个航向量直接乘以这个列向量b，得出来的结果正好就是一个数，就是大家看行向量是多少，一行 3 列，然后这个 b 的列向量是什么呢？ 3 行 1 列，所以一行 3 列乘 3 行 1 列，一定得到 1 行 1 列，也就是一个数，所以大家在右边看到也就是一个数，没有问题。\n\n[54:26](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=3266)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT53M44.68S.webp)\n\n那么这个，嗯，叉乘是否能写成矩阵的形式？这个相对困难一点，但是其实是可以的，就是说我们把这个 a 向量给这个重新组织一下，写成这个 a 星号这么一个，这么一个这个矩阵，这个不是 a 乘b，这个是 a 星号，是一个矩阵，然后它乘以这个 b 向量。OK，所以这个 a 星号是什么呢？就叫做这个 Dio matrix。\n\n然后然后这个就是从 a 把它这个向量转化成这么一个矩阵，大家可以看到是怎么排的，这个 x 要排在哪儿？ y 排在哪儿？ z 排在哪儿？加什么负号。然后就是说通过这个矩阵乘以这个 b 向量得到的结果，就是这个叉乘的结果。唉，我们为什么要这个东西？就是说我们也之前有 a 和 b 的话，我是直接是可以算这个这个叉乘的，我们之后会看到在一个旋转上面，这个这个旋转的一个推导，上面这一步是其实挺有用的，也就是说在这里大家知道说这个向量的乘积也可以写成矩阵形式就可以了。\n\n## An Example of General Transformation\n[55:27](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=3327)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT55M30.004S.webp)\n\n那么给大家看一个例子，这个例子就是这个一个运动着的相机，一个运动着的相机，然后在一个这个某一个场景中间，这个场景还是很有名的，叫做Sponsa。\n\n[55:41](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=3341)\n如果大家做这个这个 render 云做一点点，大家会知道有这么东西，然后他这个相机在这个场景中任意移动，然后大家可以想象一下这个相机，你看可以往各个不同的方向看，所以说涉及到旋转，对吧？然后它又可以往各个地方不同地方走，所以如何去定义这个相机，如何去运动，那自然就是涉及到我们的这个各种各样不同的这些变换，咱们要把它合在一块，那这就是说之后要给大家讲的内容，然后呢？到目前为止，嗯，好，OK，OK，好像大家这个没说什么，太那个的问题哈。\n\n## 提问与回答\n[56:25](\u003cfile:///Z:/阿里云盘/技术美术/GAMES课程/GAMES101-现代计算机图形学入门-闫令琪/Lecture_02_Review_of_Linear_Algebra.mp4#t=3385.037831\u003e)\n![](res/Lecture_02_Review_of_Linear_AlgebraPT56M55.968S.webp)\n\n噢，我明白了，有同学这个反映了一个事情，说这节课挺简单的，没问题，大家说的太对了，这节课非常简单，可是在 3 节课之后就会变得非常困难。好吧，大家知道有这么一个事情就好。然后就是说咱们由浅入深的说嘛，先把这一块简单的东西说明白，但是一定要相信我这课，这个只会越来越难好吗？这个这节课简单是对的，就是给大家复习一下，给告诉大家一个信息，这个线性代数，特别是咱们图形学上要用的线性代数，基本就到此为止了，到这儿就等于是就已经够了，为了学习后面的这个图形学的内容。\n\n[57:03](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=3423)\n所以我觉得如果大家觉得这个课这节课简单，那就太好了，没有什么问题，就等于是咱们把这个先行代数复习一遍，然后咱们之后这个下节课开始，到下节课咱们会讲更多的内容，比如说咱们会提到这个二维的旋转，然后不二维的各种各样的变换。然后以及说我们要开始提到一个叫做这个，这个我想一想中文应该叫，嗯，中文应该叫什么呢？这样吧，我请同学帮我翻译，我要这个跟大家说的一个概念，叫做 homogeneous coordinate，然后这个大家有知道的吗？这个应该翻译成什么？我来关注一下。\n\n[57:55](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=3475)\n对，其次坐标，对对对对对，就是这个，然后这个涉及到这块就会渐渐的开始会变得这个稍微麻烦一点，然后这个之后这个咱们慢慢的把这些事情都给说明白，好吧，希望大家之后的这几节课都会觉得跟这节课一样简单，那就成功了好吗？那咱们下一节课就会开始说transform，好吧？当然了，不是变形，而是变坏，好吧，那咱们这个这节课，哎，时间正好。\n\n[58:25](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=3505)\n那咱们就说到这里，好吗？感谢大家，然后之后希望大家继续支持，我也希望把这个课，这个每节课都说这么简单，那就好了，好，OK，对，还有一个事情，这个我在想我现在在麻烦我们的助教同学试图做一个这个作业 0 的框架。大家知道我们有那么多次作业，然后就是说我希望大家上手作业 1 的时候要相对容易一些，所以大家这个准备一个作业0，所谓作业 0 其实并不是这个一次什么作业，就是大家把这个环境配起来，这个就算是作业 0 了，没什么事情，就是说这样呢，提早也可以给大家放出来这个虚拟机和这个对应的用法。\n\n[59:07](file:///Z:\\阿里云盘\\技术美术\\GAMES课程\\GAMES101-现代计算机图形学入门-闫令琪\\Lecture_02_Review_of_Linear_Algebra.mp4#t=3547)\n大家也可以熟悉一下 visual studio code 的是一个怎么样的这个运作方式，然后之后大家写代码怎么做，就总之作业 0 没有任何实质内容，就是给大家上上手好吗？那就是这么一个事情，然后对这个之前提到这个，如果有时间咱们说一说变换，看来是没时间了，那咱们下节课再继续做好，同样这个课程结束之后官网会有录播，然后我也会立刻把这个课程的这个 PDF 对应的讲义，然后放在我们的课程网站上，好吧？那就这样，之后咱们这个下节课再见。\n\n","lastmodified":"2023-06-07T15:55:55.950775144Z","tags":["games101"]},"/notes/hosting":{"title":"Deploying Quartz to the Web","content":"\n## Hosting on GitHub Pages\nQuartz is designed to be effortless to deploy. If you forked and cloned Quartz directly from the repository, everything should already be good to go! Follow the steps below.\n\n### Enable GitHub Actions Permissions\nBy default, GitHub disables workflows from modifying your files (for good reason!). However, Quartz needs this to write the actual site files back to GitHub.\n\nHead to `Settings \u003e Action \u003e General \u003e Workflow Permissions` and choose `Read and Write Permissions`\n\n![[notes/images/github-actions.png]]\n*Enable GitHub Actions*\n\n### Enable GitHub Pages\n\nHead to the 'Settings' tab of your forked repository and go to the 'Pages' tab.\n\n1. (IMPORTANT) Set the source to deploy from `master` (and not `hugo`) using `/ (root)`\n2. Set a custom domain here if you have one!\n\n![Enable GitHub Pages](/notes/images/github-pages.png)*Enable GitHub Pages*\n\n### Pushing Changes\nTo see your changes on the internet, we need to push it them to GitHub. Quartz is a `git` repository so updating it is the same workflow as you would follow as if it were just a regular software project.\n\n```shell\n# Navigate to Quartz folder\ncd \u003cpath-to-quartz\u003e\n\n# Commit all changes\ngit add .\ngit commit -m \"message describing changes\"\n\n# Push to GitHub to update site\ngit push origin hugo\n```\n\nNote: we specifically push to the `hugo` branch here. Our GitHub action automatically runs everytime a push to is detected to that branch and then updates the `master` branch for redeployment.\n\n### Setting up the Site\nNow let's get this site up and running. Never hosted a site before? No problem. Have a fancy custom domain you already own or want to subdomain your Quartz? That's easy too.\n\nHere, we take advantage of GitHub's free page hosting to deploy our site. Change `baseURL` in `/config.toml`. \n\nMake sure that your `baseURL` has a trailing `/`!\n\n[Reference `config.toml` here](https://github.com/jackyzha0/quartz/blob/hugo/config.toml)\n\n```toml\nbaseURL = \"https://\u003cYOUR-DOMAIN\u003e/\"\n```\n\nIf you are using this under a subdomain (e.g. `\u003cYOUR-GITHUB-USERNAME\u003e.github.io/quartz`), include the trailing `/`. **You need to do this especially if you are using GitHub!**\n\n```toml\nbaseURL = \"https://\u003cYOUR-GITHUB-USERNAME\u003e.github.io/quartz/\"\n```\n\nChange `cname` in `/.github/workflows/deploy.yaml`. Again, if you don't have a custom domain to use, you can use `\u003cYOUR-USERNAME\u003e.github.io`.\n\nPlease note that the `cname` field should *not* have any path `e.g. end with /quartz` or have a trailing `/`.\n\n[Reference `deploy.yaml` here](https://github.com/jackyzha0/quartz/blob/hugo/.github/workflows/deploy.yaml)\n\n```yaml {title=\".github/workflows/deploy.yaml\"}\n- name: Deploy  \n  uses: peaceiris/actions-gh-pages@v3  \n  with:  \n\tgithub_token: ${{ secrets.GITHUB_TOKEN }} # this can stay as is, GitHub fills this in for us!\n\tpublish_dir: ./public  \n\tpublish_branch: master\n\tcname: \u003cYOUR-DOMAIN\u003e\n```\n\nHave a custom domain? [Learn how to set it up with Quartz ](notes/custom%20Domain.md).\n\n### Ignoring Files\nOnly want to publish a subset of all of your notes? Don't worry, Quartz makes this a simple two-step process.\n\n❌ [Excluding pages from being published](notes/ignore%20notes.md)\n\n## Docker Support\nIf you don't want to use a hosting service, you can host using [Docker](notes/docker.md) instead!\nI would *not use this method* unless you know what you are doing.\n\n---\n\nNow that your Quartz is live, let's figure out how to make Quartz really *yours*!\n\n\u003e Step 6: 🎨 [Customizing Quartz](notes/config.md)\n\nHaving problems? Checkout our [FAQ and Troubleshooting guide](notes/troubleshooting.md).\n","lastmodified":"2023-06-07T15:55:55.950775144Z","tags":["setup"]},"/notes/ignore-notes":{"title":"Ignoring Notes","content":"\n### Quartz Ignore\nEdit `ignoreFiles` in `config.toml` to include paths you'd like to exclude from being rendered.\n\n```toml\n...\nignoreFiles = [  \n    \"/content/templates/*\",  \n    \"/content/private/*\", \n    \"\u003cyour path here\u003e\"\n]\n```\n\n`ignoreFiles` supports the use of Regular Expressions (RegEx) so you can ignore patterns as well (e.g. ignoring all `.png`s by doing `\\\\.png$`).\nTo ignore a specific file, you can also add the tag `draft: true` to the frontmatter of a note.\n\n```markdown\n---\ntitle: Some Private Note\ndraft: true\n---\n...\n```\n\nMore details in [Hugo's documentation](https://gohugo.io/getting-started/configuration/#ignore-content-and-data-files-when-rendering).\n\n### Global Ignore\nHowever, just adding to the `ignoreFiles` will only prevent the page from being access through Quartz. If you want to prevent the file from being pushed to GitHub (for example if you have a public repository), you need to also add the path to the `.gitignore` file at the root of the repository.","lastmodified":"2023-06-07T15:55:55.950775144Z","tags":[]},"/notes/machine_leaning/begin_home_work/%E4%BD%9C%E4%B8%9A%E8%AE%AD%E7%BB%83%E8%90%A5-%E7%AC%AC%E4%B8%80%E5%91%A8":{"title":"机器学习基础作业班 第一周","content":"\n## 理论部分\n\n### 1. 请你来尝试回答，在机器利⽤鸢尾花数据集对鸢尾花进⾏分类任务的学习过程中，三要素分别对应什么？\n\n\u003e T：对给定的单个鸢尾花数据（萼片长度、萼片宽度、花瓣长度、花瓣宽度）预测对应的标签值（类别）\n\u003e \n\u003e E：150个鸢尾花数据样本（萼片长度、萼片宽度、花瓣长度、花瓣宽度）和对应的标签数据（类别）\n\u003e \n\u003e P指标：对给定的鸢尾花数据（萼片长度、萼片宽度、花瓣长度、花瓣宽度）预测对应的标签值（类别））的准确度\n\n### 2. 如何理解“使⽤机器学习解决问题的⽬的是为了获得⼀个复杂函数”这句话？\n\n\u003e 用计算机解决问题可以看成是用 已知的信息 进行计算最终可以得到 需要的信息，也就是 F(已知信息) = 需要的信息\n\u003e 所以机器学习的过程可以理解为 可以从 经验E 中让学习系统去调整 函数F(已知信息) = 需要的信息 黑箱中的系数，让 P指标 也就是 函数F(已知信息) 结果的准确度更高\n\n### 3. 请分别以⼿写数字识别数据集（MNIST）和鸢尾花数据集（iris）为例解释以下相关概念：样本、特征、特征取值、特征向量、特征维数；类别与类别标签、训练集、测试集、验证集。\n\u003e 样本是指被观察或研究对象的个体，MNIST手写数字识别数据集的样本是各种手写数字图片，特征是每个图片中每个像素点的灰度值，特征取值为0~255，特征向量是包含所有像素点灰度值的向量，特征维数为784（28x28）。\n\u003e MNIST 手写数字识别数据集的类别是数字 0~9，共有 10 类。类别标签是数字数组，取值范围为 0~9，与图像一一对应。训练集包含 60,000 幅图像，用于神经网络模型学习。测试集包含 10,000 幅图像，用于评估模型的识别准确率。针对 MNIST 数据集，将训练集的 60,000 幅图像分成 50,000 幅训练图像和 10,000 幅验证图像。验证集可以用于调整神经元网络中的超参数，如学习速率等。\n\u003e 鸢尾花数据集有150个样本，每个样本都是一个鸢尾花植物，包括萼片、花瓣等部位的测量结果。特征是指描述每个样本的属性，鸢尾花数据集的特征包括萼片长度、萼片宽度、花瓣长度和花瓣宽度。特征取值是指每个特征对应的值，这些值可以是实数或整数等多种类型，具体取决于样本的属性。特征向量是指将每个样本的特征按一定顺序组成的向量，鸢尾花数据集的特征向量长度为4，分别对应每个特征的值。特征维数是指特征向量的长度或者枚举类型的种类数，鸢尾花数据集的特征维数为4，即每个样本有4个特征。\n\u003e 鸢尾花数据集包括150个样本，每个样本对应一朵鸢尾花，其中记录了萼片长度、萼片宽度、花瓣长度和花瓣宽度等特征。每个样本属于3种不同的鸢尾花品种之一，分别为Setosa、Versicolor和Virginica。这些品种被称为鸢尾花数据集的类别标签。\n\u003e 在对该数据集进行机器学习任务时，通常会将这些数据划分为三个部分：训练集、验证集和测试集。其中训练集用于训练模型，验证集用于为模型选择超参数和对模型进行评估，最后测试集用于评估模型的最终性能。具体划分比例可以根据具体情况进行设置。\n\n### 4. \"识别或预测给定的⼿写体数字图像的类别\"是⼀个典型的分类任务，除此之外，机器学习常⻅的学习任务还包括“回归”、“聚类”、“特征降维”等，请分别举例解释说明。\n\u003e 分类：如果最后的预测结果是离散且有限的，那么就是分类任务，比如说识别人像照片是男是女。\n\u003e 回归：如果最后的预测结果会是连续的，那么就是回归问题，比如说线性回归，需要用一条直线来拟合测试数据。\n\u003e 聚类：是给定的数据没有标签的情况下，根据数据的特点（比如说在所在空间的距离）自进行类别划分。\n\u003e 特征降维：对训练数据进行学习时，数据的维度越大那么学习和训练的难度也就越大，可以将多维数据转换为带有其中要特征的低维数据的过程就是数据降维。\n\n### 5. 举例解释监督学习、⽆监督学习（⾮监督学习）和强化学习分别是怎样的学习范式。\n\u003e 监督学习：提供给学习系统的数据是带有预先标注好的信息的（标签），也就是我们需要预测得到的信息。\n\u003e 无监督学习：提供给学习系统的数据是没有预先标注的信息，需要学习系统自己从中学习数据之间的规律与特征。\n\u003e 强化学习：引入了奖励函数，如果做对了某一些事情就可以得分，这个算引入了“人在回路”的学习\n\n### 6. 让机器进⾏学习前需要先确定三件事： 1. 选择⼀个具有未知参数的模型；2. 定义损失函数；3. 确定优化⽅法。之后便根据优化⽅法开始训练模型，直⾄达到预期标准后，将训练好的模型进⾏保存，就可以⽤它对未知数据进⾏预测了。这个过程具体是怎样的？请举例详尽说明。\n\u003e 1：选择⼀个具有未知参数的模型其实也就是根据需要解决的问题，选择合适的模型结构（超参数），而此时的模型中参数是未知的\n\u003e 2：定义损失函数就是用来度量模型预测值和真实值差异程度的非负实数函数，用来评价当前模型预测的准确程度\n\u003e 3：确定优化⽅法就是确定使用哪些优化模型的算法，根据损失函数变化的程度来确定如何去调整模型中的参数\n\n### 7. 经常⽤“⽋拟合”和“过拟合”来对模型性能进⾏评价，请举例解释这两个概念。\n\u003e 比如说用神经网络来拟合一些点，欠拟合就相当于是当前神经网络生成的曲线在训练数据上都没有很好的拟合。而过拟合就相当于是很好的拟合了训练数据，但是是以蜿蜒曲折的曲线进行拟合的，并没有学习到一般的规律。\n\n### 8. 请举例说明如何基于交叉验证进⾏模型的选择与评价？\n\u003e 有两种交叉验证模型的方法，一个是 K折交叉验证，一个是重复的K折验证。\n\u003e  K折交叉验证：是一种评估机器学习模型性能的方法。它将可用数据划分为K个互不重叠的子集，每次用K-1个子集作为训练集，剩下的那个子集作为验证集，这样就可获得K个训练/测试集。最终返回di 的是这K个测试结果的均值。这种方法可以评估模型的泛化性能，避免验证集和测试集过小造成的方差小或者过大。\n\u003e  重复的K折验证：它是对K折交叉验证方法的改进，特别适用于可用数据相对较少的情况。在重复的K折验证中，使用K折验证方法来多次对数据进行分割和评估，最终的评估结果是每次K折验证结果的平均值。它通常会先将数据随机打乱，然后进行多次K折验证来提高评估结果的准确性。\n\n## 编程部分\n\n###  1. NumPy 、  Pandas 、  Matplotlib 分别通常⽤来解决什么样的问题。\n\u003e Numpy 是用来解决矩阵运算的，Pandas很像表格数据，可以用来进行查询，而Matplotlib可以进行图表的绘制\n\n###  2. NumPy 和  Pandas 中的数据类型是什么。\n\u003e Numpy中的数据类型是 NDArray，而Pandas中的数据是DataFrame 还有 Series（DataFrame可以看成是多个Series组成的字典）\n\n###  3. NumPy 的  ndarray 和  Python 中的原⽣数组之间有什么区别？\n\u003e ndarray 底层运算是在GPU中运行的，而原生数组是在cpu中运行。同时ndarray支持的切片操作有很多种，而Python中的原生数组只支持下标切片。而且ndarray还支持广播操作还有一个常用的函数操作。ndarray中的数据是相同类型的，但是原生数组可以存储任何类型的数据。\n\n###   4. NumPy 的多维数组创建⽅式分别是如何使⽤的。\n#### np.array() \n\u003e np.array() 在使用时值需要传入一个序列型的数据，就可以得到对应的ndarray序列\n\n#### np.ones() ， np.zeros() 和 np.empty()\n\u003e 使用方法是一样的：1.传入一个数字代表着一维数据的元素个数。2. 传入 [x1,x2,...,xn]序列数据可以指定每一维数据的数量进行创建。\n\u003e 不同之处在于ones创建的ndarray中每一个元素值都是1，zeros创建的都是0，而empty创建的元素值是没有初始化的\n\n#### np.arange()\n\u003e np.arange可以指定开始值，结束值以及步长得到一个一维的ndarray序列\n\n#### np.ones_like() 和 np.zeros_like()\n\u003e 都可以传入一个原生多维数组或者ndarray生成一个对应维度的ndarray，不过ndarray中的每一个元素都会分别变成1（前者）,0（后者）\n\n#### np.random.random()\n\u003e 可以传入size表示需要生成的ndarray数组的维度，数组中的每一个元素的数值为0到1之间的随机小数\n\n### 5. 已知有数据：    a = np.arange(25).reshape(5, 5)\n问题1：交换2⾏和3⾏，交换4列和5列\n```python\n前者是 a[[1,2]] = a[[2,1]] ，后者是 a[:,[3,4]] = a[:, [4,3]]\n```\n\n问题2：将所有奇数替换为-1 \n```python\na[c % 2 == 1] = -1\n```\n\n问题3：将对⻆线上的值替换为0\n```python\nidx = np.diag_indices(5)\na[idx]=0\n```\n\n### 6. 使⽤  Numpy 创建⼀个  5\\*5 的矩阵，每⾏数值都是从  0-4\n```python\nnp.arange(0,5) * np.ones([5,5])\n```\n\n### 7. 从⼆维数组  a_2d = np.array(\\[\\[3,3,3],\\[4,4,4],\\[5,5,5]]) 中的每⾏减去⼀维数组  b_1d = np.array(\\[1,2,3]) 中相应的值？\n```python\na_2d - b_1d\n```\n\n### 8. 创建⼀个形状为（6，4）的  NumPy 随机数组，⽤该数组创建⼀个列名为['A', 'B', 'C', 'D']的  DataFrame 。\n```python\nr = np.random.random([6,4])\ndf = pd.DataFrame(r, dolumns=['A', 'B', 'C', 'D'])\n```\n\n### 9. 已知有⼀个Python字典存储了3位同学的4⻔课成绩，课程分别是'C, Java, Python, JS',请创建DataFrame来 显示成绩单。字典如下：\n```python\nd = {\n\t'LiYongzhu' : [95, 96, 90, 88], 'LiuQingyuan' : [90, 93, 95, 90], 'GuoYimeng' : [100, 100, 100, 100]\n}\ndf = pd.DataFrame(d)\n```\n\n\n### 10. 根据  babynames 数据集，解决以下问题：\nA. 分别统计男⽣和⼥⽣的出⽣⼈数\n```python\nimport os\n\n# 设置数据集所在目录和文件名前缀\ndata_dir = \"dataset/names\"\nprefix = \"yob\"\n\n# 存储男生和女生的数量\nmale_count = 0\nfemale_count = 0\n\n# 遍历目录中所有符合条件的文件\nfor filename in os.listdir(data_dir):\n    if filename.startswith(prefix):\n        # 打开文件并逐行读取数据\n        with open(os.path.join(data_dir, filename), \"r\") as f:\n            data = f.readlines()\n            for line in data:\n                # 解析每一行数据，获取性别和出生人数\n                name, gender, count = line.strip().split(\",\")\n                if gender == \"M\":\n                    male_count += int(count)\n                elif gender == \"F\":\n                    female_count += int(count)\n\n# 输出男生和女生的数量\nprint(\"Male count:\", male_count)\nprint(\"Female count:\", female_count)\n```\n\nB. 有多少个不重复的名字\n```python\nimport os\n\n# 设置数据集所在目录和文件名前缀\ndata_dir = \"dataset/names\"\nprefix = \"yob\"\n\n# 存储所有不重复的名字\nunique_names = set()\n\n# 遍历目录中所有符合条件的文件\nfor filename in os.listdir(data_dir):\n    if filename.startswith(prefix):\n        # 打开文件并逐行读取数据\n        with open(os.path.join(data_dir, filename), \"r\") as f:\n            data = f.readlines()\n            for line in data:\n                # 解析每一行数据，获取姓名并添加到集合中\n                name, _, _ = line.strip().split(\",\")\n                unique_names.add(name)\n\n# 输出不重复的名字数量\nprint(\"Number of unique names:\", len(unique_names))\n```\n\nC. 横坐标为年份，纵坐标为数量，男⼥不同的⾛势图\n```python\nimport os\nimport matplotlib.pyplot as plt\n\n# 设置数据集所在目录和文件名前缀\ndata_dir = \"dataset/names\"\nprefix = \"yob\"\n\n# 存储男生和女生每年的出生人数\nmale_counts = {}\nfemale_counts = {}\n\n# 遍历目录中所有符合条件的文件\nfor filename in os.listdir(data_dir):\n    if filename.startswith(prefix):\n        # 解析文件名，获取年份\n        year = int(filename[3:-4])\n\n        # 打开文件并逐行读取数据\n        with open(os.path.join(data_dir, filename), \"r\") as f:\n            data = f.readlines()\n            for line in data:\n                # 解析每一行数据，获取性别和出生人数\n                name, gender, count = line.strip().split(\",\")\n                if gender == \"M\":\n                    if year not in male_counts:\n                        male_counts[year] = 0\n                    male_counts[year] += int(count)\n                else:\n                    if year not in female_counts:\n                        female_counts[year] = 0\n                    female_counts[year] += int(count)\n\n# 整理数据，生成绘图所需的 X 和 Y 坐标\nx = list(male_counts.keys())\ny_male = list(male_counts.values())\ny_female = list(female_counts.values())\n\n# 绘制男女数量走势图\nplt.plot(x, y_male, label=\"Male\")\nplt.plot(x, y_female, label=\"Female\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Count\")\nplt.title(\"Number of Births by Gender and Year\")\nplt.legend()\nplt.show()\n```\n\n### 11. 根据  drinks 数据集，解决以下问题：\nA. 哪个国家的饮酒量最⼤\n```python\nimport pandas as pd  \n  \n# 读取数据集  \ndrinks = pd.read_csv('dataset/drinks.csv')\nrank = drinks.groupby('country')['total_litres_of_pure_alcohol'].sum().sort_values(ascending=False)\n\nprint(\"饮酒量最⼤的国家是：\", rank.index[0])\n```\n\n\nB. 哪个⼤洲的饮酒量最⼤\n```python\nimport pandas as pd\n\n# 读取数据集\ndrinks = pd.read_csv(\"dataset/drinks.csv\")\n\n# 按照大陆对总消耗量进行分组求和，并排序\ncontinent_drinks = drinks.groupby('continent')['total_litres_of_pure_alcohol'].sum().sort_values(ascending=False)\n\n# 输出饮酒量最大的大洲\nprint(\"饮酒量最大的大洲是：\", continent_drinks.index[0])\n\n```\n","lastmodified":"2023-06-07T15:55:55.954775445Z","tags":["机器学习"]},"/notes/machine_leaning/begin_home_work/%E4%BD%9C%E4%B8%9A%E8%AE%AD%E7%BB%83%E8%90%A5-%E7%AC%AC%E4%B8%89%E5%91%A8":{"title":"机器学习基础作业班 第三周","content":"\n这次课我们开始说计算机视觉任务⾥的主流技术：卷积神经⽹络。\n\n\n ![](res/begin_home_work/week3/image1.png)下⾯是⼀张简笔画图⽚，我知道当你看到这样图⽚的时候能够很快的识别图像⾥⾯是⼀只⼤象。\n\n\n\n 有⼈认为，那是因为这张图⾥有两只圆圆⼤⼤的⽿朵、⼀个⻓⻓的⿐⼦、粗壮的四肢和细细的尾巴，在你的经验\n\n  \n\n ⾥把这四个特征放到⼀起时，是⼤象⽆疑了。\n\n  \n\n 机器是不是也可以先识别这些局部特征再综合判断图中的动物类别？答案是肯定的。下⾯的问题就变成如何能够表示这些特征以及如何能够识别这些特征。\n\n  \n\n 为了解决这个问题，先补充两个知识点：补充1. 图⽚在计算机中的表示⽅式。\n\n  \n\n![](res/begin_home_work/week3/image2.png)\n\n\n\n如上图所示，图⽚在机器⾥由很多像素点构成，每个像素点都是取值在\\`0-255\\`的数值。当图⽚像上图⼀样是⿊⽩图⽚(灰度图⽚)时，每个像素点由⼀个数值构成，上图⼤⼩就是\\`1\\*18\\*18\\`；如果图⽚是彩⾊的，每个像素点通常由三个数值构成，分别表示这个点上红绿蓝的强弱，这就是RGB表示⽅式(R-red\\\\G-green\\\\B-blue)。为了理解的更清楚，我们再看下图这个例⼦：\n\n  \n\n ![](res/begin_home_work/week3/image3.png)\n\n  \n 彩⾊图⽚中，每个像素点上表示红⾊强弱的数值组合成⼀个矩阵，我们管这个矩阵叫做图⽚的⼀个通道，同理，图⽚的绿⾊蓝⾊也会各⾃构成⼀个矩阵，所以这张彩⾊图⽚具有对应RGB的三个通道，上图⼤⼩就是 3\\*8\\*6 ( 通道数\\*⾼\\*宽)。注意，在⽹络中图⽚的⼤⼩通常和输⼊层的⼤⼩密切相关。\n\n\n 补充2. 图⽚中的特征构成。\n\n  \n\n![](res/begin_home_work/week3/image4.jpeg)\n\n  \n\n###  以上图为例，如果要识别⼈脸通常会先去识别⼀些更⼩的特征，⽐如上例最底图中各种\\\"边\\\"，①为什么可以这样做？这样做有什么好处？\n\u003e 可以这样子做是因为物体的一些基本特征具有很好的区分度和稳定性。而这样子做的好处是可以提高识别的准确性，将复杂的问题分而治之，降低计算量。\n\n\n所以现在问题换为在图中识别⼩特征：\n\n\"边1\"：\n ![](res/begin_home_work/week3/image5.png)\n\n\n 根据上⾯补充1中内容，\"边1\"对应矩阵信息表示为：\n\n\n ![](res/begin_home_work/week3/image6.png)\n\n ![](res/begin_home_work/week3/image7.png)现在特征的表示已经有了，如何在图⽚中来识别这些特征？这就是我们⽤的\\\"卷积\\\"过程了。现在我在图中找到两个⼦图，\\\"⼦图1\\\"以及对应矩阵信息表示为：\n\n ![](res/begin_home_work/week3/image8.png)为了表示\\\"⼦图1\\\"是否是\\\"边1\\\"要找的特征，我们进⾏这样的计算：\n\n  \n\n 这⾥的\"44500\"是计算的结果。 \"⼦图2\"以及对应矩阵信息表示为：\n\n\n![](res/begin_home_work/week3/image9.png)\n  \n\n### ②按照上⾯的计算⽅法，将\\\"⼦图2\\\"与\\\"边1\\\"进⾏计算的计算结果应该是？\n\u003e 30 x 100+100 x 100=3000+10000=13000\n  \n\n### ③将这两个计算结果对⽐，你会发现什么？这说明什么？\n\u003e 44500 \u003e 13000 说明子图1比子图2有更大的可能性为边”1”\n\n### ④参考我的视频\\\"卷积神经⽹络(CNN)卷积过程\\\"，理解卷积神经⽹络中\\\"卷积\\\"的过程具体是怎样的？\n在卷积神经网络(Convolutional Neural Network, CNN)中，卷积运算是一种重要的图像处理操作，用于提取输入数据中的特征信息。下面具体介绍卷积过程：\n\n1.  首先确定卷积核的大小和深度(即通道数)，通常是一个小矩阵，如3x3或5x5。卷积核的深度等于输入数据的深度。\n2.  将卷积核从输入数据的最左上角位置开始，按照步长(stride)进行滑动，在每个位置上执行卷积运算。\n3.  在每个位置上，将卷积核与输入数据的对应部分进行逐元素相乘，并将结果相加得到输出特征图上该位置的值。\n4.  重复2-3步骤，直到卷积核滑动完整个输入数据，生成了一个新的输出特征图。\n\n### ⑤视频中选⽤的步⻓为1，如果步⻓为2，有什么意义？\n\u003e 步长为1时，卷积核每次沿着输入数据的水平和垂直方向分别移动一个像素。这种方式可以捕获输入数据中更细小的特征，但会生成更大尺寸和更高分辨率的输出特征图。\n\u003e \n\u003e 步长为2时，卷积核每次沿着输入数据的水平和垂直方向分别移动两个像素，跳过了一部分输入数据。这种方式可以减少输出特征图的尺寸和分辨率，从而降低模型的计算复杂度，并且可以增强模型的泛化性能。\n  \n\n按照这个\\\"卷积\\\"过程，由\\\"边1\\\"按照步⻓为1在原图中进⾏计算，将计算结果拼成的矩阵叫做\\\"特征图\\\"，那么\n\\\"边1\\\"对应的特征图如下：\n\n  \n\n ![](res/begin_home_work/week3/image10.png)\n\n  \n\n### ⑥观察上例\\\"特征图\\\"，\\\"特征图\\\"和\\\"边1\\\"是什么关系？如何理解\\\"特征图\\\"？\n\u003e 特征图就是”边1”在滑动的过程中对应位置上的可能性。而”特征图”就是经过卷积核操作后提取出来的更高层次的特征表示，每一个特征图都对应着一组卷积核的输出结果。\n\n ![](res/begin_home_work/week3/image11.png)如果现在有\\\"边1\\\"、\\\"边2\\\"和\\\"边3\\\"，就会得到三个\\\"特征图\\\"(如下图)，通常为了表述清楚我们说这⾥得到的是⼀个\\\"特征图\\\"，⼀个具有三通道的\\\"特征图\\\"(只是我个⼈的表述习惯)。\n\n  \n\n 像这⾥的\\\"边1\\\"这样⽤来表示某个\\\"特征\\\"的元素就是\\\"卷积核\\\"。\n\n  \n\n###  ![](res/begin_home_work/week3/image12.png)⑦卷积核的数量和特征图\\\"通道数\\\"间有什么关系？注意：卷积核也是可以多个通道的，如下图：\n\u003e 卷积核的数量和特征图的”通道数”是保持一致的\n\n\n这⾥是⼀个卷积核，⼀个具有三通道的卷积核，最终获得了⼀个通道的特征图。\n\n\n### ⑧请⽤你的理解解释上图。\n\u003e 原图有三个通道的数据，使用了具有三个通道的卷积核，每一个通道的数据使用了对应层的卷积核进行操作分别得到对应的临时特征图，而这3个临时特征图可以合并为一个具有3通道的特征图。\n\n### ⑨如果希望对彩⾊图⽚(RGB)进⾏\\\"卷积\\\"后得到多个通道的特征图，需要怎么做？\n\u003e 可以对其应用多通道的卷积核进行操作\n\n### ⑩假如有⼀张⼤⼩为12px\\*12px⼤⼩的png图⽚，⽤2个3\\*3⼤⼩的卷积核处理，在步⻓为1的情况下，得到的特征图形状(⼤⼩)为？(注意:应当表示为\\\"通道数\\*⾼\\*宽 \\\")\n\u003e 2 x 10 x10\n\n### ⑪请总结输⼊图⽚的通道数、卷积核的通道数、卷积核的数量和输出特征图的通道数之间是怎样的关系。\n\u003e 输入图片的通道数和卷积核的通道数保持一致，卷积核的数量和输出特征图的通道数保持一致\n\n### ⑫经常会在卷积之后加上\\\"池化\\\"操作，\\\"池化\\\"的意义是什么？\n\u003e 池化操作就是一个没有可习得的参数的卷积核操作，池化的意义就是可以进行下采样\n\n### ⑬常⻅到的池化有\\\"最⼤池化\\\"和\\\"平均池化\\\"，请举例说明分别如何计算。\n“最大池化”在当前卷积核大小中取的当前区域范围内的最大值，而 “平均池化” 则是求得当前范围内的平均值。\n\n池化层的步长(stride)决定了池化窗口在输入数据上移动的距离。最大池化(Max Pooling)和平均池化(Average Pooling)的滑动步长都可以是任意正整数，但通常选择的步长为池化窗口的大小的一半，即 $stride = \\frac{pool\\_size}{2}$。\n\n这样的步长选择有以下原因：\n\n1.  保证没有重叠区域：使用步长为池化窗口大小的一半，可以确保每个池化窗口之间不存在重叠区域，避免了类似于卷积操作中的局部重叠现象，从而减少输出特征图的尺寸和计算量。\n2.  避免信息丢失：使用过大的步长会使得池化操作忽略一些重要的特征信息，从而影响模型的性能和鲁棒性。使用步长为池化窗口大小的一半，可以保证池化操作不会过于粗略，同时也不会过度压缩特征信息。\n  \n\n### ⑭在卷积和池化的时候经常先给图⽚进⾏\\\"填充\\\"，常⻅到的填充⽅式有哪些？填充的意义是什么？\n在卷积神经网络(Convolutional Neural Network, CNN)中，卷积和池化操作通常都需要将输入数据进行填充(padding)，以控制输出特征图的尺寸、边缘效应等。下面是一些常见的填充方式及其意义：\n\n1.  零填充(Zero Padding)：零填充是最常用的一种填充方式，在输入数据的周围添加一圈值为0的像素，使得原始输入数据和填充后的数据尺寸相同。零填充的主要作用是保持输入数据的空间大小不变，并且可以减少边缘效应的影响，从而提高模型的准确性和鲁棒性。\n2.  边界填充(Border Padding)：边界填充是在输入数据的四周添加与边缘颜色相同的像素，一般情况下会使用黑色或白色像素进行填充。这种填充方式可以避免由于零填充引入的额外噪声和干扰，但同时也会导致输出特征图的尺寸比输入数据更小。\n3.  反射填充(Reflect Padding)：反射填充是在输入数据的边缘上按照对称镜像的方式添加像素，使得输入数据和输出数据具有相同的尺寸和形状。反射填充可以有效地避免边缘效应，并且可以保留输入数据的平滑性和连续性，从而提高模型的稳定性和可靠性。\n4.  复制填充(Copy Padding)：复制填充是将输入数据的边缘像素复制若干次，使得输入数据和填充后的数据具有相同的尺寸。这种填充方式可以避免边缘效应，但同时也可能会引入额外的重复信息和噪声，从而影响模型的性能和鲁棒性。\n\n填充的主要作用是控制输出特征图的尺寸，比如说可以保持处理后的特征图和原图的尺寸一致。\n  \n\n### ⑮输⼊图⽚⾼宽、输出特征图⾼宽、卷积核⾼宽、步⻓和填充⼤⼩之间有怎样的关系？\nout_height = (input_height + 2 x padding_height - kernel_height) / stride + 1\nout_width = (input_width + 2 x padding_width - kernel_width) / stride + 1\n\n现在，你已经对卷积神经⽹络⾥⾯的基本概念有所了解，可以尝试思考回答以下较为深⼊的问题。\n\n### ⑯如何理解卷积过程中的\\\"平移不变性\\\"？\n\u003e “平移不变性”指在进行卷积核操作的时候是进行滑动计算的，所以就算特征发生了平移的情况也可以找到对应的特征。\n\n### ⑰为什么说卷积神经⽹络是\\\"稀疏连接\\\"的？\n\u003e 因为卷积核进行计算的时候是进行滑动且一一对应的，所以对应到神经网络节点的连接上去就是稀疏的\n\n### ⑱如何理解卷积过程中的\\\"参数共享\\\"？\n\u003e 卷积神经网络在进行连接的时候是稀疏连接的，而卷积核操作需要使用固定的卷积核参数进行计算，所以卷积核上的位置对应的连接权重是相同的，这个就是”参数共享”\n\n  \n最最最最最最最最最最最最最重要的问题来了：\n  \n\n![](res/begin_home_work/week3/image13.png)\n\n  \n\n别告诉我你没⻅过，这是LeNet⽹络结构图，是卷积神经⽹络的⿐祖。\n\n### ⑲ 如果卷积核是代表的图中局部小特征，那么如何找到这里面的填充值？\n\u003e 这部分的填充值是需要在投喂数据的时候进行训练的，是网络自己习得的。\n\n### ⑳ 在今天学习的内容里面，哪些是在卷积神经网络里的超参数？\n\u003e 超参数是网络自身无法习得的参数，比如说网络的连接结构，卷积核的大小，卷积核操作时的步长以及填充方式和大小。\n\n### ㉑请你看图解释这个⽹络的设计(别着急查资料，先⾃⼰读图试试，都是我们学习过的内容)。\n\u003e 先是32x32的图片输入，经过一次卷积核操作（6@5x5）得到6@28x28的特征图，再进行一次池化操作（下采样，卷积核为 6@2x2）得到6@14x14特征图，在进行一次卷积核操作（16@5x5）得到16@10x10的特征图，再进行一次池化操作（下采样，卷积核为16@6x6）得到16@5x5的特征图，然后展开通过一层全连接达到C5，一次全连接达到F6，最后使用softmax分类器进行多分类，输出概率最大的类别\n  \n\n### 读懂⽹络结构图之后，就⽤代码去实现吧，像搭积⽊⼀样，把对应的功能封装成函数后拼装在⼀起，这⾥⾯⽤到了全连接层、卷积层和池化层和激活函数。我们之前定义过激活函数，㉒你完全可以⾃⼰实现⼀个卷积函数和⼀个池化函数（选做）。当然，这只是为了练习和学习，通常使⽤时会直接调⽤框架中给我们提供的接⼝。\n \n### ㉓PyTorch中的卷积层和池化层应当如何调⽤函数实现？如何设置窗⼝⼤⼩、填充和步⻓？\n\n在PyTorch中，可以使用torch.nn模块中的Conv2d和MaxPool2d类来实现卷积层和池化层，具体的调用方法如下：\n\n1.  Conv2d函数的调用方法：\n\n```python\nimport torch.nn as nn\n\n# 定义一个卷积层，输入特征图通道数为in_channels，输出特征图通道数为out_channels，卷积核大小为kernel_size，步长为stride，填充方式为padding\nconv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n\n```\n\n2. MaxPool2d函数的调用方法：\n\n```python\nimport torch.nn as nn\n\n# 定义一个池化层，窗口大小为kernel_size，步长为stride，填充方式为padding\npool_layer = nn.MaxPool2d(kernel_size, stride=stride, padding=padding)\n\n```\n\n其中，in_channels和out_channels分别表示输入和输出特征图的通道数；kernel_size表示卷积核或池化窗口的大小；stride表示卷积或池化操作时在输入数据上移动的步长；padding表示在输入数据周围添加的额外像素。\n\n例如，可以定义一个卷积层，它的输入特征图通道数为3，输出特征图通道数为16，卷积核大小为3x3，步长为1，填充方式为1，代码如下：\n\n```python\nimport torch.nn as nn\nconv_layer = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n```\n\n也可以定义一个最大池化层，它的窗口大小为2x2，步长为2，填充方式为0，代码如下：\n\n```python\nimport torch.nn as nn\npool_layer = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n```\n\n通过这些方法可以轻松地实现卷积和池化操作，并且可以根据需要设置窗口大小、填充和步长等超参数。\n\n### ㉔组件都会⽤了，你可以尝试使⽤PyTorch将这个LeNet⽹络的定义实现。\n\n```python\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\n\n# 定义LeNet模型\nclass LeNet(nn.Module):\n    def __init__(self):\n        super(LeNet, self).__init__()\n        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n        self.sigm1 = nn.Sigmoid()\n        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n        self.sigm2 = nn.Sigmoid()\n        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.sigm3 = nn.Sigmoid()\n        self.fc2 = nn.Linear(120, 84)\n        self.sigm4 = nn.Sigmoid()\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool1(self.sigm1(self.conv1(x)))\n        x = self.pool2(self.sigm2(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = self.sigm3(self.fc1(x))\n        x = self.sigm4(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n# 加载MNIST数据集\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\ntrain_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntest_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False)\n\n# 定义LeNet模型、损失函数和优化器\nlenet = LeNet()\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(lenet.parameters(), lr=0.001)\n\n# 训练LeNet模型\nnum_epochs = 10\ntotal_step = len(train_loader)\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # 前向传播\n        outputs = lenet(images)\n        loss = criterion(outputs, labels)\n\n        # 反向传播及优化\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if (i+1) % 100 == 0:\n            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n\n# 测试LeNet模型在测试集上的准确率\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in test_loader:\n        outputs = lenet(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print('Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n\n\n```\n\n在上述代码中，我们首先定义了一个`LeNet`类，用于实现LeNet模型。该模型包含两个卷积层和三个全连接层，其中第一个卷积层的输入通道数为1，输出通道数为6，卷积核大小为5x5，步长为1；第二个卷积层的输入通道数为6，输出通道数为16，卷积核大小为5x5，步长为1。两个卷积层后接两个最大池化层，窗口大小为2x2，步长为2。其余部分为三个全连接层，分别包含120、84和10个神经元。\n\n然后，我们定义损失函数为交叉熵损失函数，优化器为随机梯度下降(SGD)算法，并开始训练模型。在训练过程中，我们使用Mini-Batch SGD进行迭代优化，并计算每次迭代的损失值，以便进行模型效果评估。\n\n最后，我们在测试集上对模型进行测试，并输出预测准确率。\n\n###  最后补充⼀个⾮常重要的知识点：\\\"1 \\* 1卷积核\\\"，没错，就是宽和⾼的值都为1的卷积核。㉕\\\"1 \\* 1卷积核\\\"的使⽤能解决哪些问题？\n\n\n\n这周内容主要是卷积神经⽹络基础知识，在解决计算机视觉相关问题⼗分重要。\n\n","lastmodified":"2023-06-07T15:55:55.954775445Z","tags":["机器学习"]},"/notes/machine_leaning/begin_home_work/%E4%BD%9C%E4%B8%9A%E8%AE%AD%E7%BB%83%E8%90%A5-%E7%AC%AC%E4%BA%8C%E5%91%A8":{"title":"机器学习基础作业班 第二周","content":"\n## 一、理论部分\n\n\n有了机器学习的基础知识, 这一周咱们进入人工神经网络的学习。\n\n\n### (1)人工神经网络? 不是深度学习么? 二者是什么关系?\n人工神经网络是一种模仿人脑神经网络的数学模型，由多个节点（人工神经元）互相连接而成，用来对数据之间的复杂关系进行建模。每个节点代表一种特定函数，来自其他节点的信息经过其相应的权重计算后被传递给下一个节点，以此模拟生物神经网络的工作机制。\n深度学习是机器学习的一个分支领域，强调从连续的层（layer）中进行学习，这些层对应于越来越有意义的表示。深度学习是从数据中学习表示的一种新方法，现代深度学习通常包含数十个甚至上百个连续的表示层，这些表示层全都是从训练数据中自动学习的。与其他机器学习方法相比，深度学习的重点往往是学习多个层次的数据表示，从而学会更深层次的理解，因此有时也被称为分层表示学习或层级表示学习。\n所以深度学习是通过构建多层神经网络来实现对数据的高层次抽象和复杂模式的提取，从而实现对数据的分类，识别和预测。\n\n随着人们对自然界的认识, 仿生学也逐渐发展起来, 我们要学习的人工神经网络就是这其中一个产物。1943年, McCulloch与Pitts从对人脑神经元的认识抽象出下图所示的简单模型, 这就是著名的“M-P神经元模型”。在这个模型中, 神经元接收到来自$n$ 个其它神经元传递过来的输入信号, 这些输入信号通过带权重的连接对输入信号进行 线性变换, 神经元接收到的总输入与神经元的阈值进行比较, 然后通过激活函数处理以产生神经元的输出。\n\n\n![](res/media/2023_05_15_3adf0d30128fda106703g-1.jpg)\n\n\n注意: 上述模型中$x_{1}, x_{2}, \\ldots, x_{n}$ 表示外部刺激, 也可记为$\\mathbf{x}$, 也就是模型输入,$w_{1}, w_{2}, \\ldots, w_{n}$ 为突触权值, 也就是模型自由参数, 也可记为$\\mathbf{w}, b$ 表示阈值。$y$ 代表计算后的输出。函数$f$ 我们下面细说。\n\n根据上图请尝试回答:\n\n### (2)为什么说M-P神经元将所有信号的输入看做是同时发生的。\n\nM-P神经元是一种最早的神经元模型，它是由 Marvin Minsky 和 Seymour Papert 在 1969 年提出的。M-P神经元（也称为阶跃函数神经元）的输入和输出可以用以下公式表示：\n\n$$\ny=\\begin{cases}\n1, \u0026\\text{if } \\sum_{i=1}^n w_i x_i + b \\geq 0\\\\\n-1, \u0026\\text{otherwise}\n\\end{cases}\n$$\n\n其中，$x_i$ 是输入向量 $\\mathbf{x}$ 的第 $i$ 个分量，$w_i$ 是权重向量 $\\mathbf{w}$ 的第 $i$ 个分量，$b$ 是偏置项。\n\n在 M-P 神经元模型中，所有输入信号被同时处理，因此被视为是同时发生的。这意味着，在一个时间点上，M-P 神经元模型可以接收多个输入信号，并且将它们组合成一个输出信号。如果 $\\sum_{i=1}^n w_i x_i + b \\geq 0$，则神经元会输出1，否则输出-1。这个判断过程类似于人类大脑中的神经元，当足够的刺激通过突触传递到一个神经元时，该神经元就会被激活，并生成一个输出信号。\n\n需要注意的是，M-P神经元模型虽然非常简单，但它只能处理线性可分问题，且无法学习非线性决策边界。由于其局限性，M-P神经元很快被更加强大的模型所取代，如感知机、多层感知机等。\n\n\n### (3)输入的信号在传导的过程中分别做了怎样的计算。\n在M-P神经元模型中，每个神经元接收来自 $n$ 个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接对输入信号进行线性变换，然后将所有的输入信号加权求和，并与神经元的阈值进行比较。\n\n具体而言，假设第 $i$ 个神经元接收到来自 $n$ 个其他神经元传递过来的输入信号，分别为 $x_1, x_2, ..., x_n$。这些输入信号通过带权重的连接进行线性变换，得到：\n\n$$z = \\sum_{j=1}^{n} w_j x_j $$\n\n其中，$w_j$ 表示第 $j$ 个神经元到第 $i$ 个神经元的连接权重。\n\n然后，我们将所有的输入信号加权求和，得到该神经元的总输入：\n\n$$y = z + b $$\n\n其中，$b$ 是神经元的阈值（或者说是偏置），用于控制神经元的激活程度。\n\n最后，我们将总输入 $y$ 传递给激活函数 $f(y)$ 进行处理，得到神经元的输出：\n\n$$o = f(y) $$\n\n综上所述，M-P神经元输入的信号在传导的过程中，分别经历了线性变换、加权求和、阈值比较和激活函数处理等计算步骤。这些计算步骤产生了神经元的总输入和输出，用于描述神经元对输入信号的响应和处理方式。\n\n假设人脑神经元有兴奋和抑制两种状态, 为了模拟类似效果, 现在希望输入信息在经过神经元之后会得到两种输出, “1”和“0”, 分别代表神经元兴奋和神经元抑制。而这里的函数$f$ 的作用就是来完成这件事, 这就是我们常说的 激活函数, 如下图左图所示。但现在这个激活函数具有不连续、不光滑等不太好的性质, 因此实际中常常使用 Sigmoid函数作为激活函数, 其中常用的一种Sigmoid激活为Logistic激活函数如下图右图所示。\n\n\n![](/res/begin_home_work/image2.jpeg)\n\n\n### (4)常见到的激活函数还有Relu与leakyRelu，tanh等，请写出各自函数定义。\n\n1. ReLU函数（Rectified Linear Unit）\n- 函数定义：$f(x) = max(0, x)$\n- 图像：\n![](res/ReLU%20激活函数.png)\n\n2. Leaky ReLU函数\n- 函数定义： $f(x) = \\begin{cases} \\alpha x \u0026 x\u003c0 \\\\ x \u0026 x\\geq 0 \\end{cases}, \\alpha \\in [0, 1]$\n- 图像：\n\n![](res/Leaky%20ReLU.png)\n\n3. Tanh函数\n- 函数定义：$f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n- 图像：\n\n![](res/双曲正切激活函数.png)   \n\n![](res/tanh%20函数和%20sigmoid%20函数的曲线.png)\n\n\n### (5)在Python环境下使用Numpy编写Sgn、Logistic激活函数。\n```python\nimport numpy as np\n\n# Sgn激活函数\ndef sgn(x):\n    return np.sign(x)\n\n# Logistic激活函数\ndef logistic(x):\n    return 1 / (1 + np.exp(-x))\n\n# 测试Sgn激活函数\nx = np.array([-5, -1, 0, 1, 5])\nprint(sgn(x))  # 输出：[-1. -1.  0.  1.  1.]\n\n# 测试Logistic激活函数\nx = np.array([-5, -1, 0, 1, 5])\nprint(logistic(x))  # 输出：[0.00669285 0.26894142 0.5        0.73105858 0.99330715]\n\n```\n\n说到这里, 不得不提一下大名鼎鼎的Rosenblatt感知器。在McCulloch and Pitts (1943) 引入神经网络的概念, 并设计了M-P模型后, Rosenblatt (1958) 提出的感知器则作为神经网络中监督学习的第一个模型。它在M-P模 型的基础上添加了训练方法, 并且Rosenblatt证明了当用来训练感知器的模式 (向量) 取自两个线性可分的类 时, 感知器算法是收敛的。\n\n### 算法的收敛性证明被称为感知器收敛定理 \n\n感知器算法的收敛定理是感知器算法的重要理论基础。它表明，当训练集是线性可分的时候，感知器算法可以通过有限次迭代得到一个分类器，使得该分类器对训练集中的所有样本都进行正确分类。\n\n具体来说，感知器算法的收敛定理可以描述如下：\n\n假设训练数据集 $T=\\{(\\mathbf{x}_1,y_1),(\\mathbf{x}_2,y_2),\\cdots,(\\mathbf{x}_N,y_N)\\}$ 是线性可分的，即存在某个超平面 $w \\cdot x+b=0$ 能够将正负样本完全分开，其中 $\\mathbf{x}_i$ 表示第 $i$ 个样本的特征向量，$y_i\\in\\{-1,+1\\}$ 表示第 $i$ 个样本的真实标签。\n\n定义误分类点集合 $M$ 表示当前分类器对于训练集中的错误样本的集合，即：\n\n$$\nM=\\{\\mathbf{x}_i | y_i(w\\cdot\\mathbf{x}_i+b) \\leq 0\\}\n$$\n\n则感知器算法在经过有限次迭代后，能够找到一组分类器参数 $w,b$，使得 $M$ 为空集，即对于任意一个样本 $(\\mathbf{x}_i,y_i)$ 都有 $y_i(w\\cdot\\mathbf{x}_i+b)\u003e0$。此时，我们就得到了一个对训练数据集 $T$ 进行完美分类的超平面 $w\\cdot x+b=0$。\n\n感知器收敛定理的证明可以分为以下几个步骤：\n\n1. 假设训练数据集 $T=\\{(\\mathbf{x}_1,y_1),(\\mathbf{x}_2,y_2),\\cdots,(\\mathbf{x}_N,y_N)\\}$ 是线性可分的，即存在某个超平面 $w \\cdot x+b=0$ 能够将正负样本完全分开。\n\n2. 定义误分类点集合 $M$ 表示当前分类器对于训练集中的错误样本的集合，即：\n\n$$M=\\{\\mathbf{x}_i | y_i(w\\cdot\\mathbf{x}_i+b) \\leq 0\\}$$\n\n3. 假设该超平面的法向量 $\\mathbf{w}$ 的范数为 $\\|\\mathbf{w}\\|_2=1$，并且假设所有误分类样本 $\\mathbf{x}_i$ 到超平面的距离为 $\\gamma$。由于数据集是线性可分的，因此 $\\gamma\u003e0$。\n\n4. 在进行第 $t$ 次迭代时，假设我们选取了一个误分类样本 $(\\mathbf{x}_i,y_i)$ 进行更新，得到新的权重向量 $\\mathbf{w}_{t+1}$ 和偏置项 $b_{t+1}$。那么，根据感知器算法的更新规则，我们有：\n\n$$\\begin{cases}\\mathbf{w}_{t+1}=\\mathbf{w}_t + \\eta y_i \\mathbf{x}_i \\\\ b_{t+1}=b_t + \\eta y_i\\end{cases}$$\n\n5. 我们现在来计算分类器在第 $t+1$ 轮迭代后与真实决策边界之间的距离。假设超平面的参数为 $(\\mathbf{w}_t,b_t)$，误分类样本 $(\\mathbf{x}_i,y_i)$ 到超平面的距离为 $\\gamma$，则新的超平面的参数为 $(\\mathbf{w}_{t+1},b_{t+1})$。根据向量投影的定义，我们可以将误分类样本的特征向量 $\\mathbf{x}_i$ 分解成两个部分：一个垂直于超平面的向量 $\\mathbf{v}$，和一个在超平面上的向量 $\\mathbf{u}$：\n\n$$\\mathbf{x}_i=\\mathbf{v}+\\mathbf{u}$$\n\n其中，$\\mathbf{v}$ 的长度等于 $\\gamma$，$\\mathbf{u}$ 的长度等于 $\\sqrt{\\|\\mathbf{x}_i\\|^2-\\gamma^2}$。显然，向量 $\\mathbf{v}$ 垂直于超平面，而向量 $\\mathbf{u}$ 在超平面上。我们可以将 $\\mathbf{w}_t$ 和 $\\mathbf{w}_{t+1}$ 投影到向量 $\\mathbf{v}$ 上，得到它们在 $\\mathbf{v}$ 方向上的变化量：\n\n$$\n\\begin{aligned}\n\\Delta \\mathbf{w} \u0026= \\mathbf{w}_{t+1}-\\mathbf{w}_t \\\\\n\u0026= \\eta y_i \\mathbf{x}_i \\\\\n\u0026= \\eta y_i (\\mathbf{v}+\\mathbf{u}) \\\\\n\u0026= \\eta y_i \\mathbf{v}\n\\end{aligned}\n$$\n\n6. 我们现在来计算新的分类器与真实决策边界之间的距离。由于超平面的法向量 $\\mathbf{w}$ 的长度等于1，因此分类器与真实决策边界之间的距离可以表示为：\n\n$$\n\\begin{aligned}\nd \u0026= \\frac{\\mathbf{w}_{t+1}\\cdot\\mathbf{w}}{\\|\\mathbf{w}\\|} \\\\\n\u0026= \\frac{(\\mathbf{w}_t+\\Delta \\mathbf{w})\\cdot\\mathbf{w}}{\\|\\mathbf{w}\\|} \\\\\n\u0026= \\frac{\\mathbf{w}_t\\cdot\\mathbf{w}+\\Delta \\mathbf{w}\\cdot\\mathbf{w}}{\\|\\mathbf{w}\\|} \\\\\n\u0026= d_t + \\eta y_i \\|\\mathbf{v}\\|\n\\end{aligned}\n$$\n\n其中，$d_t$ 表示第 $t$ 轮迭代后分类器与真实决策边界之间的距离。\n\n7. 我们需要证明的是，在假设 $\\mathbf{w}$ 的范数为 $1$ 的情况下，感知器算法能够收敛。因此，我们需要证明分类器与真实决策边界之间的距离随着迭代次数的增加而不断减小，即：\n\n$$d_{t+1} \\leq d_t - \\gamma^2 \\eta$$\n\n其中，$\\gamma$ 表示所有误分类样本到真实决策边界的距离的最小值。\n\n8. 因为 $(\\mathbf{x}_i,y_i)$ 是一个误分类样本，所以有：\n\n$$y_i(\\mathbf{w}_t\\cdot\\mathbf{x}_i+b_t) \\leq 0$$\n\n移项并乘以 $\\gamma$，得到：\n\n$$y_i(\\mathbf{w}_t\\cdot\\mathbf{v}+\\gamma b_t) \\leq 0$$\n\n因为 $\\mathbf{v}$ 垂直于超平面，所以 $\\mathbf{w}_t\\cdot\\mathbf{v}=-b_t$。因此，上式可以进一步化简为：\n\n$$y_i\\gamma(b_t-y_i\\mathbf{w}_t\\cdot\\mathbf{v}) \\geq \\gamma^2$$\n\n由于我们选择了一个误分类样本进行更新，因此有 $y_i\\mathbf{w}_t\\cdot\\mathbf{x}_i+y_i b_t \\leq 0$。将其代入上式中，得到：\n\n$$y_i\\gamma(b_t-y_i\\mathbf{w}_t\\cdot\\mathbf{v}) \\geq y_i\\gamma(-\\mathbf{w}_t\\cdot\\mathbf{x}_i) \\geq \\gamma^2$$\n\n因此，\n\n$$b_t-y_i\\mathbf{w}_t\\cdot\\mathbf{v} \\geq -\\eta\\gamma$$\n\n将上式代入第七步中的公式，得到：\n\n$$\n\\begin{aligned}\nd_{t+1} \u0026= d_t + \\eta y_i \\|\\mathbf{v}\\| \\\\\n\u0026= d_t - y_i(b_t-y_i\\mathbf{w}_t\\cdot\\mathbf{v}) \\\\\n\u0026\\leq d_t + \\eta\\gamma^2 \\\\\n\u0026= d_t - \\gamma^2\\eta\n\\end{aligned}\n$$\n\n这证明了感知器算法的收敛性。由此可知，当训练数据集线性可分时，感知器算法将会在有限次迭代内找到一个能够正确分类所有样本的超平面。\n\n### (6)感知器的收敛算法是怎样的? 请用自己语言描述。(选做)\n\n在感知机算法中，我们需要将每个样本投影到一个超平面上，判断它们是否被正确分类。为了实现这一目标，我们需要学习一个权重向量 $\\mathbf{w}=(w_1,w_2,\\cdots,w_n)$，使得对于所有的样本 $\\mathbf{x}_i=(x_{i1},x_{i2},\\cdots,x_{in})$，其预测值 $\\hat{y}_i=\\text{sign}(\\sum_{j=1}^n w_j x_{ij}+b)$ 能够尽可能地接近真实标签 $y_i$。\n\n在每次迭代中，我们会选取一个误分类点进行更新。具体来说，当一个样本被错误地分类时，即 $y_i\\hat{y}_i\\leq 0$，我们需要将模型参数进行调整，使得该样本能够被正确分类。其中，$y_i$ 是真实标签，$\\hat{y}_i$ 是当前模型对于该样本的预测值。为了实现这一目标，我们需要按照如下公式来更新权重和偏置：\n\n$$\n\\begin{aligned}\nw_j \u0026\\leftarrow w_j + \\eta y_i x_{ij}\\\\\nb \u0026\\leftarrow b + \\eta y_i\n\\end{aligned}\n$$\n\n其中，$w_j$ 表示第 $j$ 个权重的值，$x_{ij}$ 表示第 $i$ 个样本的第 $j$ 个特征值，$\\eta$ 是学习率，表示每次更新的步长大小。$y_i x_{ij}$ 则表示要更新的权重和样本特征值之积，可以看作是对于这个特定特征的梯度。通过反复更新权重和偏置，我们不断逼近最优解，最终得到一个能够正确分类所有样本的超平面。\n\n具体的过程如下：\n\n1. 选取初值 $w_0, b_0$：这是感知机算法的初始状态。我们需要确定权重和偏置的初始值。一般情况下，可以随机初始化权重和偏置，或者将它们设置为0。\n\n2. 在训练集中选取数据 $(x_i, y_i)$：从训练集中随机选择一个样本，并将它作为当前迭代的输入。其中，$x_i=(x_{i1},x_{i2},\\cdots,x_{in})$ 是输入特征向量，$y_i\\in\\{-1,+1\\}$ 是真实标签。\n\n3. 如果 $y_i(w \\cdot x_i+b) \\leq 0$，则将 $w,b$ 更新为：\n   - 更新权重：$w_j \\leftarrow w_j + \\eta y_i x_{ij}$ 对于所有特征。这个公式表示更新第 $j$ 个权重 $w_j$ 的值。$x_{ij}$ 表示第 $i$ 个样本的第 $j$ 个特征值。\n   - 更新偏置：$b \\leftarrow b + \\eta y_i$。这个公式表示更新偏置 $b$ 的值。$\\eta$ 是学习率，表示每次更新的步长大小。\n\n4. 转至步骤 2，直至训练集中没有误分类点：不断重复步骤 2 和 3，直到训练集中所有样本都被正确分类为止。如果训练集线性可分，这个算法会收敛；否则，它会在有限次迭代后停止。\n\n5. 输出感知机模型 $f(x)=\\operatorname{sign}(w \\cdot x+b)$：当训练完成后，我们就可以使用学习到的权重和偏置来进行新样本的预测。具体来说，对于一个输入样本 $\\mathbf{x}=(x_1,x_2,\\cdots,x_n)$，我们可以计算出其预测结果 $f(\\mathbf{x})=\\text{sign}(\\mathbf{w}\\cdot\\mathbf{x}+b)$，其中 $\\text{sign}$ 是符号函数，表示当输入大于0时输出1，否则输出-1。\n\n\n### (7)代码实现Rosenblatt感知器与收敛算法, 并使用均方误差 (MSE) 衡量模型性能。使用线性可分的双月数据集 进行训练与测试。 (选做) 参考效果如图:\n\n![](res/media/2023_05_15_3adf0d30128fda106703g-2.jpg)\n\n提示：scikit-learn 中有直接生成双月数据集的函数方法。\n\n```python\nimport numpy as np\nfrom sklearn.datasets import make_moons\nimport matplotlib.pyplot as plt\n\n# 生成包含1000个样本的双月数据集\nX, y = make_moons(n_samples=1000, noise=0.1, random_state=123)\nX[y == 0, 1] += 1\n\n# 构建 Rosenblatt感知器类\nclass Rosenblatt():\n\n    def __init__(self, max_iter=10000, eta=0.1):\n        self.max_iter = max_iter  # 最大迭代次数\n        self.eta = eta  # 学习率\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        self.w = np.zeros(n_features + 1)  # 初始化权重向量\n        X = np.insert(X, 0, 1, axis=1)  # 将偏置项 b 合并到权重向量 w 中\n        for _ in range(self.max_iter):\n            error_count = 0\n            for xi, yi in zip(X, y):\n                update = self.eta * (yi - np.sign(np.dot(xi, self.w)))\n                self.w += update * xi\n                if update != 0:\n                    error_count += 1\n            if error_count == 0:\n                break\n\n    def predict(self, X):\n        X = np.insert(X, 0, 1, axis=1)  # 添加偏置项\n        return np.sign(np.dot(X, self.w))\n\n# 训练 Rosenblatt感知器\nmodel = Rosenblatt()\nmodel.fit(X, y)\n\n# 绘制分类平面（一条线）\nplot_x = np.linspace(-2, 3, 1000)\nplot_y = - (model.w[0] + model.w[1]*plot_x) / model.w[2]\n\nplt.plot(plot_x, plot_y, color='red', linestyle='-')\nplt.scatter(X[:, 0], X[:, 1], c=y)\nplt.show()\n\n```\n\n\n以上都只是一个神经元, 还不能称之为网络。（但是复杂的神经网络也就是由这种结构的众多神经元相互连接构成。）更重要的是, 一个神经元无法完成像抑或问题这样线性不可分的复杂问题。如下图：\n\n\n![](/res/begin_home_work/image4.jpeg)\n\n在实际中遇到的大多数模式分类问题都是线性不可分问题, 这就使得感知器的用途变得极为狭窄。要解决线性不 可分问题，需要使用多层功能神经元（多于0个隐藏层）。简单的来看就是把许多个神经元排成一排，可以组成一 个单层前馈神经网络, 这个网络的每一个神经元接收相同的数据作为输入, 但每个神经元具有不同的连接权重、 阈值, 这样的一层网络可以输出与神经元一样多数量的值。单层前馈神经网络的输出也可以作为另一个单层神经 网络的输入, 以此相连在一起就组成了规模庞大的神经网络。\n\n![](res/media/2023_05_15_3adf0d30128fda106703g-3.jpg)\n\n\n\n### (8)那么，什么是输入层、隐藏层和输出层?\n\n\u003e 输入层是数据输入的那一层，而输出层是数据输出的那一层，除此之外的都是隐藏层\n\n注意: 每个层的人工神经元与前一层神经元全连接, 即为全连接神经网络（也存在其它连接形式, 如卷积神经网络的局部连接等）。\n\n\n上述异或问题只需要三层全连接的神经网络（包含一个隐藏层的神经网络）即可解决。其中隐藏层与输出层均包 含有非线性的激活函数。如下图所示, 左图为3层全连接神经网络的结构图, 图中标明了连接权重与阈值; 右图表 示对应的决策边界与决策域。\n\n  ![](/res/begin_home_work/image6.jpeg)\n\n\n这里每一个神经元模型为M-P模型，所以其激活函数为：\n\n\n$\\operatorname{sgn}(x)= \\begin{cases}1, \u0026 x \\geq 0 \\\\ 0, \u0026 x\u003c0\\end{cases}$\n\n\n### (9)请将上面右图(b)分类区域中4个点分别按左图(a)网络参数设置进行计算。\n\n1. (0, 1)\n第二层 左节点：f(0 x 1 + 1 x (-1))  = f(0 - 1) = f(-1) = 0\n第二层 右节点：f(0 x (-1) + 1 x 1) = f(0 + 1) = f(1) = 1\n第三层 输出：f(0 x 1 + 1 x 1) = f(1) = 1\n\n2. (1, 1)\n第二层 左节点：f(1 x 1 + 1 x (-1)) = f(1 - 1) = f(0) = 0\n第二层 右节点：f(1 x (-1) + 1 x 1) = f(0) = 0\n第三层 输出：f(1 x 1 + 1 x 1) = f(2) = 0\n\n3. (0, 0)\n第二层 左节点：f(0 x 1 + 0 x (-1)) = f(0) = 0\n第二层 右节点：f(0 x (-1) + 0 x 1) = f(0) = 0\n第三层 输出：f(0 x 1 + 0 x 1) = f(0) = 0\n\n4. (1, 0)\n第二层 左节点：f(1 x 1 + 0 x (-1)) = f(1) = 1\n第二层 右节点：f(1 x (-1) + 0 x (1)) = f(-1) = 0\n第三层 输出：f(1 x 1 + 0 x 1) = f(1) = 1\n\n### (10)现在你应当清楚的体会到, 模型的节点（神经元）数量越多, 参数越大, 模型的拟合能力也就越强了。那么请你 思考，激活函数的作用到底是什么? 如果没有激活函数网络会怎样?\n\u003e 激活函数可以实现非线性变换，这样子能让网络表达的能力更强。如果没有激活函数，那么再多的节点拼接而成的网络最终也只是进行了一次线性变换，也就是矩阵乘法而已。\n\n在你完成第(9)问的过程中, 就像是拿一个已经训练好的模型去进行预测。这里的参数$(\\mathbf{w}, \\mathbf{b})$ 是已知的, 你只是 将数据输入在这个网络结构里从前往后的进行了计算, 最终获得预测结果。当然你现在计算的结果和右图情况是 相符的, 因为现在你有一个能够很好拟合当前问题的模型。得到这个能够拟合你要解决的问题的模型, 就是我们 进行模型训练的目的。当模型结构已定时, 能不能拟合要解决的问题, 就取决于模型里面参数的取值。所以说人 话就是，训练模型的目的，就是去寻找当前结构下的最优参数。 上次作业讲到训练模型前首先确定三件事：1. 选择一个具有末知参数的模型；2. 定义损失函数；3. 确定优化方 法。\n\n  \n在这基础上，就可以通过“前向传播(求得损失)--\u003e反向传播(更新参数)\"的循环进行来训练模型了。\n  \n\n求得损失需要通过调用损失函数来进行。损失函数的目的就是看看现在模型的预测情况和真实值的差距有多大\n  \n\n### (有监督情况下）。11)对于回归问题、二分类问题、多分类问题通常使用的损失函数有哪些?\n对于回归问题、二分类问题、多分类问题，通常使用的损失函数有：\n\n#### 回归问题\n\n1. 均方误差（Mean Squared Error, MSE）：是最常用的回归问题损失函数，计算预测值与真实值之间的平方差，并求其平均值。\n\n2. 平均绝对误差（Mean Absolute Error, MAE）：计算预测值与真实值之间的绝对差，并求其平均值。\n\n3. Huber Loss：是一种可以平衡 MSE 和 MAE 的损失函数。它在误差较小的时候采用 MSE，误差较大的时候采用 MAE。\n\n#### 二分类问题\n\n1. 交叉熵损失（Cross Entropy Loss）：这是最常用的损失函数，计算模型输出的概率与真实标签的对数概率的负值。\n\n2. Hinge Loss：适用于支持向量机（Support Vector Machine, SVM）和感知机（Perceptron）等算法，通过最小化错误分类的样本到分类超平面的距离来学习分类器。\n\n3. Focal Loss：解决类别不平衡问题的一个损失函数，弱化易分类的样本的权重，增强难分类的样本的权重，减少易分类的样本对损失函数的贡献。\n\n#### 多分类问题\n\n1. 交叉熵损失（Cross Entropy Loss）：同样也是最常用的损失函数，但针对多分类问题，计算模型输出的概率与真实标签的对数概率的负值。\n\n2. 多项式分布损失（Multinomial Distribution Loss）：同样针对多分类问题，基于多项式分布建立损失函数，并通过最大化似然函数来最小化损失。\n\n3. Kullback-Leibler 散度损失（Kullback-Leibler Divergence Loss）：也称为相对熵损失，可以度量两个概率分布之间的距离，用于多分类问题中的类别分布预测和真实类别分布之间的匹配程度。\n\n下面就该重头戏”反向传播算法“出场了。\n  \n\n反向传播 (Backpropagation, BP) 是“误差反向传播”的简称, 是一种与最优化方法（如梯度下降法）结合使用 的, 用来训练人工神经网络的常见方法。该方法对网络中所有权重计算损失函数的梯度。这个梯度会反馈给最优 化方法, 用来更新权值以最小化损失函数。\n  \n\n人话翻译: 在前向传播获得预测之后, 就可以通过损失函数求得损失。求得损失之后, 我们希望能够对参数进行 调整，使得损失能够在当前情况下变小一点, 也就是往让损失减小的方向调整一次参数。这个让损失减小的方 向, 就可以通过求得梯度来解决。更新的基本方法为：对于一个参数$\\mathrm{w}$, 求得它的梯度$\\mathrm{d}$ 后, 让它往梯度的反方向 以指定步长a (学习率) 走一步。也就是$\\mathrm{w}(\\mathrm{new})=\\mathrm{w}-\\mathrm{a} * \\mathrm{~d}$ ，这里的a是超参数。\n  \n\n注意！根本之处在于：损失本来就是一个关于参数$\\mathbf{w}$ 和$\\mathbf{b}$ 的函数！\n\n  \n### (12) 将梯度下降过程表述清楚\n\n好的，我会详细地说明每个步骤的含义和具体操作，帮助初学者更好地理解梯度下降算法。\n\n1. 选择一个初始点作为起始点\n\n在梯度下降算法中，我们需要选择一个初始点作为起始点来开始求解函数的最小值。这个初始点可以是随机选择的，也可以是根据经验预设的。它的选取可能会影响到最终结果的收敛速度和质量，因此需要进行一定的实验和调试来确定最佳的初始点。\n\n例如，在训练神经网络时，我们通常会使用一些常见的初始化方法来设置模型的初始参数。例如，对于一个拥有 $n$ 个输入和 $m$ 个输出的全连接层，我们可以按照高斯分布或均匀分布初始化权重 $W$ 和偏置 $b$：\n\n- 高斯分布：$W_{i,j}\\sim N(0, \\sigma^2)$，其中 $\\sigma=\\sqrt{\\frac{2}{n+m}}$\n- 均匀分布：$W_{i,j}\\sim U(-\\frac{\\sqrt{6}}{\\sqrt{n+m}}, \\frac{\\sqrt{6}}{\\sqrt{n+m}})$\n\n这样的初始化方法可以帮助我们避免陷入局部极小值或者梯度消失/爆炸等问题，并提高模型的泛化能力和性能。\n\n2. 计算当前点对应的代价函数关于各个参数的导数（即梯度）\n\n在梯度下降算法中，我们需要计算当前点对应的代价函数关于各个参数的导数（即梯度），以确定函数在该点的变化方向。这个计算过程需要使用微积分知识来求解，对初学者来说可能有一定难度。\n\n例如，在训练神经网络时，我们需要使用反向传播算法来计算代价函数关于模型参数的梯度，从而利用梯度下降算法来更新参数。反向传播算法的主要思想是利用链式法则，将代价函数的梯度逐层传递给前面的层，从而计算出每个参数对代价函数的贡献。\n\n具体来说，我们可以按照以下步骤来计算代价函数关于某个参数的梯度：\n\n- 正向传播：将输入数据送入神经网络，得到输出结果。\n- 计算代价函数：根据输出结果和真实标签计算代价函数。\n- 反向传播：计算代价函数关于输出结果的梯度，然后逐层计算出每个参数对代价函数的梯度。\n- 更新参数：沿着梯度的反方向更新模型的参数，使得代价函数的值逐渐降低。\n\n通过不断地迭代，我们可以逐步优化模型的参数，从而提高模型的性能和准确率。\n\n3. 按照负梯度方向更新当前点的位置，即让当前点沿着梯度的反方向移动一定的距离\n\n在梯度下降算法中，我们需要根据当前点对应的梯度方向，按照一定的步长（即学习率）来更新当前点的位置，使得函数值逐渐减小。这个更新过程可以理解为“爬山”或“下山”的过程，即沿着梯度的反方向不断前进，直到达到最小值或者停止条件。\n\n具体来说，我们可以按照以下公式来更新当前点的位置：\n\n$$\nx_{i+1}=x_i - \\eta \\nabla f(x_i)\n$$\n\n其中，$x_i$ 表示第 $i$ 次迭代的当前点，$\\eta$ 表示学习率（即步长），$\\nabla f(x_i)$ 表示在当前点 $x_i$ 处的梯度。这个公式表示了当前点沿着梯度的反方向移动一定的距离，从而让函数值逐渐减小。\n\n例如，在训练神经网络时，我们可以使用如下公式来更新模型的参数：\n\n$$\nw_{i+1}=w_i - \\eta \\frac{\\partial \\mathcal{L}}{\\partial w_i}\n$$\n\n其中，$w_i$ 表示第 $i$ 次迭代的模型参数，$\\eta$ 表示学习率，$\\mathcal{L}$ 表示损失函数。根据梯度下降算法的定义，我们需要让损失函数逐渐降低，因此需要按照负梯度方向来更新模型的参数。\n\n4. 重复执行步骤2和步骤3，直到满足停止条件\n\n在梯度下降算法中，我们需要重复执行步骤2和步骤3，直到满足停止条件为止。停止条件可以是达到一定的迭代次数、达到一定的精度要求、出现震荡或者发散等情况等等。\n\n例如，在训练神经网络时，我们通常会将数据集分成若干个batch，然后对每个batch进行多次迭代，直到达到一定的迭代次数或者达到一定的精度要求。此外，我们还可以使用一些实用技巧来提高梯度下降算法的效果，例如加入动量、自适应学习率等。\n\n总之，梯度下降算法是深度学习中最基础、最为重要的优化算法之一，初学者需要理解其基本原理和流程，并结合具体实例进行掌握和应用。\n\n\n学到这里, 你应该已经清楚了神经网络的训练过程。下一步的问题就是搞定梯度值。\n\n\n高数那里讲的东西咱们这里就不再细说, 我只想说, 为了求导方便, 为了计算机计算方便, 这里使用链式法则来 进行求导, 也就是会从损失反向往前求得参数节点的导数。\n\n\n\n### (13)看我的视频”手撕神经网络前向、反向、求导、更参“、将例子中的 9 个参数分别手撕求导，并更新参数。\n当输入为x1 = 1，x2 = 2时预测值y = 5但是正确的y为2：\n![](res/2_简单的网络%201.jpg)\n\n我们考虑一个仅包含一个隐藏层的神经网络，其中输入层有2个节点（即$x_1$和$x_2$），隐藏层有2个节点，输出层只有1个节点。偏置 $b$ 在隐藏层和输出层中都为1。\n\n我们定义以下变量：\n\n- 隐藏层的权重：$w_{11}, w_{12}, w_{21}, w_{22}$\n- 隐藏层的偏置：$b_{11},b_{12}$\n- 输出层的权重：$w_{1}, w_{2}$\n- 输出层的偏置：$b_{21}$\n\n我们可以将前向传播计算过程分成两步:\n\n#### 步骤1：计算隐藏层\n\n$$z_1 = w_{11}x_1 + w_{12}x_2 + b_{11}$$\n$$a_1 = \\sigma(z_1)$$\n\n\n$$z_2 = w_{21}x_1 + w_{22}x_2 + b_{12}$$\n$$a_2 = \\sigma(z_2)$$\n\n其中 $\\sigma$ 是激活函数，这里使用 ReLU 函数。\n\n#### 步骤2：计算输出层\n\n$$y = w_{1}a_1 + w_{2}a_2 + b_{21}$$\n\n此处没有使用激活函数。\n\n现在我们需要根据给定的损失函数$L(y,\\hat{y})=\\frac{1}{2}(y-\\hat{y})^2$，对模型参数进行反向传播以进行训练。\n\n#### 反向传播\n\n反向传播的目标是计算代价函数相对于各个变量的导数，以便我们可以使用这些导数来更新每个变量的值。我们从代价函数开始，通过链式法则，逐层计算导数。\n\n首先计算 $L$ 相对于 $y$ 的导数。\n\n$$\\frac{\\partial L}{\\partial y} =  (y-\\hat{y}) = 3$$\n\n接下来计算 $a_1$ 的导数。\n\n$$\\frac{\\partial L}{\\partial a_1} = w_1 \\frac{\\partial L}{\\partial y} = 1 \\times 3 = 3$$\n\n然后，计算 $z_1$ 和 $w_{11}$ 的导数。\n\n$$\\frac{\\partial L}{\\partial z_1} = \\frac{\\partial L}{\\partial a_1}\\frac{\\partial a_1}{\\partial z_1} = 3 \\times [z_1 \u003e 0] = 3$$\n\n其中 $[z_1 \u003e 0]$ 表示当 $z_1 \u003e 0$ 时取值为1，否则为0。\n\n$$\\frac{\\partial L}{\\partial w_{11}} = x_1 \\frac{\\partial L}{\\partial z_1} = 1 \\times 3 = 3$$\n\n现在我们可以使用这些导数来更新模型参数 $w_{11}$\n#### 参数更新\n\n假设我们选择学习率为 $\\eta = 0.01$，则可以按照以下方式更新参数：\n\n$$w_{11} = w_{11} - \\eta \\frac{\\partial L}{\\partial w_{11}} = 1 - 0.01 \\times 3 = 0.97$$\n\n\n### (14)梯度下降、随机梯度下降与小批量梯度下降都分别是怎样设计的?\n\n- 梯度下降（Gradient Descent，GD）：在每次迭代中，计算所有训练样本的损失函数对于模型参数的梯度，然后更新模型参数。具体来说，将当前模型参数 $w$ 沿着负梯度方向移动一定步长 $\\eta$，即 $w \\leftarrow w - \\eta \\nabla_w L(w)$，其中 $\\nabla_w L(w)$ 表示损失函数 $L(w)$ 对于参数 $w$ 的梯度。梯度下降方法可以保证在所有训练样本上最小化损失函数，但每次迭代需要处理大量数据，所以计算效率低下。\n\n- 随机梯度下降（Stochastic Gradient Descent，SGD）：在每次迭代中，随机选择一个训练样本计算梯度，并更新模型参数。与梯度下降不同，SGD 每次只用一个样本计算梯度，所以速度更快。但由于随机性，SGD 更新方向不一定很好地表示了整个训练集的整体趋势，因此可能会导致损失函数收敛到局部最小值而非全局最小值。\n\n- 小批量梯度下降（Mini-batch Gradient Descent，MBGD）：在每次迭代中，随机选择一个大小为 $b$ 的小批量（mini-batch）样本计算梯度，并更新模型参数。与 SGD 不同，MBGD 每次使用多个样本计算梯度，得到的更新方向更有代表性，同时运算速度也比 GD 更快。小批量梯度下降是目前深度学习中最常用的优化算法之一，通常 $b$ 的取值范围在几十到几百之间。\n\n\n## 二、编程部分\n\n\n这次我们开始说PyTorch。\n\n\n通常对于PyTorch的导入写成这样:\n\n  \n```python\nimport torch\n```\n\n\nPyTorch中的数据类型是tensor。类似NumPy中ndarray, 唯一区别是tensor可以在GPU上运算。\n\n\n### (15)请你构建一个三行四列由随机数填充的tensor类型变量，并通过属性获得它的形状。\n```python\nimport torch\n\n# 构建一个形状为 (3, 4) 的张量变量并随机初始化\nx = torch.randn(3, 4)\n\n# 使用 shape 属性获取张量的形状\nprint(x.shape)\n```\n\n\n```python\nx = torch.tensor([[1., 0.],[-1., 1.]],requires_grad = True)\nz = x.pow(2).sum() \nz.backward()\nx.grad\n```\n\n\n### (17)这四行代码都用了哪些函数或属性，具有什么作用? 请进行详细解释。\n\n  ```python\nx = torch.tensor([[1., 0.],[-1., 1.]], requires_grad=True)\nz = x.pow(2).sum()\nz.backward()\nx.grad\n```\n\n这四行代码中使用了以下函数和属性：\n\n1. `torch.tensor()`：用于创建一个张量对象，其可接受各种输入类型（如Python列表、NumPy数组等），并将其转换为PyTorch张量。\n\n2. `.pow(2)`：计算张量x的平方，返回一个新的张量。在本例中，它计算了x的每个元素的平方。\n\n3. `.sum()`：计算张量所有元素的总和，并返回一个标量值。\n\n4. `.backward()`：计算与给定张量相关的梯度。在此示例中，我们调用backward()方法，计算z相对于x的梯度。\n\n5. `.grad`：返回一个保存了计算出的梯度的张量。在这个例子中，我们访问了张量x的grad属性，它包含相对于x的梯度值。\n\n### 18如果将 requires_grad = True 设置为 requires_grad = False 会怎样? 这个属性的作用是什么?\n\u003e `requires_grad`在PyTorch中是一个张量（Tensor）的属性，用于指定是否对该张量进行自动求梯度。如果将requires_grad属性设置为True，则表示希望跟踪所有依赖于该变量的操作，并在计算时自动计算梯度。如果requires_grad属性设置为False，则表示不需要计算相对于该张量的梯度。\n\n\u003e在给定代码中，我们创建了一个2x2的张量x，并将其requires_grad属性设置为True。然后对x的元素平方并求和得到标量z。接下来，我们调用z.backward()方法计算z相对于x的梯度。最后，我们可以通过访问x.grad属性来获得计算出的梯度。\n\n\u003e总之，requires_grad属性允许我们控制哪些张量需要计算梯度以及哪些张量不需要计算梯度，这对于训练深度学习模型非常有用。\n\n### (19) z.backward(). 执行后, 都发生了什么?\n\u003e 执行之后会计算得到对应的梯度，但是此时并没有进行相关的参数更新，需要手动更新\n\n\n### (20)将我的视频”手撕神经网络前向、反向、求导、更参“例子对应代码实现。\n```python\nimport torch\nimport torch.nn.functional as F\n\nx1 = torch.tensor(1) # 当前输入的特征 x1，因为只是输入，所以不需要计算梯度\nx2 = torch.tensor(2) # 当前输入的特征 x2，因为只是输入，所以不需要计算梯度\nY = torch.tensor(2) # 当前输入对应的真实值 Y，因为只是输出，所以不需要计算梯度\n\nw11 = torch.tensor(1., requires_grad=True) # 参数 w11，需要计算梯度\nw12 = torch.tensor(1., requires_grad=True) # 参数 w12，需要计算梯度\nw21 = torch.tensor(1., requires_grad=True) # 参数 w21，需要计算梯度\nw22 = torch.tensor(-1., requires_grad=True) # 参数 w22，需要计算梯度\nw1 = torch.tensor(1., requires_grad=True) # 参数 w1，需要计算梯度\nw2 = torch.tensor(-1., requires_grad=True) # 参数 w2，需要计算梯度\nb11, b12, b21 = torch.tensor([1., 1., 1.], requires_grad=True) # 给定偏置 b11、b12 和 b21，需要计算梯度\n\nlr = 0.01 # 定义学习率\n\n# 前向传播\nz1 = w11 * x1 + w12 * x2 + b11 # 第一层神经元输入加权和 z1\na1 = F.relu(z1) # 第一层神经元输出 a1（经过 ReLU 函数激活）\nz2 = w21 * x1 + w22 * x2 + b12 # 第二层神经元输入加权和 z2\na2 = F.relu(z2) # 第二层神经元输出 a2（经过 ReLU 函数激活）\ny = w1 * a1 + w2 * a2 + b21 # 输出层输入加权和 y\n\n# 求损失\nloss = (y - Y).pow(2) / 2 # 计算当前模型预测值 y 与真实值 Y 的平方差\n\n# 求参数梯度\nw11_grad = (y - Y) * w1 * 1 * x1 # 手动计算参数 w11 的梯度\n\nloss.backward() # 自动计算所有参数的梯度\n\n# 打印输出手动计算的梯度、自动计算的梯度以及它们是否相等\nprint(w11_grad, w11.grad, w11_grad == w11.grad)\n\nw11 = w11 - lr * w11_grad # 更新参数 w11\n\n```\n\n\n最后, 有余力的同学去看看PyTorch中的计算图是怎么回事, 这会让你对自动微分这件事儿更通透, 也会让你在网络的搭建过程中更加明朗。","lastmodified":"2023-06-07T15:55:55.954775445Z","tags":["机器学习"]},"/notes/machine_leaning/begin_home_work/%E4%BD%9C%E4%B8%9A%E8%AE%AD%E7%BB%83%E8%90%A5-%E7%AC%AC%E4%BA%94%E5%91%A8":{"title":"机器学习基础作业班 第五周","content":"\n我们已经学过卷积神经⽹络以及对应的⼀些经典模型，这次课开始进⼊时序信息处理的模型。其实这两种技术对应了深度学习⾥⾯两个主流领域，计算机视觉(cv)和⾃然语⾔处理(NLP)。\n\n先来思考⼀个问题，为什么经常使⽤卷积神经⽹络来处理图像问题，⽤时序模型来处理⾃然语⾔问题？因为在图像处理过程中，图像空间信息很重要。⼀个像素点和它上下左右周围⼀圈包围的像素点关系密切，⽽全连接神经\n\n⽹络在将图像数据展开的同时会丢掉这些信息，所以卷积核的作⽤就能将⼀个⼩空间范围内的信息⼀起处理，这就很合理了。\n\n然⽽⾃然语⾔⾥⾯的问题更多依赖的是上下⽂（当前⽂字的前⾯和后⾯的若⼲⻓度的⽂字）关系，⽽不是空间关系，就像你做完形填空，你不会着重关注这个空对应的上⼀⾏这个位置的词和下⼀⾏这个位置的词⼀样。\n\n所以我们现在关注的是前后若⼲个时间单位的信息对当前信息的影响，这就是时间序列模型的由来。这次内容我们会学习到经典的简单RNN 和 LSTM 。\n\n可以把时序模型常解决的问题根据输⼊和输出的序列⻓度分为有这样⼏种情况：\\\"1-N\\\"、\\\"N-1\\\"、\\\"N-N\\\"(输⼊输出都为多个但是等⻓)、\\\"N-M\\\"（输⼊输出都为多个但不等⻓）。\n\n### ①这可以具体对应哪些问题？分别举例说明。\n1.  \"1-N\"情况：这种情况下，输入序列长度为1，输出序列长度为N。典型的实际应用如：\n    -   输入某个时间点的数据，例如股票价格、天气信息等，预测接下来几个时间点的数据。例如，输入今天的股票价格，预测接下来3天的股票价格。\n2.  \"N-1\"情况：这种情况下，输入序列长度为N，输出序列长度为1。典型的实际应用如：\n    -   在自然语言处理中，可将一段文本输入模型，输出为该文本的情感分类（如正面、负面）。此时，输入为一段文本（N个词），输出为一个类别。\n    -   在时间序列预测中，输入历史数据，预测下一个时间点的值。例如，输入过去一个月的销量数据，预测明天的销量。\n3.  \"N-N\"情况：这种情况下，输入序列长度为N，输出序列长度也为N。典型的实际应用如：\n    -   机器翻译：将一段源语言文本翻译成目标语言文本。输入为一段源语言文本（N个词），输出为目标语言文本（N个词，源序列与目标序列一一对应）。\n    -   序列标注任务：输入一段文本，为每个词分配一个标签。例如，命名实体识别（将人名、地名等标注出来）。输入序列为文本（N个词），输出序列为标签（N个标签，与输入文本一一对应）。\n4.  \"N-M\"情况：这种情况下，输入序列长度为N，输出序列长度为M（N和M可以不等长）。典型的实际应用如：\n    -   文本摘要任务：根据输入的一段较长的文本，输出一段较短的摘要。此时，输入为一段长文本（N个词），输出为一段摘要（M个词，M小于N）。例如，根据新闻报道生成简短的新闻标题。\n    -   对话系统：输入为用户的多个对话轮次（N轮），输出为机器人的多个回复（M轮）。例如，在输入用户提问的过程中，机器人可能会在中间插入一些引导性的问题或补充提醒，从而使得输入和输出序列长度不同。\n\n假如我们现在要从这句话\\\"你知道中国有个教AI的⽼师叫陶卓吗\\\"中识别出⼈名，\n### ②这对应的问题①中哪个情况？解决这个问题该怎么做？\n\u003e 从这句话\"你知道中国有个教AI的老师叫陶卓吗\"中识别出人名，这对应前者中说的\"N-1\"情况。因为输入是一个长度为N的序列（整句话），输出是一个长度为1的序列（人名）。\n\n\n### 通常情况下会先进⾏分词然后对每个词进⾏编码（也可以不分词直接对每个字进⾏编码），编码的两种基本处理⽅法是\\\"one-hot\\\"和\\\"word2vec\\\"，③请分别解释这两种编码⽅式，并举例说明。\n要从这句话\"你知道中国有个教AI的老师叫陶卓吗\"中识别出人名，可以使用命名实体识别（Named Entity Recognition，NER）方法。首先对句子进行分词处理，然后对每个词进行编码。编码主要有两种常见方法：\"one-hot\"和\"word2vec\"。\n\n1. One-hot编码：\n\nOne-hot编码是一种用二进制向量表示词语的方法。它将每一个词汇表示为一个向量，这个向量的长度为词汇表中所有词的数量，其中向量的一个元素为1，表示当前词汇，其他元素都为0。\n\n例如，我们的词汇表中有5个词：{\"中国\"，\"有个\"，\"教AI的\"，\"老师\"，\"叫陶卓\" }，词汇表中每个词所对应的one-hot编码：\n\n中国 -\u003e [1, 0, 0, 0, 0] \n有个 -\u003e [0, 1, 0, 0, 0] \n教AI的 -\u003e [0, 0, 1, 0, 0] \n老师 -\u003e [0, 0, 0, 1, 0] \n叫陶卓 -\u003e [0, 0, 0, 0, 1]\n\n缺点是向量维度过高，耗费计算资源，且无法表示词汇之间的关联信息。\n\n2. Word2vec编码：\n\nWord2vec编码是一种连续向量空间模型，将词汇表中的每个词映射到一个连续的向量空间，词汇之间的关联性表现为向量的距离或夹角。Word2vec通常基于上下文来训练模型，捕捉词汇间的关系。\n\n例如，预先训练好的word2vec模型将每个词汇映射到一个固定长度的向量，如下所示：\n\n中国 -\u003e [0.1, 0.3, -0.2, 0.5, 0.05] \n有个 -\u003e [0.2, -0.1, 0.4, 0.3, -0.1] \n教AI的 -\u003e [0.3, 0.2, -0.1, 0.4, 0.1] \n老师 -\u003e [-0.1, 0.5, 0.1, 0.1, 0.2] \n叫陶卓 -\u003e [0.3, 0.1, 0.05, 0.2, 0.45]\n\n通过Word2vec编码，可以获得词汇之间的关系和相似性。例如，“中国”和“教AI的”在某些维度上可能有较高的值，表示它们有关联。这种编码方式的计算量较小且能表示词汇间的关联信息。\n\n总之，one-hot编码和word2vec编码都可以将词汇转换为数值表示，以便计算机处理。one-hot编码适用于简单场景，计算简单，但无法表示词汇间的关联。Word2vec编码能捕捉词汇间的关系，计算量较小，更适合表示词汇间的关联信息。\n\n## RNN ( Recurrent Neural Network )\n\n可以通过下图来解释刚才这个问题使⽤RNN是如何处理的。\n\n![](res/作业训练营_第五周_rnn.png)\n\n### ④根据上图来解释RNN的计算过程。\n\n将{\"中国\"，\"有个\"，\"教AI的\"，\"老师\"，\"叫陶卓\" }中每个词汇对应的词向量依次输入到RNN神经网络中进行训练：\n1. 最初时刻，网络的状态 $h_0$ 初始化为0。将第一个单词\"你\"转换成嵌入向量 $x_1$，并输入RNN网络中进行计算：\n\n$$a_1 = softmax(W_{ax}x_1 + W_{aa}a_0 + b_a)$$\n\n其中，$W_{ax}$ 是输入权重矩阵，$W_{aa}$ 是上一次处理结果的权重矩阵，$b_a$ 是偏置向量。\n\n然后，我们可以使用$h_1$的计算公式来计算隐藏层的状态：\n\n$$h_1 = tanh(W_{h}a_1 + b_h)$$\n\n其中，$W_{h}$ 是将输出转换为隐藏状态的权重矩阵， $b_{h}$ 是隐藏层的偏置向量。\n\n2.现在，第一个单词\"你\"已经被传输并进入第一个隐藏层，我们需要处理句子中的下一个单词\"知道\"。将\"知道\"转换为嵌入向量 $x_2$ 并输入RNN网络。我们可以将上一个时间步的处理结果 $a_1$作为输入。\n\n$$a_2 = softmax(W_{ax}x_2 + W_{aa}a_1 + b_a)$$\n\n$$h_2 = tanh(W_{h}a_2 + b_h)$$\n\n3.现在，我们将第二个单词的处理结果 $a_2$ 作为下一个隐藏层 $a_3$ 的输入。继续以这种方式进行句子中所有单词的处理。\n\n$$a_3 = softmax(W_{ax}x_3 + W_{aa}a_2 + b_a)$$\n\n$$h_3 = tanh(W_{h}a_3 + b_h)$$\n\n$$a_4 = softmax(W_{ax}x_4 + W_{aa}a_3 + b_a)$$\n\n$$h_4 = tanh(W_{h}a_4 + b_h)$$\n\n$$a_5 = softmax(W_{ax}x_5 + W_{aa}a_4 + b_a)$$\n\n$$h_5 = tanh(W_{h}a_5 + b_h)$$\n\n$$a_6 = softmax(W_{ax}x_6 + W_{aa}a_5 + b_a)$$\n\n$$h_6 = tanh(W_{h}a_6 + b_h)$$\n\n$$a_7 = softmax(W_{ax}x_7 + W_{aa}a_6 + b_a)$$\n\n$$h_7 = tanh(W_{h}a_7 + b_h)$$\n\n在这个过程中，每个时间步都充当序列中的一个单词，并生成一个输出向量 $a$ 以及一个隐藏状态向量 $h$ 作为最后的预测结果。重要的是，RNNs的隐藏状态从一个时间步流传到下一个时间步，并且在整个时间序列中共享参数。这使得RNNs非常适合处理输入数据组成的序列，如自然语言句子。\n\n### ⑤模型中需要学习的参数在哪⼉？有⼏组？请在上图中画出。\n\n\u003e 一共有3组，分别是 $W_{ax}$  $W_{aa}$  $W_h$\n\n### ⑥中间某时刻的输出h应当如何求得？\n将当前时刻输出乘以 $W_h$ 矩阵 加上 $b_h$ 偏置后应用激活函数即可\n$$h_1 = tanh(W_{h}a_1 + b_h)$$\n\n### ⑦对于当前的问题，损失应当如何求得？\n\n在当前问题中，我们使用RNN预测自然语言句子中每个单词的下一个单词。我们可以使用交叉熵损失函数（cross-entropy loss）来衡量模型的损失。交叉熵损失函数被广泛用于分类问题中，其可以衡量模型预测的概率分布与实际概率分布之间的差异。而在本问题中，我们希望用它来度量模型预测下一个单词的概率分布与实际下一个单词的标签之间的差异。\n\n对于每个时间步 $t$，我们可以使用下面的公式计算交叉熵损失函数：\n\n$$L_{t}=-\\sum_{i=1}^{V} y_{t,i} \\log a_{t,i} $$\n\n其中 $V$ 表示词汇表的大小；$y_{t}$ 是真实下一个单词的标签 one-hot 向量，其中仅有一个元素为 $1$，与当前时间步对应； $a_{t}$ 是模型在当前时间步预测下一个单词的概率分布向量，可以通过 softmax 函数进行计算。上式中，用于计算交叉熵损失函数的具体表达式可以理解为：首先对所有可能的标签概率进行对数变换，然后与对应的真实标签进行逐元素相乘后得到一组交叉熵损失值，最后对这些损失值进行求和，得到该时间步损失函数的值。\n\n总体上，我们需要对序列的所有时间步骤上的损失进行求和，用 $L_{total}$ 表示总的损失函数，公式为：\n\n$$L_{total} = \\sum_{t=1}^{T}L_{t}$$\n\n其中 $T$ 表示句子中单词的总数。最小化 $L_{total}$ 可以使模型更准确地预测下一个单词，进而提高模型的能力。\n\n### ⑧这样基于时间的反向传播如何进⾏？\n\n\n\n### ⑨为什么说这样的⽹络能够传递时间序列信息？\n\nRNN网络是一种可以处理序列数据的神经网络，其结构中具有时间步的概念，并且在不同时刻可以使用相同的权值和偏置来计算输出和隐藏层值。\n\n因此，当输入RNN网络的是一个时间序列的数据时，例如语言句子或深度学习中的音频，视频数据等，RNN网络可以通过反复传递前一时刻的隐藏状态做到信息记忆传递。\n\n在每个时间步上，RNN将输入与先前时间步的隐藏状态一起组合，并生成一个新的隐藏状态，同时生成预测输出。这个新状态可以被传递到下一个时间步上，并作为第一个新状态的一部分，这样一直循环下去。在这个过程中，RNN携带和传递了输入序列中随时间变化的信息，从而在模型中捕获数据的序列特征。这意味着 RNN 的隐藏状态在当前时刻捕捉到的信息会影响到后续时刻的隐藏状态和输出。\n\n因此，我们可以将RNN描述为一种可以处理时间序列数据的神经网络，并且随着时间的推移，每个隐藏状态可以捕捉到从序列开始到当前时间步长的信息，这使得RNN网络能够容易的传递和利用时间序列中的信息，可以用于预测和生成输出序列的任务。\n\n### ⑩参考\\\"N-1\\\"结构，设计RNN⽹络实现MNIST的识别。\n\n对于这种情况，我们需要将RNN网络改为一个接收长度为1的序列并产生长度为N的输出序列的模型。在PyTorch中，我们可以使用nn.RNN函数来创建一个基本的RNN层。为了将其应用于变长序列输入，我们需要将batch_size设置为1并向前传递一个时间步长的输入。在每个时间步长上，我们可以通过将输入以及上一时间步的隐藏状态作为输入到RNN层，然后计算输出和隐藏状态。在得到最终输出后，我们可以将输出通过softmax函数进行计算，将其转换为概率分布并得到预测结果。我们可以根据一个类似的方式来训练和优化模型。\n\n```python\nimport torch \nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets,transforms\n\n# 定义超参数和数据预处理\ninput_size = 28\nhidden_size = 128\noutput_size = 10\nnum_layers = 1\nlearning_rate = 0.01\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\n# 加载MNIST数据集\ntrain_dataset = datasets.MNIST('../data', train=True, download=True,\n            transform=transform)\n\ntest_dataset = datasets.MNIST('../data', train=False, \n            transform=transform)\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, \n                                        shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, \n                                        shuffle=True)\n\n# 构建RNN模型\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, num_layers):\n        super(RNN, self).__init__()\n\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, 1, self.hidden_size).to(device)\n        c0 = torch.zeros(self.num_layers, 1, self.hidden_size).to(device)\n\n        out, _ = self.lstm(x, (h0, c0))\n\n        out = self.fc(out[-1])\n\n        return out\n\n# 初始化模型并定义损失函数和优化器\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = RNN(input_size, hidden_size, output_size, num_layers).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# 训练模型\nfor epoch in range(10):\n    running_loss = 0.0\n\n    for images, labels in train_loader:\n        images = images.view(1, 1, -1).to(device)\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    print(f\"Epoch[{epoch+1}/10], loss: {running_loss/len(train_loader):.4f}\")\n\n# 测试模型\nwith torch.no_grad():\n    correct = 0\n\n    for images, labels in test_loader:\n        images = images.view(1, 1, -1).to(device)\n        labels = labels.to(device)\n\n        outputs = model(images)\n\n        _, predicted = torch.max(outputs, 1)\n        correct += (predicted == labels).sum().item()\n\n    print(f\"Test accuracy: {correct/len(test_loader)*100:.2f}%\")\n\n```\n\n\n## LSTM(Long Short-Term Memory unit)\n\nLSTM是上⾯简单RNN的⼀种变种，其引⼊\\\"⻔控单元\\\"来减缓梯度消失问题，使得模型更容易学习到⻓期依赖。换句话说记住⻓期信息是LSTM的默认⾏为，不是需要付出很⼤代价才能获得的能⼒。来看看它⻓什么样⼦：\n\n\n![](res/作业训练营_第五周_LSTM.png)\n\n\n\n你可以把上图理解成在LSTM中的⼀个处理信息的基本单元。在这⾥：\n\n\na 是当前单元的输出，当然根据不同的问题形式这⾥也可以不输出\nC 代表的是记忆信息\nx 是输⼊的信息\nt 代表某⼀时刻\n\n### 现在在这过程中使⽤的激活函数有\\\"sigmoid\\\"和\\\"Tanh\\\",⑪请解释上图中所有标记和符号。\n\n-   xt：当前时刻t的输入\n-   ht：当前时刻t的输出，也称为隐藏状态\n-   ct：当前时刻t的单元状态（cell state）\n-   ft：当前时刻t的遗忘门值\n-   it：当前时刻t的输入门值\n-   ot：当前时刻t的输出门值\n-   σ：sigmoid激活函数，用于门结构的激活\n-   tanh：双曲正切激活函数\n\n在这个结构的设计中，最上⾯的那条线表示模型保留和传递记忆信息 C，但在当前这个t时刻，C 中所有信息也不⼀定要全部拿来使⽤，所以设计了⼀个\\\"遗忘⻔\\\"进去，⽤来控制记忆信息的去留。\n\n\n### ⑫如何计算得到遗忘⻔的具体数值？\n在LSTM中，遗忘门（forget gate）是用于控制之前时间步的信息对当前时间步影响的一种机制。其工作原理是通过根据输入序列和上一时间步的隐藏状态来决定需要遗忘的信息。遗忘门的数值是由神经网络层和sigmoid激活函数计算得到的。\n\n具体来说，遗忘门的数值通过以下公式计算得到：\n\n$$f_{t}=\\sigma (W_{f}[h_{t-1},x_{t}]+b_{f})$$\n\n其中$W_{f}$和$b_{f}$分别表示遗忘门的权重矩阵和偏置项。$h_{t-1}$表示上一时间步的隐藏状态，而$x_{t}$表示当前时间步的输入。  $\\sigma$是sigmoid激活函数，用于将输出限制为介于0和1之间的概率。\n\n在LSTM模型的训练过程中，我们需要在每个时间步计算遗忘门的值。这样，我们就能在当前时间步中控制遗忘之前时间步的信息。具体来说，可以通过对序列中的所有时间步进行前向计算，用交叉熵损失函数计算模型的损失，再通过反向传播算法计算导数并更新权重矩阵，实现模型的训练和优化。\n\n### ⑬遗忘后的记忆信息如何表示？\n\n在LSTM神经网络中，除了遗忘门，还有记忆单元（memory cell）可以帮助模型长期记忆之前时间步的信息。具体来说，在每个时间步中，与遗忘门一起计算得到的新单元状态（cell state）可以通过公式\n\n$$c_t = f_t \\cdot c_{t-1} + i_t \\cdot \\tilde{c_t} $$\n\n中的遗忘门$f_t$与以往筛选的新单元状态$\\tilde{c_t}$来生成当前时间步的新单元状态$c_t$。这个式子意味着，若前一个时间步的遗忘门$f_{t-1}$的数值为0，则新单元状态$c_{t}$也不会受到之前时间步信息的影响。\n\n其中，$i_t$是输入门，$\\tilde{c_t}$表示新的单元状态，具体计算方式为：\n\n$$\\tilde{c_t} = tanh(W_c \\cdot [h_{t-1}, x_t] + b_c)$$\n\n其中，$W_c$和$b_c$是参数，$h_{t-1}$是上一个时间步的隐藏状态，$x_t$是当前时间步的输入。\n\n这里，遗忘门的值通过公式：\n\n$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n\n计算得到，其中$W_f$和$b_f$是参数，$\\sigma$是sigmoid激活函数。\n\n因此，遗忘后的记忆信息就是新单元状态$c_t$。该信息是由上一个时间步的记忆信息$c_{t-1}$和当前时间步的遗忘门$f_t$计算得到的，表示了模型选择保留哪些之前的信息并将其传递到下一个时间步进行计算。\n\n\n\n输⼊的信息也不⼀定都要拿来使⽤，这⾥还有⼀个\\\"输⼊⻔\\\"来控制输⼊信息的哪部分可以进⼊记忆信息。\n\n### ⑭如何计算得到输⼊⻔的具体数值？\n在LSTM中，输入门（input gate）的作用是决定当前时间步的输入信息对当前单元状态的影响。具体而言，输入门将前一个时间步的隐藏状态和当前时间步的输入计算为新的输入，该新输入可以与之前的单元状态相加得到当前的单元状态。输入门的具体数值由神经网络层和sigmoid激活函数计算得到。\n\n具体来说，输入门的数值可以通过以下公式计算得到：\n\n$$i_{t}=\\sigma (W_{i}[h_{t-1},x_{t}]+b_{i})$$\n\n其中，$W_{i}$和$b_{i}$分别为输入门的权重矩阵和偏置项，$h_{t-1}$表示上一时间步的隐藏状态，$x_{t}$表示当前时间步的输入。$\\sigma$为sigmoid激活函数，用于将输出限制为介于0和1之间的概率。\n\n在LSTM的训练过程中，我们需要在每个时间步计算输入门的数值。这样，我们就能在当前时间步中控制之前时间步的输入信息对记忆信息的影响。具体来说，可以通过对序列中的所有时间步进行前向计算，用交叉熵损失函数计算模型的损失，再通过反向传播算法计算导数并更新权重矩阵，实现模型的训练和优化。\n\n### ⑮输⼊新信息后的记忆信息如何表示？\n在LSTM中，输入门和遗忘门都帮助模型更好地控制以前的状态是否影响到当前状态，同时新的cell状态也会被生成以输入新的信息。具体来说，对于给定时间步$t$，输入门$i_t$可以决定哪些信息应该输入到新的单元状态，而遗忘门$f_t$可以决定哪些信息应该从旧的单元状态中遗忘。在加入新的信息后，新的cell状态$c_t$计算方式为：\n\n$$c_t = f_t * c_{t-1} + i_t * \\tilde{c_t} $$\n\n其中，$\\tilde{c_t}$是新的信息矩阵。根据当前时间步的输入信息$x_t$和前一时间步的隐藏状态$h_{t-1}$，可以计算出新的信息矩阵。具体计算方式为：\n\n$$\\tilde{c_t} = tanh(W_c \\cdot [h_{t-1}, x_t] + b_c)$$\n\n将上面两个公式代入可得：\n\n$$c_t = f_t * c_{t-1} + i_t * tanh(W_c \\cdot [h_{t-1}, x_t] + b_c)$$\n\n其中，$W_c$和$b_c$是新信息矩阵的权重矩阵和偏置项。这个式子表明，输入门可以通过$tanh$变换来控制当前时间步新输入数据的输入量，这相当于对新数据进行了筛选处理。遗忘门可以通过与之前倍数相乘来控制旧数据的保留量。这样一来，模型就可以在训练过程中学习长期依赖，并在每个时间步中更好地控制哪些数据会被输入，并控制当前状态与之前状态的权重分配。\n\n这样，输入新信息后的记忆信息即为新计算出的cell状态$c_t$，它是由当前时间步的输入$x_t$，上一个时间步的隐藏状态$h_{t-1}$，以及前一个时间步的cell状态$c_{t-1}$计算得到的。\n\n\n\n你可能已经猜到，现在的记忆信息是不是都要输出也不⼀定，这⾥还有⼀个\\\"输出⻔\\\"来控制现在的记忆信息中哪些要往后输出。\n\n\n### ⑯如何计算得到输出⻔的具体数值？\n在LSTM模型中，输出门（output gate）是用来决定当前时间步的隐藏状态ht对当前时间步输出y有何贡献。输出门的具体数值是由神经网络层和sigmoid激活函数计算得到。\n\n具体而言，输出门的数字可以通过以下公式计算得到：\n\n$$o_t = \\sigma(W_o [h_{t-1}, x_t] + b_o)$$\n\n其中，$W_o$和$b_o$分别是输出门的权重矩阵和偏置项，$h_{t-1}$表示上一个时间步的隐藏状态，$x_t$表示当前时间步的输入，$\\sigma$是sigmoid激活函数，用于将输出限制为介于0和1之间的概率。\n\n在LSTM的训练过程中，我们需要在每个时间步计算输出门的数值。这将使我们能够控制隐藏状态对当前时间步输出的影响程度，使我们能够进一步优化模型。我们可以通过对序列中的所有时间步进行前向计算，使用交叉熵损失函数计算模型的损失，利用反向传播算法计算导数并更新权重矩阵，最终实现模型的训练和优化。\n\n### ⑰t时刻的输出a如何求得？\n在LSTM中，当前时间步$t$的输出$a_t$由隐藏状态$h_t$和输出门$o_t$计算得到。而隐藏状态$h_t$是由当前时间步单元状态$c_t$和其对应的$tanh$函数值计算而成。具体而言，输出门$o_t$控制了当前时间步的隐藏状态$h_t$对当前时间步输出$a_t$的影响程度。单元状态$c_t$表示在此时间步长期记忆的内容。因此，$t$时刻的输出$a_t$的具体计算方式为：\n\n$$a_t = h_t = o_t * tanh(c_t)$$\n\n其中，\n\n$$h_t = tanh(c_t)$$\n\n根据之前的公式：\n\n$$c_t = f_t * c_{t-1} + i_t * \\tilde{c_t} $$\n\n可以得到单元状态$c_t$，其中$f_t$是遗忘门的值，$i_t$是输入门的值，$\\tilde{c_t}$是新的单元状态。将$c_t$代入上面公式中，得到：\n\n$$a_t = o_t * tanh(c_t) = o_t * tanh(f_t * c_{t-1} + i_t * \\tilde{c_t})$$\n\n现在我们已经知道$f_t$、$i_t$、$\\tilde{c_t}$和$o_t$的值，因此，我们可以计算出$tanh(f_t * c_{t-1} + i_t * \\tilde{c_t})$并乘以$o_t$，得到当前时间步的隐藏状态$h_t$，最终得到当前时间步的输出$a_t$。\n\n综上所述，$t$时刻的输出$a_t$是当前时间步的隐藏状态$h_t$乘以输出门$o_t$的值，即：\n\n$$a_t = h_t * o_t = tanh(c_t) * o_t = o_t * tanh(f_t * c_{t-1} + i_t * \\tilde{c_t})$$\n\n### 现在，这个LSTM的主要问题就分析完了，可是，⑱为什么这样设计就能缓解了RNN中的梯度不稳定问题呢？\n在标准的RNN中，梯度消失和梯度爆炸问题都是由于网络的长期依赖关系造成的。长期依赖关系通常会导致长序列的计算，进而使网络在反向传播过程中的梯度迅速消失或者爆炸。\n\n而LSTM可以缓解这些问题，主要有以下原因：\n\n（1）门控机制：LSTM通过引入门控机制来调节信息的流动，包括输入门、遗忘门、输出门等。\n\n在输入门的影响下，LSTM可以选择性地过滤掉无关的输入信息。在遗忘门的影响下，LSTM可以选择性地遗忘掉一些旧的记忆信息。在输出门的影响下，LSTM可以选择性地输出当前时间步的记忆信息。\n\n这些门控制机制使得LSTM可以更好地控制信息的流动，在反向传播时梯度可以更好地传播，从而避免梯度消失或者梯度爆炸的问题。可以将门视为一种筛子，使得网络可以在经过多个时间步后，仅保留有用的信息，并遗忘无关的信息，从而使网络更好地处理长序列数据。\n\n（2）长期记忆单元：LSTM引入了长期记忆单元，该单元可以结合各个时间步的信息，从而使LSTM可以处理长期的信息依赖关系。其结构保持了不同时刻的依赖关系，使得LSTM可以更好地捕捉长期的序列依赖关系。\n\n（3）非线性变换机制：LSTM使用非线性变换机制，比如双曲正切函数，这使得LSTM可以有效地循环计算信息，并保证每个时间步信息的映射有非线性变换的能力。\n\n综上所述，LSTM引入的门控制机制及长期记忆单元，以及非线性变换机制，可以有效地缓解RNN中的梯度消失和梯度爆炸的问题。从而使得LSTM可以更好地捕捉长期的依赖关系，并在处理长序列数据时，表现出更出色的性能。\n\n### ⑲有余⼒的同学，可以了解下基于LSTM的⼀些变种。\n除了标准的LSTM之外，还有许多基于LSTM的变体，其中一些变体可以进一步提高模型的性能，包括：\n\n1. 双向LSTM（BiLSTM）：BiLSTM将两个LSTM结构连接在一起。一个LSTM处理正向序列，另一个LSTM处理反向序列。这样可以利用正向和反向信息来捕捉更丰富的上下文语义信息，进一步提高模型的性能。\n\n2. 树形LSTM（Tree-LSTM）：适用于处理树型结构数据的LSTM变体。在Tree-LSTM中，每个节点的输入都由其直接前驱节点的输出和子节点的输出产生。这种结构利用了树形信息结构的递归特性，可以更好地建模自然语言中的句法结构，提高自然语言处理任务的性能。\n\n3. 注意力机制与LSTM结合（Attention-LSTM）：在Attention-LSTM中，注意力机制用于选择对当前预测最关键的信息，而LSTM用于处理序列的每个时间步。通过结合这两种方法，Attention-LSTM可以选择性地关注输入序列的不同部分，提高模型在序列任务中的性能。\n\n4. 多层LSTM：通过增加LSTM的层数，可以更好地建模复杂的序列依赖关系，提高模型的性能。多层LSTM可以在每个时间步将隐藏状态传递到下一层，从而使得模型能够捕捉不同阶段的信息，进一步提高模型的效果。\n\n5. 长短时记忆卷积神经网络（LSTM-CNN）：LSTM-CNN是一种将LSTM和卷积神经网络（CNN）结合在一起的模型。LSTM用于处理时间序列数据的长期信息，而CNN则用于学习特征。这种结合可以在不同的任务中提供很好的性能，并且在某些自然语言处理任务中特别有效。\n\n总之，以上这些LSTM的变体可以处理不同类型和形式的输入数据，并用于不同的自然语言处理任务。深入了解这些技术有助于我们更好地选择和使用适用于特定任务的模型。\n\n### ⑳来，⽤LSTM也试试实现MNIST的分类吧\n\n首先，先通过以下代码导入所需的库：\n\n```python\nimport torch\nimport torch.nn as nn\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\n```\n\n接下来，定义超参数：\n\n```python\ninput_size = 28# 输入大小，MNIST的每行有28个像素\nsequence_length = 28# 序列长度，MNIST的每张图片有28行\nnum_layers = 2# LSTM的层数\nnum_classes = 10# 10个分类结果，0到9\nbatch_size = 100# 每次批量处理的图片数量\nhidden_size = 128# 隐藏层维度\nnum_epochs = 5# 执行的epoch数\nlearning_rate = 0.01# 学习速率\n```\n\n加载MNIST数据集：\n\n```python\ntrain_dataset = dsets.MNIST(root='./data',\n                            train=True,\n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='./data',\n                           train=False,\n                           transform=transforms.ToTensor())\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=batch_size,\n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                          batch_size=batch_size,\n                                          shuffle=False)\n\n```\n\n定义LSTM网络类：\n\n```python\nclass LSTMNet(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(LSTMNet, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, x):\n# 初始化隐藏层状态和细胞状态\n        h0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n        c0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n\n# LSTM的正向传递\n        out, _ = self.lstm(x, (h0, c0))\n\n# 解码最后一个时间步的隐藏状态\n        out = self.fc(out[:, -1, :])\n        return out\n\n```\n\n实例化并配置网络参数：\n\n```python\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nrnn = LSTMNet(input_size, hidden_size, num_layers, num_classes)\nrnn.to(device)\n\n# 构建损失函数和优化器\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(rnn.parameters(), lr=learning_rate)\n\n```\n\n训练网络：\n\n```python\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = Variable(images.view(-1, sequence_length, input_size)).to(device)\n        labels = Variable(labels).to(device)\n\n# 清除梯度\n        optimizer.zero_grad()\n\n# 前向传播\n        outputs = rnn(images)\n\n# 计算损失和梯度\n        loss = criterion(outputs, labels)\n        loss.backward()\n\n# 优化参数\n        optimizer.step()\n\n# 打印信息if (i+1) % 100 == 0:\n            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n                  % (epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.item()))\n\n```\n\n测试训练好的模型：\n\n```python\ncorrect = 0\ntotal = 0\nfor images, labels in test_loader:\n    images = Variable(images.view(-1, sequence_length, input_size)).to(device)\n    labels = labels.to(device)\n    outputs = rnn(images)\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum().item()\n\nprint('Test Accuracy: %.2f%%' % (100 * correct / total))\n```\n","lastmodified":"2023-06-07T15:55:55.954775445Z","tags":["机器学习"]},"/notes/machine_leaning/begin_home_work/%E4%BD%9C%E4%B8%9A%E8%AE%AD%E7%BB%83%E8%90%A5-%E7%AC%AC%E5%9B%9B%E5%91%A8":{"title":"机器学习基础作业班 第四周","content":"\n如果在上次的作业中, 你使用PyTorch实现了LeNet网络的定义的话, 你应当已经知道一个网络的定义过程了。把 对应的功能封装成函数后根据你对网络的设计将这些零件拼装在一起, 现在我们会用到全连接层、卷积层、池化 层和激活函数, 之后随着网络设计的复杂, 你会见到更多的零件, 比如dropout、Batch normalization等等。所以 先学理论, 搞清楚这些零件的作用, 以及网络的结构, 用代码实现只是分分钟的事儿。\n\n以下是一个可以用来实现LeNet网络的class, 目的是之后实例出这个网络去跑MNIST数据集。上次有同学说定义 网络不可思议, 你看看是不是我说的零件拼装, 搞定这个以后理论上你可以尝试自己去构建你的网络了。\n```python\nimport torch\nimport torch.nn as nn \nclass LeNet(nn.Module):\n    def __init__(self):\n        super(LeNet, self).__init__()\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.pool1 = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.pool2 = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n    def forward(self, x):\n        x = self.pool1(torch.sigmoid(self.conv1(x)))\n        x = self.pool2(torch.sigmoid(self.conv2(x)))\n        x = x.view(-1, 16 * 4 * 4)\n        x = torch.sigmoid(self.fc1(x))\n        x = torch.sigmoid(self.fc2(x))\n        x = self.fc3(x)\n    return x\n```\n\n参考上例回答\n\n\n### (1) init (self) 部分定义了什么?\n定义了一些网络层，其中有卷积层，最大池化层和全连接层\n\nconv1为 输入通道数1，输出通道数6，卷积核大小为5x5的卷积层，\n\npool1为 卷积核大小为2x2 步长为2 的最大池化层，\n\nconv2为 输入通道数6，输出通道数16，卷积核大小为5x5的卷积层，\n\npool2为 卷积核大小为2x2 步长为2 的最大池化层，\n\nfc1为 输入张量特征数为16x4x4=256，输出特征数为120 的全连接层\n\nfc2为 输入张量特征数为120，输出特征数为84 的全连接层\n\nfc3为 输入张量特征数为84，输出特征数为10 的全连接层\n\n\n### (2) forward (self, x ) 部分定义了什么?\n将__init__(self)初始化过程中创建的网络层进行连接，其中还添加了sigmoid激活函数，流程大致为：\n\n输入 → conv1 卷积层 → sigmoid激活函数 → pool1最大池化 → conv2卷积层 →\n\nsigmoid激活函数 → pool2最大池化 → view卷积层摊平操作，用于拼接后续的全连接层 →\n\nfc1全连接层 → fc2全连接层 → fc3全连接层 →输出\n\n\n### (3) nn. Linear 的作用是什么?\n可以创建一个全连接层，创建时可以定义输入特征数和输出的特征数\n  \n\n### (4) x.view \\((-1,16 * 4 * 4)\\) 的作用是什么? 为什么需要这么做?\n\u003e x.view(-1, 16 * 4 * 4) 表示将x张量重塑为一个形状为[-1, 16 * 4 * 4]的新张量。其中，-1表示该维度将由计算机自动确定，以确保张量大小保持不变。在这里，它将被设置为batch size，也就是输入张量的第一维度的大小。16 * 4 * 4表示16个特征图，每个特征图的大小为4x4。在LeNet网络中的结构中，可以发现在第二个池化层后，输出张量的大小为[batch_size, 16, 4, 4]。因此，执行 x.view(-1, 16 * 4 * 4) 操作后，输出张量的大小为[batch_size, 16 * 4 * 4]，将被用于全连接层的输入。这个过程可以理解为将特征图展平成一维向量，便于全连接层进行线性变换操作。在卷积神经网络中，这种展平的操作是一个很常见的操作。\n  \n\n### (5)怎么是\" 16 x 4 x4\"?论文的结构图中不是\" 16 x 5 x5\"? 为什么会换?\n\u003e 因为原论文的数据集输入的图片大小为32x32，而我们这里的代码使用到的数据集图片输入维度为28x28，所以想要去适配新的数据集输入维度（因为网络结构后面存在全连接层，到全连接层输入的时候维度是固定的），就需要去进行相应的修改\n  \n\n这段代码里面涉及到了Python类的定义和初始化方法等基础知识, 不了解的话去看Python的函数\n\n这段代码⾥⾯涉及到了Python类的定义和初始化⽅法等基础知识，不了解的话去看Python的函数 https://docs.python.org/zh-cn/3/tutorial/controlﬂow.html#deﬁning-functions 与类 https://docs.python.org/zh-cn/3/tutorial/classes.html 这⼀部分。  模型被定义好后，使⽤之前需要先实例出来，⽐如\n\n\n```python\nnet = LeNet()\n```\n\n\n下面来看看一个网络的训练是怎么进行的。\n  \n\n先来准备好数据, 现在以MNIST数据集为例。\n\n\n```python\nimport numpy as np\ntrain_X, test_X, train_y, test_y = np.load( './mnist.npy', allow_pickle=True)\nx_train = train_X.reshape(60000, 28*28).astype(np.float32) \nx_test = test_X.reshape(10000, 28*28).astype(np.float32)\n```  \n\n现在我们要把数据集处理一下再用作训练, 对于当前数据集, 需要有以下几步：\n  \n1. 转换成 tensor 类型\n2. 再转换成 Dataset 类型\n3. 再生成 DataLoader\n\n这几步是先后依赖的。我们想用小批量随机下降的方法来进行模型的训练, 所以需要借助 DataLoader 来进行 数据的批量加载-\u003e DataLoader 在创建的时候需要数据是 Dataset 类型-\u003e Dataset 类型的创建需要数据是 tensor 类型。\n\n\n下面具体来说, 首先将 NumPy 加载进来的数据类型 ndarray 转换成 tensor 类型:\n\n```python\nx_train, y_train, x_test, y_test = map(torch.tensor, (x_train, train_y, x_test, test_y))\n```\n  \n### (6) map 函数Python中很常使用到，在这里的作用是什么?\n\u003e map(function, iterable, ...) \n\u003e map函数可以使用第一个给定的函数将后面给定的可迭代对象中的元素进行相同的处理（使用给定的函数进行处理），最后将处理之后的可迭代对象进行返回。这里的作用就是将给定的训练集和测试集数据从numpy的ndarray转换为pytorch中的张量（tensor）进行返回。\n\n下面是生成 Dataset 类。\n\n```python\nfrom torch.utils.data import TensorDataset\n```\n  \nx_train 和  y_train 都可以合并为⼀个  TensorDataset ，这将更易于迭代和切⽚。\n\n```python\ntrain_ds = TensorDataset(x_train, y_train)\n```\n  \n\n在我们自己定义数据集的时候, 可以使用 PyTorch 的 TensorDataset 来实现这一步, 可是(7)我们为什么需 要这么做?\n\u003e在深度学习的模型训练中，通常需要同时处理大量的数据，而这些数据通常存储在数组、数组列表、字典等形式的数据结构中。每次从中分批次读取一部分数据，然后对其进行计算。但如果我们每次都直接从存储数据的数据结构中读取数据，会导致每次读取数据的时间都要花费一定的时间，并降低训练的效率。因此，在实际的开发中，通常需要将数据集封装为一个可迭代的数据流，以便于高效地读取数据。\n\n\u003e 在PyTorch中，我们通常使用`DataLoader`对象来定义一个可迭代的数据集。`DataLoader`对象可以设置批量大小，并自动从数据集中读取批量大小的数据并返回。此外，它还可以随机打乱数据集，使得模型能够更好地学习数据集中的规律。\n\n\u003e 而`TensorDataset`是 PyTorch 提供的一种数据集对象，可以方便地将数据集中的数据与标签封装成一个整体，方便将数据集作为一个整体传递给`DataLoader`，以供模型读取和训练。`TensorDataset`的主要作用是将多个Tensor数组打包起来，成为一个元素的形式，并提供一个默认的索引方法，方便管理和使用数据。将训练和验证数据转化为`TensorDataset`对象后，可以将对象传给`DataLoader`对象，并设置批量大小和其他参数，以便高效地读取和使用训练和验证数据。\n\n\u003e 因此，数据集的封装可以提高模型的训练效率和程序的可读性。而`TensorDataset`和`DataLoader`这两种类的结合使用，可以使程序更加清晰简洁，且具有更好的数据索引和读取方式，从而加快深度学习模型的训练效率。\n\n\u003e 除此之外, Pytorch 的 DataLoader 负责批次管理。需要从 Dataset 创建一个 DataLoader。 DataLoader 使迭代变得更加容易, 可以自动为我们提供随机打乱的每个小批量。\n\n```python\nfrom torch.utils.data import DataLoader \nbs = 64\ntrain_ds = TensorDataset(x_train, y_train)\ntrain_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n```\n  \n\n现在, 训练时使用的数据加载器已经处理好了, 来写一个训练过程。\n\n\n补充:\n\n  \n\n1. 通常管所有数据扫完一遍叫做一个epoch。\n\n```python\nepochs = 32\n```\n\n2. Pytorch 还提供了一个包含各种优化算法的软件包 torch.optim。我们可以使用优化器中的step方法来进 行参数的更新 (注意：loss.backward() 只是求得了每一个参数的梯度并且记录在参数对应的节点上, 但 是参数并不更新)。\n\n```python\nfrom torch import optim\nopt = optim.SGD(net.parameters(), lr=0.01) # 定义⼀个优化器\n```\n\n\n3. 学习率记作 Ir\n\n  \n```python\nlr = 0.01\n```\n  \n\n4. 损失函数也提前准备好\n\n```python\nimport torch.nn.functional as F loss_func = F.cross_entropy\n```\n  \n\n现在开始训练吧。\n\n  \n```python\nfor epoch in range(epochs):\n    for xb, yb in train_dl:\n        xb = torch.reshape(xb, [-1, 1, 28, 28])\n        pred = net(xb)\n        loss = loss_func(pred, yb)\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n        print(epoch, loss)\n```\n  \n\n### 上例中, (8) torch.reshape (xb，\\[-1，1，28，28\\]). 的作用是什么?\n\n在代码中，`torch.reshape(xb, [-1, 1, 28, 28])`的作用是将形状为 `(batch_size, n_inputs)` 的二维张量 `xb` 重塑为一个四维张量，其形状为 `(batch_size, 1, 28, 28)`，以便于在`LeNet`模型中使用。这相当于把一批 `batch_size` 个平面图像数据堆叠成一个 4 维的张量。其中，第一个维度表示批次大小(batch_size)，第二个维度表示通道数(channel), 1 表示灰度图像，第三个和第四个维度分别表示图像的高和宽。\n\nPyTorch中神经网络的输入通常是一个4维张量，包括 batch_size，channel，height，width 四个维度。在这段代码中，`xb`是一个2维张量，所以需要通过重塑操作将其变成对应的4维张量，用于`LeNet`模型的输入。在重塑之后，每个输入图像就被展开为一个形状为 `(1,28,28)` 的三维张量。这个重塑操作中的负数索引是将第一维(即 batch_size)根据第二个维度(即输入图像大小)自适应地计算得到的。\n\n重塑操作之后，就可以将此时的四维张量输入到神经网络中进行前向计算、反向梯度传播以及权重优化等操作了。\n\n### (9) opt. zero_grad() 的作用是什么? 为什么需要有这一步?\n在深度学习模型中进行反向传播算法，通常需要首先计算神经网络模型的损失函数，并计算它对相应参数的梯度。接着，使用这些梯度对神经网络模型的参数进行更新，以减小训练误差并提高模型性能。在 PyTorch 中，这个流程通常需要计算完梯度后使用优化器将参数更新，即 `opt.step()` 。\n\n然而，在神经网络模型中进行反向传播算法时，因为使用的是随机梯度下降法（SGD）等优化器，每次进行前向传播计算梯度时，该计算过程会在计算图中累积梯度，调用 `backward()` 函数后，这些累积的梯度会自动累加到每个对应的参数的 `.grad` 属性中，而不会被覆盖。\n\n因此，为了避免当前的梯度和上一次计算的梯度进行叠加，需要在每次 optimizer.step() 后手动把梯度清零，避免一次次累加产生计算误差。\n\n而 `opt.zero_grad()` 这个方法的作用就是将模型参数的梯度设置为0，以避免新一次的梯度更新时被上一次的梯度累计影响，从而保证模型训练过程的正确性。所以，每次经过一次梯度下降或反向传播计算后，都需要通过调用 `opt.zero_grad()` 来清除上一次的累计梯度，`opt.step()` 来执行优化算法的参数更新，从而保证模型能够正确地学习训练数据的特点和结构。\n  \n\n### (10)尝试对Irlbslepochs进行调整再观察训练过程。\n在深度学习中，训练深度学习模型时需要调节多个参数，以使模型具有更好的性能表现。其中，`lr`、`bs`、`epochs`等参数是模型训练过程中需要进行调节的重要参数。它们分别表示学习率、批量大小和训练轮数(或称为训练迭代次数)。\n\n-   学习率（`lr`）：它控制着每一次参数更新的幅度。在模型训练过程中，学习率可以影响到参数权重的更新速度和更新方向，从而对模型的性能有着很大的影响。如果学习率过大，那么模型可能会发散而无法收敛，如果学习率过小，模型可能会收敛得很慢，从而使得模型的训练时间延长。因此，需要根据实际情况手动设置一个适当的学习率 `lr`。通常情况下，可以从 `1e-5` 到 `1e+3` 之间尝试一些不同的值，然后选择一个最优的值。\n-   批量大小 （`bs`）：批量大小指的是在模型训练过程中一次迭代所使用的样本数。批量大小越大，会使得模型学习速度快但不够精细，且在训练时可能会占用更多的内存；批量大小越小，让模型更精细、更稳定，但学习速度会更慢，训练时间可能会变得更长。因此，需要根据实际情况手动设置一个适当的批量大小。通常情况下，常用的批量大小取值范围在 `8`、`16`、`32`、`64`、`128`、`256` 等大小之间。\n-   训练轮数（`epochs`）：训练迭代次数是指模型在训练时重复从训练数据集中提取数据、进行前向传播和反向传播等过程的次数。训练轮数的多少决定了模型对训练集数据的学习与拟合程度，影响精度的高低。通常情况下，训练轮数越多，模型的拟合能力就越强，但也会面临过拟合等问题。因此，需要根据实际情况手动设置一个适当的训练轮数。\n  \n\nLeNet 就说到这里, 下面来看另一个经典网络 \"AlexNet\"。相对于LeNet, AlexNet做的改进有:\n\n1. 更深的网络结构。AlexNet使用了比LeNet更深的网络结构, 包含5层卷积层和3层全连接层, 这样可以提取更 高级别的特征, 进而提高模型的准确率。\n\n2. ReLU激活函数的使用。AlexNet在每个卷积层之后都使用了ReLU激活函数, 这个激活函数相比于sigmoid激 活函数可以大大减少梯度消失的问题, 进而训练更深的神经网络。\n\n3. Dropout。AlexNet使用Dropout技术在全连接层中加入随机丟弃部分神经元的操作, 来防止过拟合, 提高模型的泛化能力。\n\n![](res/AlexNet模型结构图.png)\n\n  \n\n### (11)用到的新零件有哪些? 如何定义?\nReLU激活函数使用nn.ReLu()进行定义，Dropout随机失活技术使用nn.Dropout()进行定义\n\n### (12)请尝试定义AlexNet网络, 然后将上述过程对应实现\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\n\n# 数据集预处理\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225))\n])\n\n# 加载数据集\ntrainset = datasets.CIFAR10(root='./data', train=True, \n                           download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n                                          shuffle=True, num_workers=2)\ntestset = datasets.CIFAR10(root='./data', train=False,\n                          download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=32,\n                                         shuffle=False, num_workers=2)\n\n# 构建AlexNet网络\nclass AlexNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super(AlexNet, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=11, stride=4),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(96, 256, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        \n        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n        self.classifier = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(256 * 6 * 6, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n# 实例化网络\nnet = AlexNet()\n\n# 定义损失函数和优化器\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n\n# 训练网络\nfor epoch in range(10): # 循环遍历数据集10次\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        inputs, labels = data\n        optimizer.zero_grad() # 清零梯度\n        outputs = net(inputs) # 前向传播\n        loss = criterion(outputs, labels) # 计算损失\n        loss.backward() # 反向传播\n        optimizer.step() # 优化参数\n        \n        running_loss += loss.item()\n        if i % 2000 == 1999: # 输出损失函数\n            print('[%d, %5d] loss: %.3f' %\n                 (epoch + 1, i + 1, running_loss / 2000))\n            running_loss = 0.0\n\nprint('Finished Training')\n\n```\n\n\n### (13)在同样的数据集, bs和Ir等超参数设置下, 两个网络的训练效果差别是怎样的? \n\n- lenet是最早的卷积神经网络之一，1998年应用到手写数字识别任务上，取得了很好的效果。它的网络结构包含5层卷积和3层全连接，使用sigmoid激活函数和最大池化层。\n- alexnet是2012年提出的一个更深的卷积神经网络，包含5层卷积和3层全连接，使用了relu激活函数、dropout抑制过拟合和LRN局部响应归一化。它在大尺寸图片数据集ImageNet上获得了冠军。\n- 在同样的数据集、bs和lr等超参数设置下，alexnet相比lenet有以下优势：\n    - relu激活函数可以减少梯度消失现象，加快训练速度，提高模型性能。\n    - dropout可以有效防止过拟合，增强模型泛化能力。\n    - LRN可以增强特征图的对比度，减小不同层之间的不相关性。\n    - 更深的网络结构可以提取更高层次的特征，增加模型表达能力。\n\n\n下面进行下一个网络, 何恺明大神的\"ResNet\"。\n\n![](res/ResNet中的残差residual结构.png)\n\n这绝对称得上是\"中国之光\", 这个网络的出现, 让深度学习网络模型真正可以变深了。请参考我的视频\"ResNet 残差神经网络\"回答下列问题:\n\n\n### (14)请画出通道数有变化和无变化两种情况下的残差块结构。\n\n\n\n### (15)在无通道数变化情况下，进入残差块的数据是x，那么输出可以表示为什么? 中间的计算过程是怎样的?\n\n在无通道数变化的情况下，进入残差块的数据是 $x$，那么输出可以表示为 $y = f(x) + x$，其中 $f(x)$ 表示当前层的残差函数，$x$ 表示当前层的输入，$y$ 表示当前层的输出。\n\n中间的计算过程如下：\n\n1.  首先，将输入 $x$ 通过一个卷积层进行变换，得到 $x_{identity}$，即 $x_{identity} = W * x$，其中 $W$ 表示卷积层的参数。\n    \n2.  然后，将 $x_{identity}$ 和 $f(x)$ 相加，得到当前层的输出 $y$，即 $y = f(x) + x_{identity}$。\n    \n3.  最后，将 $y$ 作为当前层的输出，传递给下一层进行处理。\n    \n\n需要注意的是，残差块中的卷积层通常使用 $1 \\times 1$ 的卷积核，这样可以保持输入和输出的形状不变，避免通道数变化。此外，残差块中的残差函数 $f(x)$ 通常也是由若干个卷积层和激活函数组成的，用于对输入进行变换。\n  \n\n### (16)引入残差块解决的是什么问题? 请用公式推导证明。\n引入残差块的目的是为了解决深度神经网络中的梯度消失和梯度爆炸问题，使得网络可以更深更容易训练。\n\n在传统的神经网络中，每一层的输出都是通过激活函数对输入进行变换得到的，即 $y = f(x)$，其中 $x$ 是输入，$y$ 是输出，$f$ 是激活函数。而在残差网络中，每一层的输出是由输入和残差函数相加得到的，即 $y = f(x) + x$，其中 $x$ 是输入，$f(x)$ 是残差函数，$y$ 是输出。\n\n残差函数 $f(x)$ 表示的是当前层的输入 $x$ 与当前层的输出 $y$ 之间的差距，即残差。如果当前层的输入和输出相等，那么残差就为 0，此时 $y = x$，即当前层的输出等于当前层的输入，这种情况下，残差块就相当于一个恒等映射，不会对输入进行任何变换。如果当前层的输入和输出不相等，那么残差就不为 0，此时 $y = f(x) + x$，即当前层的输出是当前层的输入和残差函数的和，这种情况下，残差块就相当于对输入进行了一定的变换。\n\n通过引入残差块，可以使得网络更深，因为每一层的输出都可以通过残差函数和输入相加得到，这样就可以避免梯度消失和梯度爆炸问题。此外，残差块还可以使得网络更容易训练，因为残差块可以让网络更加稳定，减少了训练过程中的震荡和不稳定性。\n\n下面是残差块的公式推导证明：\n\n假设当前层的输入为 $x$，输出为 $y$，残差函数为 $f(x)$，那么残差块的输出可以表示为：\n\n$$y = f(x) + x$$\n\n为了方便推导，我们假设 $f(x)$ 和 $x$ 的维度相同，即 $f(x)$ 和 $x$ 的形状都为 $(n, c, h, w)$，其中 $n$ 表示 batch size，$c$ 表示通道数，$h$ 和 $w$ 分别表示高度和宽度。\n\n为了使得 $f(x)$ 和 $x$ 可以相加，我们需要对 $x$ 进行一个变换，使得它的形状和 $f(x)$ 相同。具体来说，我们可以通过一个卷积层来实现这个变换，这个卷积层的参数可以学习得到。假设这个卷积层的参数为 $W$，那么对 $x$ 进行变换的公式可以表示为：\n\n$$x_{identity} = W * x$$\n\n其中 $x_{identity}$ 表示经过变换后的 $x$，$*$ 表示卷积操作。\n\n将 $x_{identity}$ 代入残差块的公式中，可以得到：\n\n$$y = f(x) + x_{identity}$$\n\n将 $x_{identity}$ 展开，可以得到：\n\n$$y = f(x) + W * x$$\n因此，残差块的作用就是将输入 $x$ 和残差函数 $f(x)$ 相加，得到当前层的输出 $y$。如果 $f(x)$ 为 0，那么残差块就相当于一个恒等映射，不会对输入进行任何变换；如果 $f(x)$ 不为 0，那么残差块就相当于对输入进行了一定的变换。这样就可以避免梯度消失和梯度爆炸问题，使得网络可以更深更容易训练。\n\n总结一下，残差块的公式可以表示为：\n\n$$y = f(x) + W * x$$\n\n其中 $x$ 表示当前层的输入，$f(x)$ 表示当前层的残差函数，$W$ 表示一个卷积层的参数，$x_{identity}$ 表示经过变换后的 $x$，$y$ 表示当前层的输出。如果 $f(x)$ 为 0，那么 $y$ 就等于 $x_{identity}$，即残差块相当于一个恒等映射；如果 $f(x)$ 不为 0，那么 $y$ 就等于 $f(x)$ 和 $W * x$ 的和，即残差块相当于对输入进行了一定的变换。\n\n### (17)对于ResNet你还有哪些认识? (背景、作者、影响、延伸等方向不限)\nResNet（Residual Network）是由微软亚洲研究院的 Kaiming He 等人在 2015 年提出的一种深度残差网络结构。ResNet 在 ImageNet 等计算机视觉领域的竞赛中大获全胜，创造了当时最佳的结果。该网络结构的主要特点是引入残差块（Residual Block）来解决深度网络训练过程中梯度消失和梯度爆炸等问题。\n\nResNet 的主要贡献在于解决了深度神经网络的梯度爆炸和梯度消失问题。在此之前，随着神经网络的层数增加，训练误差呈先下降后上升的趋势，即深度网络的收敛速度反而变慢了，同时梯度消失和梯度爆炸等问题也随之而来。ResNet 的残差块引入了跨层连接，通过元素加和的方式将输入和输出进行了融合，使得信息的流动更加顺畅，普遍意义上获得更好的网络性能。\n\n由于 ResNet 在深度学习领域取得了极高的成就，很多后续的研究者都以 ResNet 为基础进行了扩展，包括 DenseNet、SENet 等等。\n\n此外，ResNet 的成功还促进了人们对深度神经网络和卷积神经网络的更深入的研究和理解。对深度网络结构优化的讨论广泛开展，像深度可分离卷积、Inception V3、EfficientNet 等多种网络也开始逐渐流行起来。\n\n总之，ResNet 作为深度学习领域的里程碑之一，为解决深度网络问题提供了全新的思路和方法。它的成功不仅影响了计算机视觉领域，同时也对推动机器学习和人工智能的发展产生了积极的影响。\n\n\n如果你有信心, 也可以自己去定义这个网络, 这里就不作要求了。但如果你想使用这个网络, 可以在PyTorch中直 接下载调用预训练好的模型。\n\n### (18)如何下载调用? 请下载后重新对MNIST数据集进行训练。\n\n```python\n# 导入必要的库\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\nfrom torchvision.models import resnet18\nfrom torchvision.transforms import ToTensor\n\n# 定义超参数\nbatch_size = 64\nnum_epochs = 10\nlearning_rate = 0.01\n\n# 加载mnist数据集，转换为张量格式\ntrain_ds = MNIST(\"mnist\", train=True, download=True, transform=ToTensor())\ntest_ds = MNIST(\"mnist\", train=False, download=True, transform=ToTensor())\ntrain_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\ntest_dl = DataLoader(test_ds, batch_size=batch_size)\n\n# 使用torchvision中的resnet18模型，并修改输入层和输出层，以适应mnist数据集\nmodel = resnet18(pretrained=True)\nmodel.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\nmodel.fc = nn.Linear(model.fc.in_features, 10)\n\n# 定义损失函数和优化器\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n# 定义训练和测试函数\ndef train_epoch(model, dataloader, criterion, optimizer):\n    model.train()\n    total_loss = 0\n    total_acc = 0\n    for inputs, labels in dataloader:\n        # 前向传播，计算输出和损失\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        # 反向传播，更新梯度\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        # 计算准确率\n        _, preds = torch.max(outputs, 1)\n        acc = torch.sum(preds == labels).item() / len(labels)\n        # 累加损失和准确率\n        total_loss += loss.item() * len(inputs)\n        total_acc += acc * len(inputs)\n    # 计算平均损失和准确率\n    avg_loss = total_loss / len(dataloader.dataset)\n    avg_acc = total_acc / len(dataloader.dataset)\n    return avg_loss, avg_acc\n\ndef test_epoch(model, dataloader, criterion):\n    model.eval()\n    total_loss = 0\n    total_acc = 0\n    with torch.no_grad():\n        for inputs, labels in dataloader:\n            # 前向传播，计算输出和损失\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            # 计算准确率\n            _, preds = torch.max(outputs, 1)\n            acc = torch.sum(preds == labels).item() / len(labels)\n            # 累加损失和准确率\n            total_loss += loss.item() * len(inputs)\n            total_acc += acc * len(inputs)\n    # 计算平均损失和准确率\n    avg_loss = total_loss / len(dataloader.dataset)\n    avg_acc = total_acc / len(dataloader.dataset)\n    return avg_loss, avg_acc\n\n# 进行训练和测试循环，打印每个epoch的结果，并记录损失函数变化\ntrain_losses = []\ntest_losses = []\nfor epoch in range(num_epochs):\n    train_loss, train_acc = train_epoch(model, train_dl, criterion, optimizer)\n    test_loss, test_acc = test_epoch(model, test_dl, criterion)\n    train_losses.append(train_loss)\n    test_losses.append(test_loss)\n    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n\n# 绘制损失函数变化曲线\nimport matplotlib.pyplot as plt\n\nplt.plot(train_losses, label=\"Train Loss\")\nplt.plot(test_losses, label=\"Test Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n\n```\n\n上面的模型大多用来对图片进行分类, 下面来说图像处理中的另一个主要任务：目标检测。\n\n### (19)相对于图片分类来说, 目标检测是怎样的任务?\n相对于图片分类而言，目标检测是一种更为复杂和细致的视觉任务。\n\n在图片分类任务中，我们的目标是将整张图像分为不同的类别，而对于目标检测任务，我们需要在图像中检测出物体（对象）的位置、大小和类别。也就是说，目标检测需要我们更加细致地分析图像中的每一个物体，将物体与背景区分开来，并区分不同物体之间的差异，同时还要给出每个物体的位置和大小。\n\n在目标检测任务中，通常需要利用图像处理技术来检测出物体的位置和大小，包括边缘检测、图像分割、关键点检测等等。然后，需要依据物体的位置和大小对物体进行分类，经典的目标检测算法通常采用滑动窗口和锚点回归等技术来实现。\n\n同时，为了提高模型的检测性能，还需要处理一些特殊情况，比如遮挡、物体形变、光照变化等，这些都会影响物体的位置、大小和类别的检测。\n\n因此，相对于图片分类任务而言，目标检测任务需要更加细致、复杂的分析和处理，需要从多个方面来获取图像信息，并将其综合起来，达到精确检测目标的目的。\n\n\n目标检测中有两种常见解决思路, two stages 和 one stage。\n\n其中R-CNN网络可以看做是 two stages 的代表, 后续还有Fast-RCNN和Faster-RCNN。\n\n![](res/R_CNN_regions_with_cnn_features.png)\n\n### 看我的视频\"R-CNN的设计思想\"后, 20简述\"R-CNN的设计思想\"\nR-CNN（Region-based Convolutional Neural Networks）是一种经典的目标检测算法，其设计思想主要有以下几点：\n\n1. 候选区域提取\n\n传统的物体检测算法通常需要计算所有位置的滑动窗口，这种方法计算复杂度高、效率低、准确率低。而在 R-CNN 中，采用了选择性搜索（Selective Search）算法来提取物体候选框，此算法能够基于图像的局部相似性找到预选的目标区域，从而避免了遍历大量的滑动窗口。\n\n2. 特征提取\n\n提取候选框附近的图像特征，采用的是通过预训练模型提取卷积特征，再将卷积特征送入多层全连接网络（FCN）进行分类和回归。这个全连接网络相当于是针对每个候选框都生成的一个独立的小型神经网络。\n\n3. 模型训练\n\n在模型训练中，采用了两阶段的方式进行。首先，采用分类SVM来分类每个候选框是否包含物体。然后，对于分类正确的候选框，再采用回归器来微调其对应的边框。也就是说，模型在两个任务上进行训练：判断是否包含物体和边框的微调。\n\n4. 目标检测\n\n完成模型的训练后，就可以用于目标检测任务了。对于一张输入图像，首先采用选择性搜索算法提取出若干个候选框，然后将这些候选框送入卷积神经网络进行分类和回归，最后根据分类的结果和回归的结果得到最终的目标检测结果。\n\n以上就是 R-CNN 的设计思想。通过使用选择性搜索算法来提取候选框，使用预训练模型来提取特征并采用两阶段的训练方式进行目标检测，R-CNN 在其提出时创下了目标检测领域的新里程碑，从而推动了目标检测算法的发展。\n\n\none stage 的代表当然就是你一定听过的YOLO模型了, 我们来以YOLOv1为例进行学习。\n\n![](res/yolov1_模型结构.png)\n  \n\n### (21)测试过程中, 网络的输入图像是多大?\n\u003e 3 x 448 x 448\n  \n\n### (22)输出的是 \\(7 x 7 x 30\\) 的张量, 每一个位置的数值分别代表什么意义?\n\u003e 7 x 7 表示有49个格子来分别进行目标检测，30代表着这一个格子中处理的结果，其中前5个是一个框的描述，分别为 x,y,w,h,c，其中x,y表示框的中心点（必须在这个格子内，而w,h就是这个框的宽度和高度（可以超过格子的大小），而c则是这个框中存在检测目标的置信度。后面5个也是如此，这样子就是10个数据了。而后面的20个数字则是代表着20种类型的可能性。\n\n### (23)参考下图解释, 模型预测出来的框是98个, 之后经过哪些处理才变为下图中的3个框?\n训练结束后，进行预测时也是按照着训练时的标签含义进行解析的，也就是说预测的结果是中间的效果。会发现有很多框在描述同一个物体，这个时候就需要使用 NMC非极大意志 进行框的合并，这样子就可以得到右边的结果。\n\nNMC 非极大意志（Non-maximum suppression）是一种用于抑制目标检测算法输出中冗余的边界框的方法。在目标检测中，一张图像可能会包含多个物体，而物体的检测通常是通过算法输出一组边界框的形式体现的。这时，就需要使用 NMS 对这些边界框进行筛选，使得每个物体只被匹配到一个边框，达到非极大抑制的目的。\n\n具体来说，NMS 的处理过程大致如下：\n\n1. 对所有输出边界框按照其置信度（confidence score）从高到低排序；\n\n2. 取出置信度最高的边界框，并从剩余的边界框中删除与其 IoU（Intersection over Union）大于一定阈值的边界框；\n\n3. 重复上述步骤，直到所有置信度较高的边界框都被检查过。\n\n这样处理之后，就可以通过一个阈值来控制最终筛选出来的边界框的数量，并保证每个物体只被匹配到一个边框。\n\nNMS 是目标检测算法中非常重要的一步，通常在 Faster R-CNN、YOLO等经典目标检测算法中都有应用。\n\n![](res/yolov1模型预测结果.png)\n  \n\n### (24)训练过程中, 选用的损失函数是怎样的? 请详细解释。\n\n\n1. 对边框 x 和 y 的位置进行纠正。这一部分是计算预测的边框中心点和真实的边框中心点之间的平方差，只有当网格中存在物体时才计算。这一部分的目的是使预测的边框更接近真实的边框。\n2. 对边框 w 和 h 的大小进行纠正。这一部分是计算预测的边框宽度和高度的平方根和真实的边框宽度和高度的平方根之间的平方差，只有当网格中存在物体时才计算。这一部分的目的是使预测的边框更符合真实的边框尺寸，同时减小对大尺寸边框的惩罚。\n3. 对有物体的情况置信度进行纠正。这一部分是计算预测的置信度和真实的置信度之间的平方差，只有当网格中存在物体时才计算。真实的置信度是指预测的边框和真实的边框之间的交并比 (IoU)。这一部分的目的是使预测的置信度更接近真实的置信度，即更能反映预测边框和真实边框之间的重合程度。\n4. 对没有物体的情况置信度进行纠正。这一部分是计算预测的置信度和零之间的平方差，只有当网格中不存在物体时才计算。这一部分的目的是使预测的置信度更接近零，即更能反映预测边框和真实边框之间的不重合程度。\n5. 对每一个类别的条件概率进行纠正。这一部分是计算预测的类别概率和真实的类别概率之间的平方差，只有当网格中存在物体时才计算。这一部分的目的是使预测的类别概率更接近真实的类别概率，即更能反映网格中物体所属的类别。\n\n根据搜索结果，yolov1 损失函数的表达式如下¹：\n\n$$\n\\lambda_{coord} \\sum_{i=0}^{S^2} \\sum_{j=0}^B \\mathbb{1}_{ij}^{obj} [(x_i - \\hat{x}_i)^2 + (y_i - \\hat{y}_i)^2] + \\lambda_{coord} \\sum_{i=0}^{S^2} \\sum_{j=0}^B \\mathbb{1}_{ij}^{obj} [(\\sqrt{w_i} - \\sqrt{\\hat{w}_i})^2 + (\\sqrt{h_i} - \\sqrt{\\hat{h}_i})^2] + \\sum_{i=0}^{S^2} \\sum_{j=0}^B \\mathbb{1}_{ij}^{obj} (C_i - \\hat{C}_i)^2 + \\lambda_{noobj} \\sum_{i=0}^{S^2} \\sum_{j=0}^B \\mathbb{1}_{ij}^{noobj} (C_i - \\hat{C}_i)^2 + \\sum_{i=0}^{S^2} \\mathbb{1}_{i}^{obj} \\sum_{c \\in classes}(p_i(c) - \\hat{p}_i(c))^2\n$$\n\n其中：\n\n- $S$ 是网格的大小，例如 $S=7$ 表示将图像划分为 $7\\times 7$ 的网格；\n- $B$ 是每个网格预测的边框的数量，例如 $B=2$ 表示每个网格预测两个边框；\n- $\\mathbb{1}_{ij}^{obj}$ 是一个指示函数，表示第 $i$ 个网格的第 $j$ 个边框是否负责预测某个物体，如果是则为 1，否则为 0；\n- $\\mathbb{1}_{ij}^{noobj}$ 是一个指示函数，表示第 $i$ 个网格的第 $j$ 个边框是否不负责预测任何物体，如果是则为 1，否则为 0；\n- $\\mathbb{1}_{i}^{obj}$ 是一个指示函数，表示第 $i$ 个网格是否包含某个物体的中心点，如果是则为 1，否则为 0；\n- $(x_i, y_i)$ 和 $(\\hat{x}_i, \\hat{y}_i)$ 分别是第 $i$ 个网格的第 $j$ 个边框的预测和真实的中心点坐标，归一化到 [0, 1] 区间；\n- $(w_i, h_i)$ 和 $(\\hat{w}_i, \\hat{h}_i)$ 分别是第 $i$ 个网格的第 $j$ 个边框的预测和真实的宽度和高度，归一化到 [0, 1] 区间；\n- $C_i$ 和 $\\hat{C}_i$ 分别是第 $i$ 个网格的第 $j$ 个边框的预测和真实的置信度，表示边框和物体之间的交并比 (IoU)；\n- $p_i(c)$ 和 $\\hat{p}_i(c)$ 分别是第 $i$ 个网格的预测和真实的类别概率，表示网格中物体属于类别 $c$ 的概率；\n- $\\lambda_{coord}$ 和 $\\lambda_{noobj}$ 是两个权重系数，用于平衡不同部分的损失。\n\n对于每一部分进行详细的说明：\n\n- 第一部分和第二部分是位置损失。这两部分是计算预测的边框和真实的边框之间的位置和尺寸差异，只有当网格中存在物体时才计算。这两部分的目的是使预测的边框更接近真实的边框。这两部分乘以了一个较大的系数 $\\lambda_{coord}$，表示位置损失相比其他损失更重要。\n- 第三部分和第四部分是置信度损失。这两部分是计算预测的置信度和真实的置信度之间的差异。当网格中存在物体时，真实的置信度等于预测边框和真实边框之间的 IoU；当网格中不存在物体时，真实的置信度等于零。这两部分的目的是使预测的置信度更接近真实的置信度。这两部分乘以了不同的系数 $\\lambda_{obj}$ 和 $\\lambda_{noobj}$，表示有物体和无物体时置信度损失的权重不同。通常 $\\lambda_{obj}$ 大于 $\\lambda_{noobj}$，表示有物体时置信度损失相比无物体时更重要。\n- 第五部分是类别损失。这一部分是计算预测的类别概率和真实的类别概率之间的差异，只有当网格中存在物体时才计算。这一部分的目的是使预测的类别概率更接近真实的类别概率。\n\n\n造不造轮子这个看个人, 但是使用轮子的话, 把具体内容了解清楚了也肯定会收益。强烈建议有余力的同学, 将 本次内容中涉及到的所有论文进行精读, 他们都非常经典。","lastmodified":"2023-06-07T15:55:55.954775445Z","tags":["机器学习"]},"/notes/obsidian":{"title":"Obsidian Vault Integration","content":"\n## Setup\nObsidian is the preferred way to use Quartz. You can either create a new Obsidian Vault or link one that your already have.\n\n### New Vault\nIf you don't have an existing Vault, [download Obsidian](https://obsidian.md/) and create a new Vault in the `/content` folder that you created and cloned during the [setup](notes/setup.md) step.\n\n### Linking an existing Vault\nThe easiest way to use an existing Vault is to copy all of your files (directory and hierarchies intact) into the `/content` folder.\n\n## Settings\nGreat, now that you have your Obsidian linked to your Quartz, let's fix some settings so that they play well.\n\nOpen Settings \u003e Files \u0026 Links and look for these two items:\n\n1. Set the **New link format** to **Absolute Path in vault**. If you have a completely flat vault (no folders), this step isn't necessary.\n2. Turn **on** the **Automatically update internal links** setting.\n\n\n![[notes/images/obsidian-settings.png]]*Obsidian Settings*\n\n## Templates\nInserting front matter everytime you want to create a new Note gets annoying really quickly. Luckily, Obsidian supports templates which makes inserting new content really easily.\n\n\u003e [!WARNING]\n\u003e \n\u003e **If you decide to overwrite the `/content` folder completely, don't remove the `/content/templates` folder!**\n\nHead over to Options \u003e Core Plugins and enable the Templates plugin. Then go to Options \u003e Hotkeys and set a hotkey for 'Insert Template' (I recommend `[cmd]+T`). That way, when you create a new note, you can just press the hotkey for a new template and be ready to go!\n\n\u003e 👀 Step 4: [Preview Quartz Changes](notes/preview%20changes.md)\n","lastmodified":"2023-06-07T15:55:55.954775445Z","tags":["setup"]},"/notes/philosophy":{"title":"Quartz Philosophy","content":"\n\u003e “[One] who works with the door open gets all kinds of interruptions, but [they] also occasionally gets clues as to what the world is and what might be important.” — Richard Hamming\n\n## Why Quartz?\nHosting a public digital garden isn't easy. There are an overwhelming number of tutorials, resources, and guides for tools like [Notion](https://www.notion.so/), [Roam](https://roamresearch.com/), and [Obsidian](https://obsidian.md/), yet none of them have super easy to use *free* tools to publish that garden to the world.\n\nI've personally found that\n1. It's nice to access notes from anywhere\n2. Having a public digital garden invites open conversations\n3. It makes keeping personal notes and knowledge *playful and fun*\n\nI was really inspired by [Bianca](https://garden.bianca.digital/) and [Joel](https://joelhooks.com/digital-garden)'s digital gardens and wanted to try making my own.\n\n**The goal of Quartz is to make hosting your own public digital garden free and simple.** You don't even need your own website. Quartz does all of that for you and gives your own little corner of the internet.\n","lastmodified":"2023-06-07T15:55:55.954775445Z","tags":[]},"/notes/preview-changes":{"title":"Preview Changes","content":"\nIf you'd like to preview what your Quartz site looks like before deploying it to the internet, the following\ninstructions guide you through installing the proper dependencies to run it locally.\n\n\n## Install `hugo-obsidian`\nThis step will generate the list of backlinks for Hugo to parse. Ensure you have [Go](https://golang.org/doc/install) (\u003e= 1.16) installed.\n\n```bash\n# Install and link `hugo-obsidian` locally\ngo install github.com/jackyzha0/hugo-obsidian@latest\n```\n\nIf you are running into an error saying that `command not found: hugo-obsidian`, make sure you set your `GOPATH` correctly (see [[notes/troubleshooting#`command not found: hugo-obsidian`|the troubleshooting page]])! This will allow your terminal to correctly recognize hugo-obsidian as an executable.\n\n##  Installing Hugo\nHugo is the static site generator that powers Quartz. [Install Hugo with \"extended\" Sass/SCSS version](https://gohugo.io/getting-started/installing/) first. Then,\n\n```bash\n# Navigate to your local Quartz folder\ncd \u003clocation-of-your-local-quartz\u003e\n\n# Start local server\nmake serve\n\n# View your site in a browser at http://localhost:1313/\n```\n\n\u003e [!INFO] Docker Support\n\u003e\n\u003e If you have the Docker CLI installed already, you can avoid installing `hugo-obsidian` and `hugo`. Instead, open your terminal, navigate to your folder with Quartz and run `make docker`\n\nAfterwards, start the Hugo server as shown above and your local backlinks and interactive graph should be populated! Now, let's get it hosted online.\n\n\u003e 🌍 Step 5: [Hosting Quartz online!](notes/hosting.md)\n","lastmodified":"2023-06-07T15:55:55.954775445Z","tags":["setup"]},"/notes/search":{"title":"Search","content":"\nQuartz supports two modes of searching through content.\n\n## Full-text\nFull-text search is the default in Quartz. It produces results that *exactly* match the search query. This is easier to setup but usually produces lower quality matches.\n\n```yaml {title=\"data/config.yaml\"}\n# the default option\nenableSemanticSearch: false\n```\n\n## Natural Language\nNatural language search is powered by [Operand](https://beta.operand.ai/). It understands language like a person does and finds results that best match user intent. In this sense, it is closer to how Google Search works.\n\nNatural language search tends to produce higher quality results than full-text search.\n\nHere's how to set it up.\n\n1. Login or Register for a new Operand account. Click the verification link sent to your email, and you'll be redirected to the dashboard. (Note) You do not need to enter a credit card to create an account, or get started with the Operand API. The first $10 of usage each month is free. To learn more, see pricing. If you go over your free quota, we'll (politely) reach out and ask you to configure billing.\n2. Create your first index. On the dashboard, under \"Indexes\", enter the name and description of your index, and click \"Create Index\". Note down the ID of the index (obtained by clicking on the index name in the list of indexes), as you'll need it in the next step. IDs are unique to each index, and look something like `uqv1duxxbdxu`.\n3. Click into the index you've created. Under \"Index Something\", select \"SITEMAP\" from the dropdown and click \"Add Source\".\n4. For the \"Sitemap.xml URL\", put your deployed site's base URL followed by `sitemap.xml`. For example, for `quartz.jzhao.xyz`, put `https://quartz.jzhao.xyz/sitemap.xml`. Leave the URL Regex empty. \n5. Get your API key. On the dashboard, under \"API Keys\", you can manage your API keys. If you don't already have an API key, click \"Create API Key\". You'll need this for the next step.\n6. Open `data/config.yaml`. Set `enableSemanticSearch` to `true`, `operandApiKey` to your copied key, and `operandIndexId` to the ID of the index we created from earlier..\n\n```yaml {title=\"data/config.yaml\"}\n# the default option\nsearch:\n  enableSemanticSearch: true\n  operandApiKey: \"jp9k5hudse2a828z98kxd6z3payi8u90rnjf\"\n  operandIndexId: \"s0kf3bd6tldw\"\n```\n7. Push your changes to the site and wait for it to deploy.\n8. Check the Operand dashboard and wait for your site to index. Enjoy natural language search powered by Operand!\n","lastmodified":"2023-06-07T15:55:55.954775445Z","tags":[]},"/notes/setup":{"title":"Setup","content":"\n## Making your own Quartz\nSetting up Quartz requires a basic understanding of `git`. If you are unfamiliar, [this resource](https://resources.nwplus.io/2-beginner/how-to-git-github.html) is a great place to start!\n\n### Forking\n\u003e A fork is a copy of a repository. Forking a repository allows you to freely experiment with changes without affecting the original project.\n\nNavigate to the GitHub repository for the Quartz project:\n\n📁 [Quartz Repository](https://github.com/jackyzha0/quartz)\n\nThen, Fork the repository into your own GitHub account. **Make sure that when you fork, you _uncheck_ the 'Copy the `hugo` branch only' option**.\n\nIf you don't have an account, you can make on for free [here](https://github.com/join). More details about forking a repo can be found on [GitHub's documentation](https://docs.github.com/en/get-started/quickstart/fork-a-repo).\n\n![[notes/images/fork.png]]\n\n### Cloning\nAfter you've made a fork of the repository, you need to download the files locally onto your machine. Ensure you have `git`, then type the following command in your terminal replacing `YOUR-USERNAME` with your GitHub username.\n\n```shell\ngit clone https://github.com/YOUR-USERNAME/quartz\n```\n\n## Editing\nGreat! Now you have everything you need to start editing and growing your digital garden. If you're ready to start writing content already, check out the recommended flow for editing notes in Quartz.\n\n\u003e ✏️ Step 2: [Editing Notes in Quartz](notes/editing.md)\n\nHaving problems? Checkout our [FAQ and Troubleshooting guide](notes/troubleshooting.md).\n","lastmodified":"2023-06-07T15:55:55.954775445Z","tags":["setup"]},"/notes/showcase":{"title":"Showcase","content":"\nWant to see what Quartz can do? Here are some cool community gardens :)\n\n- [Quartz Documentation (this site!)](https://quartz.jzhao.xyz/)\n- [Jacky Zhao's Garden](https://jzhao.xyz/)\n- [Scaling Synthesis - A hypertext research notebook](https://scalingsynthesis.com/)\n- [AWAGMI Intern Notes](https://notes.awagmi.xyz/)\n- [Shihyu's PKM](https://shihyuho.github.io/pkm/)\n- [SlRvb's Site](https://slrvb.github.io/Site/)\n- [Course notes for Information Technology Advanced Theory](https://a2itnotes.github.io/quartz/)\n- [Brandon Boswell's Garden](https://brandonkboswell.com)\n- [Siyang's Courtyard](https://siyangsun.github.io/courtyard/)\n- [Data Dictionary 🧠](https://glossary.airbyte.com/)\n- [sspaeti.com's Second Brain](https://brain.sspaeti.com/)\n- [oldwinterの数字花园](https://garden.oldwinter.top/)\n- [SethMB Work](https://sethmb.xyz/)\n- [Abhijeet's Math Wiki](https://abhmul.github.io/quartz/Math-Wiki/)\n- [Mike's AI Garden 🤖🪴](https://mwalton.me/)\n\nIf you want to see your own on here, submit a [Pull Request adding yourself to this file](https://github.com/jackyzha0/quartz/blob/hugo/content/notes/showcase.md)!\n","lastmodified":"2023-06-07T15:55:55.954775445Z","tags":[]},"/notes/troubleshooting":{"title":"Troubleshooting and FAQ","content":"\nStill having trouble? Here are a list of common questions and problems people encounter when installing Quartz.\n\nWhile you're here, join our [Discord](https://discord.gg/cRFFHYye7t) :)\n\n### Does Quartz have Latex support?\nYes! See [CJK + Latex Support (测试)](notes/CJK%20+%20Latex%20Support%20(测试).md) for a brief demo.\n\n### Can I use \\\u003cObsidian Plugin\\\u003e in Quartz?\nUnless it produces direct Markdown output in the file, no. There currently is no way to bundle plugin code with Quartz.\n\nThe easiest way would be to add your own HTML partial that supports the functionality you are looking for.\n\n### My GitHub pages is just showing the README and not Quartz\nMake sure you set the source to deploy from `master` (and not `hugo`) using `/ (root)`! See more in the [hosting](/notes/hosting) guide\n\n### Some of my pages have 'January 1, 0001' as the last modified date\nThis is a problem caused by `git` treating files as case-insensitive by default and some of your posts probably have capitalized file names. You can turn this off in your Quartz by running this command.\n\n```shell\n# in the root of your Quartz (same folder as config.toml)\ngit config core.ignorecase true\n\n# or globally (not recommended)\ngit config --global core.ignorecase true\n```\n\n### Can I publish only a subset of my pages?\nYes! Quartz makes selective publishing really easy. Heres a guide on [excluding pages from being published](notes/ignore%20notes.md).\n\n### Can I host this myself and not on GitHub Pages?\nYes! All built files can be found under `/public` in the `master` branch. More details under [hosting](notes/hosting.md).\n\n### `command not found: hugo-obsidian`\nMake sure you set your `GOPATH` correctly! This will allow your terminal to correctly recognize `hugo-obsidian` as an executable.\n\n```shell\n# Add the following 2 lines to your ~/.bash_profile (~/.zshrc if you are on Mac)\nexport GOPATH=/Users/$USER/go\nexport PATH=$GOPATH/bin:$PATH\n\n# In your current terminal, to reload the session\nsource ~/.bash_profile # again, (~/.zshrc if you are on Mac)\n```\n\n### How come my notes aren't being rendered?\nYou probably forgot to include front matter in your Markdown files. You can either setup [Obsidian](notes/obsidian.md) to do this for you or you need to manually define it. More details in [the 'how to edit' guide](notes/editing.md).\n\n### My custom domain isn't working!\nWalk through the steps in [the hosting guide](notes/hosting.md) again. Make sure you wait 30 min to 1 hour for changes to take effect.\n\n### How do I setup analytics?\nQuartz by default uses [Plausible](https://plausible.io/) for analytics. \n\nIf you would prefer to use Google Analytics, you can follow this [guide in the Hugo documentation](https://gohugo.io/templates/internal/#google-analytics). \n\nAlternatively, you can also import your Google Analytics data into Plausible by [following this guide](https://plausible.io/docs/google-analytics-import).\n\n\n### How do I change the content on the home page?\nTo edit the main home page, open `/content/_index.md`.\n\n### How do I change the colours?\nYou can change the theme by editing `assets/custom.scss`. More details on customization and themeing can be found in the [customization guide](notes/config.md).\n\n### How do I add images?\nYou can put images anywhere in the `/content` folder.\n\n```markdown\nExample image (source is in content/notes/images/example.png)\n![Example Image](/content/notes/images/example.png)\n```\n\n### My Interactive Graph and Backlinks aren't up to date\nBy default, the `linkIndex.json` (which Quartz needs to generate the Interactive Graph and Backlinks) are not regenerated locally. To set that up, see the guide on [local editing](notes/editing.md)\n\n### Can I use React/Vue/some other framework?\nNot out of the box. You could probably make it work by editing `/layouts/_default/single.html` but that's not what Quartz is designed to work with. 99% of things you are trying to do with those frameworks you can accomplish perfectly fine using just vanilla HTML/CSS/JS.\n\n## Still Stuck?\nQuartz isn't perfect! If you're still having troubles, file an issue in the GitHub repo with as much information as you can reasonably provide. Alternatively, you can message me on [Twitter](https://twitter.com/_jzhao) and I'll try to get back to you as soon as I can.\n\n🐛 [Submit an Issue](https://github.com/jackyzha0/quartz/issues)\n","lastmodified":"2023-06-07T15:55:55.954775445Z","tags":[]},"/notes/updating":{"title":"Updating","content":"\nHaven't updated Quartz in a while and want all the cool new optimizations? On Unix/Mac systems you can run the following command for a one-line update! This command will show you a log summary of all commits since you last updated, press `q` to acknowledge this. Then, it will show you each change in turn and press `y` to accept the patch or `n` to reject it. Usually you should press `y` for most of these unless it conflicts with existing changes you've made! \n\n```shell\nmake update\n```\n\nOr, if you don't want the interactive parts and just want to force update your local garden (this assumed that you are okay with some of your personalizations been overriden!)\n\n```shell\nmake update-force\n```\n\nOr, manually checkout the changes yourself.\n\n\u003e [!warning] Warning!\n\u003e\n\u003e If you customized the files in `data/`, or anything inside `layouts/`, your customization may be overwritten!\n\u003e Make sure you have a copy of these changes if you don't want to lose them.\n\n\n```shell\n# add Quartz as a remote host\ngit remote add upstream git@github.com:jackyzha0/quartz.git\n\n# index and fetch changes\ngit fetch upstream\ngit checkout -p upstream/hugo -- layouts .github Makefile assets/js assets/styles/base.scss assets/styles/darkmode.scss config.toml data \n```\n","lastmodified":"2023-06-07T15:55:55.954775445Z","tags":[]}}